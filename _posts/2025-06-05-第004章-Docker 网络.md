---
layout: post
title: 2025-06-05-第004章-Docker 网络.md
categories: [Docker 容器]
description: 
keywords: Docker 网络.md
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---
# Network

## Base Concept

```bash
$ ip addr
# 本机回环地址
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
# 服务器内网地址
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:16:3e:0e:69:dc brd ff:ff:ff:ff:ff:ff
    inet 172.19.104.23/20 brd 172.19.111.255 scope global dynamic eth0
       valid_lft 239300182sec preferred_lft 239300182sec
# docker0
3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:c1:ab:0f:28 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
# 容器地址
18: veth3776ec8@if17: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default 
    link/ether de:6b:54:d6:fd:42 brd ff:ff:ff:ff:ff:ff link-netnsid 2
# 容器地址
168: veth160e3cd@if167: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP group default 
    link/ether 06:41:98:85:ff:89 brd ff:ff:ff:ff:ff:ff link-netnsid 0
```



①、本机回环地址

lo,127.0.0.1，不属于任何一个有类别地址类。它代表设备的本地虚拟接口，通常在安装网卡前就可以ping通这个本地回环地址。

一般用来测试本机的网络配置，能PING通 127.0.0.1 说明本机的IP协议安装没有问题。

②、服务器内网地址

ens33,192.168.88.2，这也是我创建docker宿主机的真实IP地址。

注意：我这里是安装虚拟机，如果是真实物理机，这个名字可能是eth0，eth0表示第一块网卡，同理eth2表示第二块网卡。

③、docker0

Docker启动的时候会在主机上自动创建一个docker0网桥（注意名字一定是docker0，会有docker1,docker2之类），实际上是一个 Linux 网桥，所有容器的启动如果在docker run的时候没有指定网络模式的情况下都会挂载到docker0网桥上。

④、容器地址



## Type Of Network

当docker安装后，会自动在服务器中创建三种网络：none、host和bridge。

```sh
$ sudo docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
528c3d49c302   bridge    bridge    local
464b3d11003c   host      host      local
faa8eb8310b4   none      null      local
```



### None

该模式关闭了容器的网络功能。

none网络就是什么都没有的网络，使用此网络的容器除了lo没有其他任何网卡，所以此容器不提供任何网络服务，也无法访问其它容器，那么这种封闭网络有什么用呢？这个一般会用于对网络安全要求较高并且不需要联网的应用。

```sh
$ sudo docker run -it --network=none busybox
/ # ifconfig
lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
```



### Host

容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。

使用host网络的容器将共享docker host的网络栈，容器的网络配置与host一样。大家可以发现，使用此网络的容器能看到服务器所有网卡，这种网络模式的使用场景是那些对网络传输效率要求较高的应用，当然此模式也存在不够灵活的问题，例如端口冲突问题。

```sh
$ sudo docker run -it --network=host busybox
/ # ifconfig
docker0   Link encap:Ethernet  HWaddr 02:42:6B:5A:FC:54
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:17 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 B)  TX bytes:1526 (1.4 KiB)

ens33     Link encap:Ethernet  HWaddr 00:0C:29:D5:73:1C
          inet addr:172.16.194.135  Bcast:172.16.194.255  Mask:255.255.255.0
          inet6 addr: fe80::20c:29ff:fed5:731c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:32097 errors:0 dropped:0 overruns:0 frame:0
          TX packets:20666 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:24596169 (23.4 MiB)  TX bytes:9708382 (9.2 MiB)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:270 errors:0 dropped:0 overruns:0 frame:0
          TX packets:270 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:24089 (23.5 KiB)  TX bytes:24089 (23.5 KiB)
```



### Bridge

此模式会为每一个容器分配、设置IP等，并将容器连接到一个docker0虚拟网桥，通过docker0网桥以及Iptables nat表配置与宿主机通信。

Docker安装时会创建一个名为docker0的网桥（使用brctl工具可以查看Linux下所有网桥），容器启动时如果不指定--network则默认都是bridge网络。

```sh
$ brctl show
bridge name  bridge id    STP enabled  interfaces
docker0    8000.02426b5afc54  no
```



此时我们启动一个Nginx容器看看该网桥的变化：

```sh
$ sudo docker run -d nginx
27644bd64114482f58adc47a52d3f768732c0396ca0eda8f13e68b10385ea359
$ brctl show
bridge name  bridge id    STP enabled  interfaces
docker0    8000.0242cef1fc32  no    vethe21ab12
```



我们可以看到此时一个新的网络接口vethe21ab12挂到docker0上，vethe21ab12就是Nginx容器的虚拟网卡。我们接着去Nginx容器中看看网络配置情况：

```sh
$ sudo docker exec -it 27644bd64114 bash
root@27644bd64114:/# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
45: eth0@if46: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
```



我们看到容器中有一个网卡eth0@if46，为什么不是vethe21ab12呢？实际上，eth0@if46和vethe21ab12是一对veth-pair，那什么是veth-pair呢？

我们还能看到这个容器中eth0@if46配置的ip是172.17.0.2，为什么是这个网段呢？我们看下网桥的网络配置：

```sh
$ sudo docker network inspect bridge
[
    {
        "Name": "bridge",
        "Id": "6f0087fd32cd0e3b4a7ef1133e2f5596a2e74429cf5240e42012af85d1146b9f",
        "Created": "2021-10-17T06:43:11.849460184Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "172.17.0.0/16",
                    "Gateway": "172.17.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {
            "27644bd64114482f58adc47a52d3f768732c0396ca0eda8f13e68b10385ea359": {
                "Name": "lucid_moore",
                "EndpointID": "b6c9de5a52e2f0b858a6dbf6c8e51b535aec3500fca7d1f1dcca98613e57dabd",
                "MacAddress": "02:42:ac:11:00:02",
                "IPv4Address": "172.17.0.2/16",
                "IPv6Address": ""
            }
        },
        "Options": {
            "com.docker.network.bridge.default_bridge": "true",
            "com.docker.network.bridge.enable_icc": "true",
            "com.docker.network.bridge.enable_ip_masquerade": "true",
            "com.docker.network.bridge.host_binding_ipv4": "0.0.0.0",
            "com.docker.network.bridge.name": "docker0",
            "com.docker.network.driver.mtu": "1500"
        },
        "Labels": {}
    }
]
$ ifconfig docker0
docker0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255
        inet6 fe80::42:ceff:fef1:fc32  prefixlen 64  scopeid 0x20<link>
        ether 02:42:ce:f1:fc:32  txqueuelen 0  (Ethernet)
        RX packets 2827  bytes 115724 (115.7 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 3853  bytes 10144700 (10.1 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```



原来网桥的网络配置的subnet就是172.17.0.0，网关则是172.17.0.1。这个网关就在docker0上，所以当前容器网络拓扑图如下：

![img](https://oss.xubighead.top/oss/image/202506/1930511856713175042.jpg)



#### veth-pair

veth-pair是一种虚拟设备接口，它是成对出现的，一端连着协议栈，一端彼此相连着，在本例中大家可以想象成它们是由一根虚拟网线连接起来的一对网卡，网卡的一头（eth0@if46）在容器里，另一头（vethe21ab12）挂在网桥docker0上，最终的效果就是把eth0@if46也挂在docker0上。

veth-pair 就是一对的虚拟设备接口，它都是成对出现的。一端连着协议栈，一端彼此相连着，因为这个特性，它常常充当着一个桥梁，连接着各种虚拟网络设备，典型的例子像“两个 namespace 之间的连接”，“Bridge、OVS 之间的连接”，“Docker 容器之间的连接” 等等，以此构建出非常复杂的虚拟网络结构，比如 OpenStack Neutron。

![img](https://oss.xubighead.top/oss/image/202506/1930511874383777793.jpg)



## Custom Network

Docker允许创建自定义网络，提供了三种网络驱动供我们选择：bridge、macvlan和overlay，其中macvlan和ovelay都是用于创建跨主机网络。



### Create Custom Network

通过docker network create命令创建test_net1网络：

```sh
$ sudo docker network create --driver bridge test_net1
c7c20444a940135c92958f4434ca2b7428ba17f70a2a8a954a6cf160a011b513
$ sudo docker network ls
NETWORK ID     NAME        DRIVER    SCOPE
6f0087fd32cd   bridge      bridge    local
464b3d11003c   host        host      local
faa8eb8310b4   none        null      local
c7c20444a940   test_net1   bridge    local
$ brctl show
bridge name  bridge id    STP enabled  interfaces
br-c7c20444a940    8000.024200e01cd2  no
docker0    8000.0242cef1fc32  no
```



此时系统中新增了一个网桥br-c7c20444a940，其中c7c20444a940就是我们刚刚创建的桥接网络的ID。我们可以看下test_net1的详细配置信息：

```sh
$ sudo docker network inspect test_net1
[
    {
        "Name": "test_net1",
        "Id": "c7c20444a940135c92958f4434ca2b7428ba17f70a2a8a954a6cf160a011b513",
        "Created": "2021-10-23T07:07:47.870947684Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "172.19.0.0/16",
                    "Gateway": "172.19.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {},
        "Options": {},
        "Labels": {}
    }
]
```



如果在创建网络时，不指定--subnet和--gateway参数时，Docker会自动为其分配网段和网关。

创建一个网络并指定其网段和网关：

```sh
$ sudo docker network create --driver bridge --subnet 173.20.0.0/16 --gateway 173.20.0.1  test_net2
47542e86dc4474779a3b105f1d22f005fc39a529b1f19aa7af2c68297c1c0a41
$ sudo docker network inspect test_net2
[
    {
        "Name": "test_net2",
        "Id": "47542e86dc4474779a3b105f1d22f005fc39a529b1f19aa7af2c68297c1c0a41",
        "Created": "2021-10-23T07:21:47.684305897Z",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "173.20.0.0/16",
                    "Gateway": "173.20.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {},
        "Options": {},
        "Labels": {}
    }
]
$ brctl show
bridge name  bridge id    STP enabled  interfaces
br-47542e86dc44    8000.024273a34080  no
br-c7c20444a940    8000.024200e01cd2  no
docker0    8000.0242cef1fc32  no
$ ifconfig br-47542e86dc44
br-47542e86dc44: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
        inet 173.20.0.1  netmask 255.255.0.0  broadcast 173.20.255.255
        ether 02:42:73:a3:40:80  txqueuelen 0  (Ethernet)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```



### Use Custom Network

容器要使用自定义的网络，同样需在启动时通过--network参数指定即可：

```sh
$ sudo docker run -it --network=test_net2 busybox
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
50: eth0@if51: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:42:ad:14:00:02 brd ff:ff:ff:ff:ff:ff
    inet 173.20.0.2/16 brd 173.20.255.255 scope global eth0
       valid_lft forever preferred_lft forever
```



Docker会根据容器所使用的网络的网段自动为容器分配IP，当然，容器IP也是可以指定的，即通过--ip参数，但是要注意两点：

- 只有使用--subnet参数指定网段创建的网络才能指定IP
- 所指定的IP一定要在所使用的网络的网段中



下图所示的错误就是容器使用的网络并没有通过--subnet指定网段：

```sh
$ sudo docker run -it --network=test_net1 --ip 172.19.0.5  busybox
docker: Error response from daemon: user specified IP address is supported only when connecting to networks with user configured subnets.
```



下图所示的错误是容器所指定IP不在其网络的指定网段内：

```sh
$ sudo docker run -it --network=test_net2 --ip 172.20.0.2  busybox
ERRO[0000] error waiting for container: context canceled
docker: Error response from daemon: Invalid address 172.20.0.2: It does not belong to any of this network's subnets.
```



以下是正确的指定容器IP的例子：

```sh
$ sudo docker run -it --network=test_net2 --ip 173.20.0.4  busybox
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
54: eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:42:ad:14:00:04 brd ff:ff:ff:ff:ff:ff
    inet 173.20.0.4/16 brd 173.20.255.255 scope global eth0
       valid_lft forever preferred_lft forever
```



### Network Isolate And Principle

目前使用test_net2网络创建了两个容器，这两个容器网络应该是互通的，验证一下：

```sh
$ sudo docker run -it --network=test_net2 --ip 173.20.0.4  busybox
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
54: eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:42:ad:14:00:04 brd ff:ff:ff:ff:ff:ff
    inet 173.20.0.4/16 brd 173.20.255.255 scope global eth0
       valid_lft forever preferred_lft forever
/ # ping 173.20.0.2
PING 173.20.0.2 (173.20.0.2): 56 data bytes
64 bytes from 173.20.0.2: seq=0 ttl=64 time=1.319 ms
64 bytes from 173.20.0.2: seq=1 ttl=64 time=0.106 ms
64 bytes from 173.20.0.2: seq=2 ttl=64 time=0.107 ms
^C
--- 173.20.0.2 ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.106/0.510/1.319 ms
```



所以，使用同一网络创建的容器是能够互通的。相反，如果是不同网络创建的容器应该是无法通信，使用test_net1创建一个容器验证一下：

```sh
$ sudo docker run -it --network=test_net1 busybox
/ # ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
58: eth0@if59: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:42:ac:13:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.19.0.2/16 brd 172.19.255.255 scope global eth0
       valid_lft forever preferred_lft forever
/ # ping 173.20.0.2
PING 173.20.0.2 (173.20.0.2): 56 data bytes
^C
--- 173.20.0.2 ping statistics ---
146 packets transmitted, 0 packets received, 100% packet loss
```



从结果来看确实无法通信，但是如果开启系统的ip forwarding，把当前主机当作路由，那么不同网络应该是能通信的，首先来看下系统的配置：

```sh
$ ip r
default via 172.16.194.2 dev ens33 proto dhcp src 172.16.194.135 metric 100
172.16.194.0/24 dev ens33 proto kernel scope link src 172.16.194.135
172.16.194.2 dev ens33 proto dhcp scope link src 172.16.194.135 metric 100
172.19.0.0/16 dev br-c7c20444a940 proto kernel scope link src 172.19.0.1
173.20.0.0/16 dev br-47542e86dc44 proto kernel scope link src 173.20.0.1
$ sysctl net.ipv4.ip_forward
net.ipv4.ip_forward = 1
```



主机已经开启了ip forwarding并且172.19.0.0/16和173.20.0.0/16的路由也定义好了，那为什么网络还是不通呢？看看iptables：

```sh
$ sudo iptables-save
# Generated by iptables-save v1.6.1 on Sat Oct 23 08:19:03 2021
*filter
...
# 如果数据包是从br-47542e86dc44流入但不从br-47542e86dc44流出则跳转到DOCKER-ISOLATION-STAGE-2处理
-A DOCKER-ISOLATION-STAGE-1 -i br-47542e86dc44 ! -o br-47542e86dc44 -j DOCKER-ISOLATION-STAGE-2
# 如果数据包是从br-c7c20444a940流入但不从br-c7c20444a940流出则跳转到DOCKER-ISOLATION-STAGE-2处理
-A DOCKER-ISOLATION-STAGE-1 -i br-c7c20444a940 ! -o br-c7c20444a940 -j DOCKER-ISOLATION-STAGE-2
# 如果数据包是从docker0流入但不从docker0流出则跳转到DOCKER-ISOLATION-STAGE-2处理
-A DOCKER-ISOLATION-STAGE-1 -i docker0 ! -o docker0 -j DOCKER-ISOLATION-STAGE-2
# 如果非以上三种情况则返回上一层处理
-A DOCKER-ISOLATION-STAGE-1 -j RETURN

# 如果数据包流出到br-47542e86dc44则drop
-A DOCKER-ISOLATION-STAGE-2 -o br-47542e86dc44 -j DROP
# 如果数据包流出到br-c7c20444a940则drop
-A DOCKER-ISOLATION-STAGE-2 -o br-c7c20444a940 -j DROP
# 如果数据包流出到docker0则drop
-A DOCKER-ISOLATION-STAGE-2 -o docker0 -j DROP
# 如果非以上三种情况则返回上一层处理
-A DOCKER-ISOLATION-STAGE-2 -j RETURN
-A DOCKER-USER -j RETURN
...
```



Docker创建了一系列网络隔离策略，当数据包流入的网络与流出的网络相同则不作处理，不同则弃数据包，通过一个流程图来说明DOCKER-ISOLATION-STAGE-1和DOCKER-ISOLATION-STAGE-2规则的逻辑：

![img](https://oss.xubighead.top/oss/image/202506/1930512100179939330.jpg)



如果我一定要两个使用不同网络的容器通信，只需要给容器添加对应的网络的的网卡即可，通过docker network connect命令可以实现：

```sh
# 0da5e260a4b6容器使用的是test_net1，另外两个使用的是test_net2，
# 如果不确定的话可以使用docker inspect 容器id进行确定 
$ sudo docker ps
CONTAINER ID   IMAGE     COMMAND   CREATED       STATUS       PORTS     NAMES
0da5e260a4b6   busybox   "sh"      2 hours ago   Up 2 hours             objective_blackburn
c6871db3efbb   busybox   "sh"      2 hours ago   Up 2 hours             goofy_swartz
c683eacc4eae   busybox   "sh"      2 hours ago   Up 2 hours             modest_swartz
yangye@ayato:~$ sudo docker network connect test_net2 0da5e260a4b6
```



在容器0da5e260a4b6中先ping下173.20.0.2发现网络是通的，再看看其网络配置，发现容器中添加了my_net2网络的网卡eth1：

```sh
/ # ping 173.20.0.2
PING 173.20.0.2 (173.20.0.2): 56 data bytes
64 bytes from 173.20.0.2: seq=0 ttl=64 time=0.291 ms
64 bytes from 173.20.0.2: seq=1 ttl=64 time=0.100 ms
64 bytes from 173.20.0.2: seq=2 ttl=64 time=0.097 ms
64 bytes from 173.20.0.2: seq=3 ttl=64 time=0.100 ms
64 bytes from 173.20.0.2: seq=4 ttl=64 time=0.147 ms
^C
--- 173.20.0.2 ping statistics ---
5 packets transmitted, 5 packets received, 0% packet loss
round-trip min/avg/max = 0.097/0.147/0.291 ms
/ # ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:AC:13:00:02
          inet addr:172.19.0.2  Bcast:172.19.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:25 errors:0 dropped:0 overruns:0 frame:0
          TX packets:150 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:1830 (1.7 KiB)  TX bytes:14476 (14.1 KiB)

eth1      Link encap:Ethernet  HWaddr 02:42:AD:14:00:03
          inet addr:173.20.0.3  Bcast:173.20.255.255  Mask:255.255.0.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:18 errors:0 dropped:0 overruns:0 frame:0
          TX packets:7 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:1440 (1.4 KiB)  TX bytes:574 (574.0 B)

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
```



## Communication

### Before Communication

通常一个 Web 项目上线，我们会把开发完成的服务部署在Tomcat 服务器里面，然后需要的持久化数据会存放在数据库 Mysql，那么在服务运行时，少不了 Tomcat 和 Mysql 的交互。

对应的，应用到 Docker 中，就是 Tomcat 容器和 Mysql 容器间的交互。



#### Build Tomcat Image

创建Tomcat镜像：

```dockerfile
FROM tomcat:latest
WORKDIR /usr/local/tomcat/webapps
COPY ./webapps/ /usr/local/tomcat/webapps/
RUN apt update && apt install -y iproute2 && apt install -y iputils-ping && apt install -y vim
```



这是制作 Tomcat 镜像的 Dockerfile，因为目前最新版的官方 Tomcat 镜像没有一个网络查看命令，所以需要手动安装。构建Tomcat镜像命令如下：

```bash
$ docker build -f Dockerfile -t myNamespace/mytomcat8:1.0 .
```



启动并进入Tomcat容器：

```bash
$ docker run -it -p 8080:8080 --name tomcat1 myNamespace/mytomcat8:1.0 /bin/bash
```



#### Build MySQL Image

创建MySQL镜像：

```dockerfile
FROM mysql:8.0
RUN apt update && apt install -y iproute2 && apt install -y iputils-ping && apt install -y vim
```



构建MySQL镜像命令：

```bash
$ docker build -f Dockerfile -t myNamespace/mysql8:1.0 .
```



启动并进入MySQL容器：

```bash
$ docker run -it -p 3306:3306 --name mysql1 myNamespace/mysql8:1.0 /bin/bash
```



### By IP

IP通信很简单，只要容器使用相同网络，那么就可以使用IP进行访问。

通过如下`ip addr`命令获取MySQL容器IP地址172.17.0.3，并在Tomcat容器ping MySQL容器：

![img](https://oss.xubighead.top/oss/image/202506/1930511925122273281.jpg)



通过 IP 通信，我们看似解决了容器间通信的问题，但在实际生产中，我们容器是会经常重新启动的，而上面的容器 IP 是Docker 分配的虚拟IP，这个IP是会变得，假设我们每次重新构建一个容器，那就要重新修改服务配置IP，生产环境会有几十个几百个容器，都要进行修改，这将是很麻烦的。



### By Container Name

#### Container One-way Communication

在启动容器的时候通过增加 --link 容器名参数：

```bash
$ docker run -it -p 8080:8080 --name tomcat1 --link mysql1 3336fdaf451a /bin/bash
```



然后在 tomcat1 容器ping mysql1 ：

![img](https://oss.xubighead.top/oss/image/202506/1930511952448163841.jpg)



为什么说是单向通信，如果启动 mysql1 容器的时候没有增加--link 参数，则 mysql1 访问不了 tomcat1。

查看tomcat1 容器的 /etc/hosts 文件，发现 --link 就是增加了名字解析：

![img](https://oss.xubighead.top/oss/image/202506/1930511982655541250.jpg)



而mysql1 容器的 /etc/hosts 则没有名字解析。



使用IP通信存在一个最大的问题就是容器的IP地址一般是随机的不固定的，所以导致这种方式不够灵活，对于这个问题，可以通过Docker自带的DNS服务器解决。这种方式使用也很简单，即启动容器时通过--name参数即可指定其域名。我们使用前面创建的test_net1，启动两个nginx容器：

sudo docker run -d --network=test_net1 --name=ng1 nginx

sudo docker run -d --network=test_net1 --name=ng2 nginx

接着我们进入ng1，然后尝试ping一下ng2:

```sh
# ping ng2
PING ng2 (172.18.0.3) 56(84) bytes of data.
64 bytes from ng2.test_net1 (172.18.0.3): icmp_seq=1 ttl=64 time=0.094 ms
64 bytes from ng2.test_net1 (172.18.0.3): icmp_seq=2 ttl=64 time=0.101 ms
64 bytes from ng2.test_net1 (172.18.0.3): icmp_seq=3 ttl=64 time=0.097 ms
64 bytes from ng2.test_net1 (172.18.0.3): icmp_seq=4 ttl=64 time=0.086 ms
^C
--- ng2 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 59ms
rtt min/avg/max/mdev = 0.086/0.094/0.101/0.011 ms
```



使用Docker DNS有个限制：只能在用户自定义网络中使用。也就是说默认的Bridge网络是无法使用的。

```sh
$ sudo docker run -d  --name ng3 nginx
53fd36e0e778bc0b29ae70ac3b5c5d76c74b4a8d987b23290a868f7f0f02d435
$ sudo docker run -d  --name ng4 nginx
c91d26a3b948089a1b19db08b6a719c101e5bef4007c9d727662c2363a3c6d9f
$ sudo docker exec -it 53fd36e0e778bc0b29ae70ac3b5c5d76c74b4a8d987b23290a868f7f0f02d435 /bin/sh
# ping ng4
ping: ng4: Name or service not known
```



#### Container Two-way Communication

多个容器之间通信依赖 veth-pair 技术：

![img](https://oss.xubighead.top/oss/image/202506/1930512012049223681.jpg)

容器间双向通信其实就是利用网桥链接新创建的容器和宿主机，上面图片的 docker0 就是一个网桥。**使用自定义的网桥可以控制哪些容器可以互相通信，可以通过容器名通信（自动DNS解析名称到IP地址，这个docker0是不支持的）。**



##### By Single Bridge

- 创建自定义网桥

```bash
$ docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 myBridge
```



- 启动容器

```bash
docker run -it -p 8080:8080 --name tomcat1 --net myBridge 3336fdaf451a /bin/bash
docker run -it -p 3306:3306 --name mysql1 --net myBridge adaa6a5d739c /bin/bash
```



![img](https://oss.xubighead.top/oss/image/202506/1930512031682760705.jpg)



##### By Multi Bridge

![img](https://oss.xubighead.top/oss/image/202506/1930512055774842882.jpg)



如果想要网桥内的容器能够跨网桥进行互相通信，则需要将容器连接到对应的网桥上。

```bash
$ docker network connect [OPTIONS] NETWORK CONTAINER
```



比如，我们要把默认网桥 docker0 上面的 tomcat1-docker0 容器能链接 myBridge 网桥里面的容器，只需要执行以下命令即可。

```bash
$ docker network connect myBridge tomcat1-docker0
```



然后进入 tomcat1-docker0 容器，发现可以 ping 通 myBridge 网桥里面的容器了。并且查看 tomcat1-docker0 容器的ip，你会发现有两个 ip了，也就是一个容器，多个ip。



### By Joined Container

joined容器是另一种实现容器间通信的方式，这种方式非常特别，它可以使多个容器共享同一个网络栈，共享网卡和配置信息。

joined容器方式非常适合以下场景：

- 不同容器中的程序希望通过loopback高效快速通信，比如Web Server和App Server；
- 安装在独立容器中的监控服务对其它容器中应用进行监控。



首先启动一个nginx容器：

```sh
$ sudo docker run -d -it --name ng nginx
128be82cc38adb2bc56ee4e8aa2a7e7026601c6cd056af6848539f5e27b6210d
```



接着启动redis容器，并通过--network参数join到nginx容器网络中，我们进入容器看下其网络配置：

```sh
sudo docker run -it -d --network=container:ng redis
344d10125f1693d96bea471c94a13c36f8c868b203dbe234cbb8863fec24f775
$ sudo docker exec -it 344d10125f1693d96bea471c94a13c36f8c868b203dbe234cbb8863fec24f775 /bin/sh
注意此处请大家先自行安装iproute2工具
# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
43: eth0@if44: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
```



接着，我们在进入nginx容器中看下它的网络配置：

```sh
$ sudo docker exec -it 128be82cc38a /bin/sh
# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
43: eth0@if44: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
```



这两个容器网络配置一样的，它们共享相同的网络栈，这样redis容器可以直接通过127.0.0.1访问nginx：

```sh
# curl 127.0.0.1:80
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```