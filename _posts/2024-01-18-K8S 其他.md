---
layout: post
title: K8S 其他.md
categories: [Kubernetes 容器编排]
description: 
keywords: K8S 其他.md
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---
# ectd

etcd 是兼顾一致性与高可用性的键值对数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。保持 etcd 集群的稳定对 Kubernetes 集群的稳定性至关重要。



## Introduction

etcd是使用Go语言开发的一个开源的、高可用的分布式key-value存储系统，可以用于配置共享和服务的注册和发现。类似项目有zookeeper和consul。
etcd具有以下特点：

- 完全复制：集群中的每个节点都可以使用完整的存档
- 高可用性：Etcd可用于避免硬件的单点故障或网络问题
- 一致性：每次读取都会返回跨多主机的最新写入
- 简单：包括一个定义良好、面向用户的API（gRPC）
- 安全：实现了带有可选的客户端证书身份验证的自动化TLS
- 快速：每秒10000次写入的基准速度
- 可靠：使用Raft算法实现了强一致、高可用的服务存储目录

Kubernetes 集群对etcd集群有几点要求：

- 运行的 etcd 集群个数成员为奇数。
- etcd 是一个 leader-based 分布式系统。确保主节点定期向所有从节点发送心跳，以保持集群稳定。
- 确保不发生资源不足：集群的性能和稳定性对网络和磁盘 I/O 非常敏感。任何资源匮乏都会导致心跳超时， 从而导致集群的不稳定。不稳定的情况表明没有选出任何主节点。 在这种情况下，集群不能对其当前状态进行任何更改，这意味着不能调度新的 Pod。
- 保持 etcd 集群的稳定对 Kubernetes 集群的稳定性至关重要。 因此，请在专用机器或隔离环境上运行 etcd 集群， 以满足所需资源需求。
- 在生产中运行的 etcd 的最低推荐版本是 3.2.10+。



## API

### V2

etcd写入数据的时候有两个版本：2版本和3版本，默认是2版本，Set environment variable ETCDCTL_API=3 to use v3 API or ETCDCTL_API=2 to use v2 API

现在还没有数据

```shell
[root@etcd1 ~]# etcdctl ls /
```

etcd1创建数据

```shell
[root@etcd1 ~]# etcdctl mkdir /comeon

[root@etcd1 ~]# etcdctl ls /
/comeon
```

客户端连接也可以看到数据

```shell
[root@etcd2 ~]# etcdctl --endpoints http://192.168.110.133:2379 ls /
/comeon
```

删除数据

```shell
[root@etcd1 ~]# etcdctl rmdir /comeon

[root@etcd1 ~]# etcdctl ls /
```



### V3

现在使用3版本的API

```shell
[root@etcd1 ~]# export ETCDCTL_API=3
```

设置为3版本的API之后，help显示的内容也会不同

```shell
[root@etcd1 ~]# etcdctl --help
NAME:
        etcdctl - A simple command line client for etcd3.

USAGE:
        etcdctl

VERSION:
        3.3.11

API VERSION:
        3.3


COMMANDS:
        get			Gets the key or a range of keys
        put			Puts the given key into the store
        del			Removes the specified key or range of keys [key, range_end)
        txn			Txn processes all the requests in one transaction
        compaction		Compacts the event history in etcd
        alarm disarm		Disarms all alarms
        alarm list		Lists all alarms
        defrag			Defragments the storage of the etcd members with given endpoints
        endpoint health		Checks the healthiness of endpoints specified in `--endpoints` flag
        endpoint status		Prints out the status of endpoints specified in `--endpoints` flag
        endpoint hashkv		Prints the KV history hash for each endpoint in --endpoints
        move-leader		Transfers leadership to another etcd cluster member.
        watch			Watches events stream on keys or prefixes
        version			Prints the version of etcdctl
        lease grant		Creates leases
        lease revoke		Revokes leases
        lease timetolive	Get lease information
        lease list		List all active leases
        lease keep-alive	Keeps leases alive (renew)
        member add		Adds a member into the cluster
        member remove		Removes a member from the cluster
        member update		Updates a member in the cluster
        member list		Lists all members in the cluster
        snapshot save		Stores an etcd node backend snapshot to a given file
        snapshot restore	Restores an etcd member snapshot to an etcd directory
        snapshot status		Gets backend snapshot status of a given file
        make-mirror		Makes a mirror at the destination etcd cluster
        migrate			Migrates keys in a v2 store to a mvcc store
        lock			Acquires a named lock
        elect			Observes and participates in leader election
        auth enable		Enables authentication
        auth disable		Disables authentication
        user add		Adds a new user
        user delete		Deletes a user
        user get		Gets detailed information of a user
        user list		Lists all users
        user passwd		Changes password of user
        user grant-role		Grants a role to a user
        user revoke-role	Revokes a role from a user
        role add		Adds a new role
        role delete		Deletes a role
        role get		Gets detailed information of a role
        role list		Lists all roles
        role grant-permission	Grants a key to a role
        role revoke-permission	Revokes a key from a role
        check perf		Check the performance of the etcd cluster
        help			Help about any command

......
  -w, --write-out="simple"			set the output format (fields, json, protobuf, simple, table)
```

写数据

```shell
[root@etcd1 ~]# etcdctl put student1 99
OK
```

查数据

```shell
[root@etcd1 ~]# etcdctl get student1
student1
99
```

**注意**：2版本API和3版本API不可互用，从k8s1.5版本，etcd就开始使用3版本往etcd里写数据

```shell
[root@etcd2 ~]# export ETCDCTL_API=3

[root@etcd2 ~]# etcdctl --endpoints http://192.168.110.133:2379 get student1
student1
99
```

etcd单节点搭建完毕，接下来添加两个节点变为etcd集群。



## Function

### Backup Data By Snapshot

生产环境中，有些重要数据是要备份的，以免造成数据丢失。**etcd对数据做快照可以进行数据备份**。

设置API版本为3

```shell
[root@etcd1 ~]# export ETCDCTL_API=3
```

为了避免数据丢失，可以进行数据备份，即数据做快照，数据快照的help如下

```shell
[root@etcd1 ~]# etcdctl snap --help
NAME:
        snapshot - Manages etcd node snapshots

USAGE:
        etcdctl snapshot <subcommand>

API VERSION:
        3.3


COMMANDS:
        save	Stores an etcd node backend snapshot to a given file
        restore	Restores an etcd member snapshot to an etcd directory
        status	Gets backend snapshot status of a given file

GLOBAL OPTIONS:
      --cacert=""				verify certificates of TLS-enabled secure servers using this CA bundle
      --cert=""					identify secure client using this TLS certificate file
      --command-timeout=5s			timeout for short running command (excluding dial timeout)
      --debug[=false]				enable client-side debug logging
      --dial-timeout=2s				dial timeout for client connections
  -d, --discovery-srv=""			domain name to query for SRV records describing cluster endpoints
      --endpoints=[127.0.0.1:2379]		gRPC endpoints
      --hex[=false]				print byte strings as hex encoded strings
      --insecure-discovery[=true]		accept insecure SRV records describing cluster endpoints
      --insecure-skip-tls-verify[=false]	skip server certificate verification
      --insecure-transport[=true]		disable transport security for client connections
      --keepalive-time=2s			keepalive time for client connections
      --keepalive-timeout=6s			keepalive timeout for client connections
      --key=""					identify secure client using this TLS key file
      --user=""					username[:password] for authentication (prompt if password is not supplied)
  -w, --write-out="simple"			set the output format (fields, json, protobuf, simple, table)
```

进行快照

```shell
[root@etcd1 ~]# etcdctl snap save student.data
Snapshot saved at student.data
```



### Data Recovery

删除数据使用快照进行恢复

```shell
[root@etcd1 ~]# etcdctl del student1
1
[root@etcd1 ~]# etcdctl del student2
1

[root@etcd1 ~]# etcdctl get student1

[root@etcd1 ~]# etcdctl get student2
```

使用快照恢复数据的时候需要把快照复制到其他节点

```shell
[root@etcd1 ~]# scp student.data etcd2:~/
root@etcd2's password: 
student.data                                                                                                                                                                    100%   20KB  14.5MB/s   00:00    
[root@etcd1 ~]# scp student.data etcd3:~/
root@etcd3's password: 
student.data                                                                                                                                                                    100%   20KB   8.1MB/s   00:00   
```

在所有节点停止etcd并清空数据

```shell
[root@etcd1 ~]# systemctl stop etcd
[root@etcd1 ~]# rm -rf /var/lib/etcd/*
[root@etcd1 ~]# chown etcd:etcd student.data 

[root@etcd2 ~]# systemctl stop etcd
[root@etcd2 ~]# rm -rf /var/lib/etcd/*
[root@etcd2 ~]# chown etcd:etcd student.data

[root@etcd3 ~]# systemctl stop etcd
[root@etcd3 ~]# rm -rf /var/lib/etcd/*
[root@etcd3 ~]# chown etcd:etcd student.data
```

在每个节点上使用快照恢复数据

```shell
[root@etcd1 ~]# etcdctl snapshot restore student.data --name etcd133 --initial-cluster etcd133=http://192.168.110.133:2380,etcd131=http://192.168.110.131:2380,etcd132=http://192.168.110.132:2380 --initial-advertise-peer-urls http://192.168.110.133:2380 --data-dir /var/lib/etcd/cluster.etcd
2022-01-11 16:44:08.375319 I | etcdserver/membership: added member 341a3c460c1c993a [http://192.168.110.131:2380] to cluster dd7594df5e81191b
2022-01-11 16:44:08.375393 I | etcdserver/membership: added member 4679fe0fcb37326d [http://192.168.110.132:2380] to cluster dd7594df5e81191b
2022-01-11 16:44:08.375404 I | etcdserver/membership: added member ab23bcc86cf3190b [http://192.168.110.133:2380] to cluster dd7594df5e81191b

[root@etcd2 ~]# etcdctl snapshot restore student.data --name etcd131 --initial-cluster etcd133=http://192.168.110.133:2380,etcd131=http://192.168.110.131:2380,etcd132=http://192.168.110.132:2380 --initial-advertise-peer-urls http://192.168.110.131:2380 --data-dir /var/lib/etcd/cluster.etcd
2022-01-11 16:45:18.378931 I | etcdserver/membership: added member 341a3c460c1c993a [http://192.168.110.131:2380] to cluster dd7594df5e81191b
2022-01-11 16:45:18.378991 I | etcdserver/membership: added member 4679fe0fcb37326d [http://192.168.110.132:2380] to cluster dd7594df5e81191b
2022-01-11 16:45:18.379000 I | etcdserver/membership: added member ab23bcc86cf3190b [http://192.168.110.133:2380] to cluster dd7594df5e81191b

[root@etcd3 ~]# etcdctl snapshot restore student.data --name etcd132 --initial-cluster etcd133=http://192.168.110.133:2380,etcd131=http://192.168.110.131:2380,etcd132=http://192.168.110.132:2380 --initial-advertise-peer-urls http://192.168.110.132:2380 --data-dir /var/lib/etcd/cluster.etcd
2022-01-11 16:46:26.826533 I | etcdserver/membership: added member 341a3c460c1c993a [http://192.168.110.131:2380] to cluster dd7594df5e81191b
2022-01-11 16:46:26.826584 I | etcdserver/membership: added member 4679fe0fcb37326d [http://192.168.110.132:2380] to cluster dd7594df5e81191b
2022-01-11 16:46:26.826595 I | etcdserver/membership: added member ab23bcc86cf3190b [http://192.168.110.133:2380] to cluster dd7594df5e81191b
```

修改所有节点数据目录的属主

```shell
[root@etcd1 ~]# chown -R etcd:etcd /var/lib/etcd/

[root@etcd2 ~]# chown -R etcd:etcd /var/lib/etcd/

[root@etcd3 ~]# chown -R etcd:etcd /var/lib/etcd/
```

启动etcd

```shell
[root@etcd1 ~]# systemctl start etcd

[root@etcd2 ~]# systemctl start etcd

[root@etcd3 ~]# systemctl start etcd
```

可以发现数据已经恢复

```shell
[root@etcd1 ~]# etcdctl get student1
student1
59

[root@etcd3 ~]# etcdctl get student2
student2
62
```



## Install

### Install Single Node

安装etcd单节点，etcd架构：etcd1机器作为etcd的服务端，etcd2机器作为客户端访问。

| 服务器                | 操作系统版本                         | CPU架构 | 进程 | 功能描述   |
| --------------------- | ------------------------------------ | ------- | ---- | ---------- |
| etcd1/192.168.110.133 | CentOS Linux release 7.4.1708 (Core) | x86_64  | etcd | etcd服务端 |
| etcd2/192.168.110.131 | CentOS Linux release 7.4.1708 (Core) | x86_64  | etcd | etcd客户端 |



#### Install Basic Env

先配置节点的基本环境，所有节点都要同时设置，在此以etcd1作为示例

首先设置主机名

```shell
[root@localhost ~]# cat /etc/hostname
etcd1
```

配置IP地址（可选）

```shell
[root@localhost ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32

[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens32
TYPE=Ethernet
BOOTPROTO=static
NAME=ens32
DEVICE=ens32
ONBOOT=yes
DNS1=114.114.114.114
IPADDR=192.168.110.133
NETMASK=255.255.255.0
GATEWAY=192.168.110.2
ZONE=trusted

#重启网卡
[root@localhost ~]# service network restart
Restarting network (via systemctl):                        [  确定  ]

[root@localhost ~]# systemctl restart NetworkManager
```

重启机器之后看是否能访问网络

```shell
[root@etcd1 ~]# ping www.baidu.com
PING www.a.shifen.com (14.215.177.38) 56(84) bytes of data.
64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=1 ttl=128 time=31.1 ms
64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=2 ttl=128 time=30.5 ms
64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=3 ttl=128 time=31.9 ms
64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=4 ttl=128 time=30.6 ms
^C
--- www.a.shifen.com ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3007ms
rtt min/avg/max/mdev = 30.593/31.075/31.926/0.533 ms
```

配置IP和主机名映射

```shell
[root@etcd1 ~]# vim /etc/hosts

[root@etcd1 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.110.133 etcd1
192.168.110.131 etcd2
192.168.110.132 etcd3
```

复制到另外两个主机

```shell
[root@etcd1 ~]# scp /etc/hosts etcd3:/etc/hosts
root@etcd3's password: 
hosts                                                                                                                                                                          100%  224   281.3KB/s   00:00    

[root@etcd1 ~]# scp /etc/hosts etcd2:/etc/hosts
root@etcd2's password: 
hosts                                                                                                                                                                          100%  224   218.1KB/s   00:00
```

能相互ping通则正常

```shell
[root@etcd1 ~]# ping etcd1
PING etcd1 (192.168.110.133) 56(84) bytes of data.
64 bytes from etcd1 (192.168.110.133): icmp_seq=1 ttl=64 time=0.029 ms
64 bytes from etcd1 (192.168.110.133): icmp_seq=2 ttl=64 time=0.033 ms
64 bytes from etcd1 (192.168.110.133): icmp_seq=3 ttl=64 time=0.043 ms
^C
--- etcd1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2002ms
rtt min/avg/max/mdev = 0.029/0.035/0.043/0.005 ms

[root@etcd1 ~]# ping etcd2
PING etcd2 (192.168.110.131) 56(84) bytes of data.
64 bytes from etcd2 (192.168.110.131): icmp_seq=1 ttl=64 time=1.61 ms
64 bytes from etcd2 (192.168.110.131): icmp_seq=2 ttl=64 time=1.92 ms
^C
--- etcd2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1010ms
rtt min/avg/max/mdev = 1.612/1.769/1.926/0.157 ms

[root@etcd1 ~]# ping etcd3
PING etcd3 (192.168.110.132) 56(84) bytes of data.
64 bytes from etcd3 (192.168.110.132): icmp_seq=1 ttl=64 time=0.484 ms
64 bytes from etcd3 (192.168.110.132): icmp_seq=2 ttl=64 time=2.65 ms
64 bytes from etcd3 (192.168.110.132): icmp_seq=3 ttl=64 time=2.65 ms
^C
--- etcd3 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2021ms
rtt min/avg/max/mdev = 0.484/1.931/2.657/1.023 ms
```

关闭屏保（可选）

```shell
[root@etcd1 ~]# setterm -blank 0
```

配置yum源

```shell
[root@etcd1 ~]# rm -rf /etc/yum.repos.d/* ;wget ftp://ftp.rhce.cc/k8s/* -P /etc/yum.repos.d/
```

关闭selinux

```shell
[root@etcd1 ~]# cat /etc/selinux/config 

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=disabled
# SELINUXTYPE= can take one of three two values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected. 
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted 

[root@etcd1 ~]# getenforce
Disabled
```

配置防火墙允许所有数据通过

```shell
[root@etcd1 ~]# firewall-cmd --set-default-zone=trusted
Warning: ZONE_ALREADY_SET: trusted
success

[root@etcd1 ~]# firewall-cmd --get-default-zone
trusted
```



#### Install ectd

先配置节点的基本环境，所有节点都要同时设置，在此以etcd1作为示例

首先设置主机名

```shell
[root@localhost ~]# cat /etc/hostname
etcd1
```

配置IP地址（可选）

```shell
[root@localhost ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens32

[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-ens32
TYPE=Ethernet
BOOTPROTO=static
NAME=ens32
DEVICE=ens32
ONBOOT=yes
DNS1=114.114.114.114
IPADDR=192.168.110.133
NETMASK=255.255.255.0
GATEWAY=192.168.110.2
ZONE=trusted

#重启网卡
[root@localhost ~]# service network restart
Restarting network (via systemctl):                        [  确定  ]

[root@localhost ~]# systemctl restart NetworkManager
```

重启机器之后看是否能访问网络

```shell
[root@etcd1 ~]# ping www.baidu.com
PING www.a.shifen.com (14.215.177.38) 56(84) bytes of data.
64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=1 ttl=128 time=31.1 ms
64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=2 ttl=128 time=30.5 ms
64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=3 ttl=128 time=31.9 ms
64 bytes from 14.215.177.38 (14.215.177.38): icmp_seq=4 ttl=128 time=30.6 ms
^C
--- www.a.shifen.com ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3007ms
rtt min/avg/max/mdev = 30.593/31.075/31.926/0.533 ms
```

配置IP和主机名映射

```shell
[root@etcd1 ~]# vim /etc/hosts

[root@etcd1 ~]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.110.133 etcd1
192.168.110.131 etcd2
192.168.110.132 etcd3
```

复制到另外两个主机

```shell
[root@etcd1 ~]# scp /etc/hosts etcd3:/etc/hosts
root@etcd3's password: 
hosts                                                                                                                                                                          100%  224   281.3KB/s   00:00    

[root@etcd1 ~]# scp /etc/hosts etcd2:/etc/hosts
root@etcd2's password: 
hosts                                                                                                                                                                          100%  224   218.1KB/s   00:00
```

能相互ping通则正常

```shell
[root@etcd1 ~]# ping etcd1
PING etcd1 (192.168.110.133) 56(84) bytes of data.
64 bytes from etcd1 (192.168.110.133): icmp_seq=1 ttl=64 time=0.029 ms
64 bytes from etcd1 (192.168.110.133): icmp_seq=2 ttl=64 time=0.033 ms
64 bytes from etcd1 (192.168.110.133): icmp_seq=3 ttl=64 time=0.043 ms
^C
--- etcd1 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2002ms
rtt min/avg/max/mdev = 0.029/0.035/0.043/0.005 ms

[root@etcd1 ~]# ping etcd2
PING etcd2 (192.168.110.131) 56(84) bytes of data.
64 bytes from etcd2 (192.168.110.131): icmp_seq=1 ttl=64 time=1.61 ms
64 bytes from etcd2 (192.168.110.131): icmp_seq=2 ttl=64 time=1.92 ms
^C
--- etcd2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1010ms
rtt min/avg/max/mdev = 1.612/1.769/1.926/0.157 ms

[root@etcd1 ~]# ping etcd3
PING etcd3 (192.168.110.132) 56(84) bytes of data.
64 bytes from etcd3 (192.168.110.132): icmp_seq=1 ttl=64 time=0.484 ms
64 bytes from etcd3 (192.168.110.132): icmp_seq=2 ttl=64 time=2.65 ms
64 bytes from etcd3 (192.168.110.132): icmp_seq=3 ttl=64 time=2.65 ms
^C
--- etcd3 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2021ms
rtt min/avg/max/mdev = 0.484/1.931/2.657/1.023 ms
```

关闭屏保（可选）

```shell
[root@etcd1 ~]# setterm -blank 0
```

配置yum源

```shell
[root@etcd1 ~]# rm -rf /etc/yum.repos.d/* ;wget ftp://ftp.rhce.cc/k8s/* -P /etc/yum.repos.d/
```

关闭selinux

```shell
[root@etcd1 ~]# cat /etc/selinux/config 

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=disabled
# SELINUXTYPE= can take one of three two values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected. 
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted 

[root@etcd1 ~]# getenforce
Disabled
```

配置防火墙允许所有数据通过

```shell
[root@etcd1 ~]# firewall-cmd --set-default-zone=trusted
Warning: ZONE_ALREADY_SET: trusted
success

[root@etcd1 ~]# firewall-cmd --get-default-zone
trusted
```



#### Access By Client

现在etcd2机器作为客户端访问etcd1机器上的etcd服务

首先etcd2机器安装etcd

```shell
[root@etcd2 ~]# yum -y install etcd
```

查看etcdctl的帮助

```shell
[root@etcd2 ~]# etcdctl --help
```

连接192.168.110.133上的etcd服务，发现拒绝

```shell
[root@etcd2 ~]# etcdctl --endpoints http://192.168.110.133:2379 ls /
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp [::1]:2379: connect: connection refused

error #0: dial tcp [::1]:2379: connect: connection refused
```

修改etcd1上的配置文件ETCD_ADVERTISE_CLIENT_URLS="[http://192.168.110.133:2379](http://192.168.110.133:2379/),[http://localhost:2379](http://localhost:2379/)"，并重启etcd服务

```shell
[root@etcd1 ~]# cat /etc/etcd/etcd.conf | egrep -v "^#|^$"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="http://192.168.110.133:2380,http://localhost:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.110.133:2379,http://localhost:2379"
ETCD_NAME="default"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.110.133:2379,http://localhost:2379"

#重启
[root@etcd1 ~]# systemctl restart etcd

[root@etcd1 ~]# systemctl status etcd
```

此时客户端连接192.168.110.133成功，ls /表示查询根目录下内容

```shell
[root@etcd2 ~]# etcdctl --endpoints http://192.168.110.133:2379 ls /
```



### Install Cluster

etcd集群架构：etcd1为leader，etcd2为follower，etcd3为follower

| 服务器                | 操作系统版本                         | CPU架构 | 进程 | 功能描述 |
| --------------------- | ------------------------------------ | ------- | ---- | -------- |
| etcd1/192.168.110.133 | CentOS Linux release 7.4.1708 (Core) | x86_64  | etcd | leader   |
| etcd2/192.168.110.131 | CentOS Linux release 7.4.1708 (Core) | x86_64  | etcd | follower |
| etcd3/192.168.110.132 | CentOS Linux release 7.4.1708 (Core) | x86_64  | etcd | follower |



#### Add ectd2 To Cluster

首先还原环境变量

```shell
[root@etcd1 ~]# unset ETCDCTL_API
```

停止etcd1机器的etcd服务

```shell
[root@etcd1 ~]# systemctl stop etcd

[root@etcd1 ~]# systemctl status etcd
● etcd.service - Etcd Server
   Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled)
   Active: inactive (dead) since 二 2022-01-11 15:21:30 CST; 49s ago
  Process: 1582 ExecStart=/bin/bash -c GOMAXPROCS=$(nproc) /usr/bin/etcd --name="${ETCD_NAME}" --data-dir="${ETCD_DATA_DIR}" --listen-client-urls="${ETCD_LISTEN_CLIENT_URLS}" (code=killed, signal=TERM)
 Main PID: 1582 (code=killed, signal=TERM)
```

查看etcd的数据目录

```shell
[root@etcd1 ~]# cat /etc/etcd/etcd.conf | grep -i data
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
```

删除原始数据

```shell
[root@etcd1 ~]# rm -rf /var/lib/etcd/*
```

修改配置文件，把etcd2机器加入配置

```shell
[root@etcd1 ~]# vim /etc/etcd/etcd.conf

[root@etcd1 ~]# cat /etc/etcd/etcd.conf | egrep -v "^#|^$"
#配置数据目录
ETCD_DATA_DIR="/var/lib/etcd/cluster.etcd"
ETCD_LISTEN_PEER_URLS="http://192.168.110.133:2380,http://localhost:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.110.133:2379,http://localhost:2379"
ETCD_NAME="etcd133"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.110.133:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.110.133:2379,http://localhost:2379"
#目前是两个节点，所以这里是两个节点的etcd
ETCD_INITIAL_CLUSTER="etcd133=http://192.168.110.133:2380,etcd131=http://192.168.110.131:2380"
#集群token
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
#新创建etcd集群的时候ETCD_INITIAL_CLUSTER_STATE="new"，往已经存在的etcd集群添加etcd节点时：ETCD_INITIAL_CLUSTER_STATE="existing"
ETCD_INITIAL_CLUSTER_STATE="new"
```

复制配置文件到etcd2

```shell
[root@etcd1 ~]# scp /etc/etcd/etcd.conf etcd2:/etc/etcd/etcd.conf
root@etcd2's password: 
etcd.conf                                                                                                                                                                       100% 1813     1.7MB/s   00:00    
```

etcd2机器修改配置文件

```shell
[root@etcd2 ~]# unset ETCDCTL_API

[root@etcd2 ~]# vim /etc/etcd/etcd.conf 

[root@etcd2 ~]# cat /etc/etcd/etcd.conf | egrep -v "^#|^$"
ETCD_DATA_DIR="/var/lib/etcd/cluster.etcd"
ETCD_LISTEN_PEER_URLS="http://192.168.110.131:2380,http://localhost:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.110.131:2379,http://localhost:2379"
ETCD_NAME="etcd131"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.110.131:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.110.131:2379,http://localhost:2379"
ETCD_INITIAL_CLUSTER="etcd133=http://192.168.110.133:2380,etcd131=http://192.168.110.131:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
```

两个节点都启动etcd

```shell
[root@etcd1 ~]# systemctl start etcd 

[root@etcd1 ~]# systemctl status etcd

[root@etcd2 ~]# systemctl enable etcd --now
Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.

[root@etcd2 ~]# systemctl status etcd
```

查看etcd集群成员，可以看到192.168.110.133节点是Leader

```shell
[root@etcd1 ~]# etcdctl member list
341a3c460c1c993a: name=etcd131 peerURLs=http://192.168.110.131:2380 clientURLs=http://192.168.110.131:2379,http://localhost:2379 isLeader=false
ab23bcc86cf3190b: name=etcd133 peerURLs=http://192.168.110.133:2380 clientURLs=http://192.168.110.133:2379,http://localhost:2379 isLeader=true
```

集群健康状态

```shell
[root@etcd1 ~]# etcdctl cluster-health
member 341a3c460c1c993a is healthy: got healthy result from http://192.168.110.131:2379
member ab23bcc86cf3190b is healthy: got healthy result from http://192.168.110.133:2379
cluster is healthy
```

现在两个节点的etcd集群搭建完毕，数据也同步了

```shell
[root@etcd1 ~]# etcdctl ls /

[root@etcd1 ~]# etcdctl mkdir /public

[root@etcd1 ~]# export ETCDCTL_API=3

[root@etcd1 ~]# etcdctl put student1 59
OK

[root@etcd2 ~]# etcdctl ls /
/public

[root@etcd2 ~]# export ETCDCTL_API=3

[root@etcd2 ~]# etcdctl get student1
student1
59
```



#### Add ectd3 To Cluster

现在添加一个节点etcd3到集群

etcd3安装etcd

```shell
[root@etcd3 ~]# yum -y install etcd
```

加入新节点的时候，使用API2版本

```shell
[root@etcd1 ~]# export ETCDCTL_API=2
```

执行添加节点命令，注意：ETCD_INITIAL_CLUSTER_STATE="existing"

```shell
[root@etcd1 ~]# etcdctl member add etcd132 http://192.168.110.132:2380
Added member named etcd132 with ID 7d816f4fa2bea295 to cluster

ETCD_NAME="etcd132"
ETCD_INITIAL_CLUSTER="etcd131=http://192.168.110.131:2380,etcd132=http://192.168.110.132:2380,etcd133=http://192.168.110.133:2380"
ETCD_INITIAL_CLUSTER_STATE="existing"
```

查看集群成员，发现192.168.110.132显示不正常

```shell
[root@etcd1 ~]# etcdctl member list
341a3c460c1c993a: name=etcd131 peerURLs=http://192.168.110.131:2380 clientURLs=http://192.168.110.131:2379,http://localhost:2379 isLeader=false
7d816f4fa2bea295[unstarted]: peerURLs=http://192.168.110.132:2380
ab23bcc86cf3190b: name=etcd133 peerURLs=http://192.168.110.133:2380 clientURLs=http://192.168.110.133:2379,http://localhost:2379 isLeader=true
```

复制配置文件到etcd3

```shell
[root@etcd1 ~]# scp /etc/etcd/etcd.conf etcd3:/etc/etcd/etcd.conf
root@etcd3's password: 
etcd.conf                                                                                                                                                                       100% 1813     1.1MB/s   00:00  
```

etcd3修改配置文件

```shell
 #注意：添加到一个已经存在的集群，etcd133和etcd131配置文件不变，只修改etcd132配置文件
[root@etcd3 ~]# vim /etc/etcd/etcd.conf 

[root@etcd3 ~]# cat /etc/etcd/etcd.conf | egrep -v "^#|^$"
ETCD_DATA_DIR="/var/lib/etcd/cluster.etcd"
ETCD_LISTEN_PEER_URLS="http://192.168.110.132:2380,http://localhost:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.110.132:2379,http://localhost:2379"
ETCD_NAME="etcd132"
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.110.132:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.110.132:2379,http://localhost:2379"
ETCD_INITIAL_CLUSTER="etcd133=http://192.168.110.133:2380,etcd131=http://192.168.110.131:2380,etcd132=http://192.168.110.132:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="existing"
```

etcd3启动etcd

```shell
[root@etcd3 ~]# systemctl enable etcd --now
Created symlink from /etc/systemd/system/multi-user.target.wants/etcd.service to /usr/lib/systemd/system/etcd.service.
```

查看etcd集群成员，etcd集群有三个节点了

```shell
[root@etcd1 ~]# etcdctl member list
341a3c460c1c993a: name=etcd131 peerURLs=http://192.168.110.131:2380 clientURLs=http://192.168.110.131:2379,http://localhost:2379 isLeader=false
7d816f4fa2bea295: name=etcd132 peerURLs=http://192.168.110.132:2380 clientURLs=http://192.168.110.132:2379,http://localhost:2379 isLeader=false
ab23bcc86cf3190b: name=etcd133 peerURLs=http://192.168.110.133:2380 clientURLs=http://192.168.110.133:2379,http://localhost:2379 isLeader=true
```

自此etcd集群搭建完毕，接下来进行etcd服务的管理。



### Use As Pod

etcd在Kubernetes集群中可以以pod的方式运行，也可以以物理机部署的方式运行，本章讲解以pod方式运行的etcd。

在k8s中etcd以pod的方式运行，那配置文件在哪里，数据目录在哪里？

查看etcd pod，k8s中etcd为etcd-k8scloude1

```shell
[root@k8scloude1 ~]# kubectl get pods
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6b9fbfff44-4jzkj   1/1     Running   4          2d4h
calico-node-bdlgm                          1/1     Running   2          2d4h
calico-node-hx8bk                          1/1     Running   2          2d4h
calico-node-nsbfs                          1/1     Running   2          2d4h
coredns-545d6fc579-7wm95                   1/1     Running   2          2d4h
coredns-545d6fc579-87q8j                   1/1     Running   2          2d4h
etcd-k8scloude1                            1/1     Running   2          2d4h
kube-apiserver-k8scloude1                  1/1     Running   2          2d4h
kube-controller-manager-k8scloude1         1/1     Running   2          2d4h
kube-proxy-599xh                           1/1     Running   2          2d4h
kube-proxy-lpj8z                           1/1     Running   2          2d4h
kube-proxy-zxlk9                           1/1     Running   2          2d4h
kube-scheduler-k8scloude1                  1/1     Running   2          2d4h
metrics-server-bcfb98c76-k5dmj             1/1     Running   1          33h
```

在k8s中etcd以pod的形式运行， 此etcd的配置文件在哪？在/etc/kubernetes/manifests/etcd.yaml

```shell
[root@k8scloude1 ~]# ls /etc/kubernetes/manifests/etcd.yaml 
/etc/kubernetes/manifests/etcd.yaml

[root@k8scloude1 ~]# cat !$
cat /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.110.130:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.168.110.130:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --initial-advertise-peer-urls=https://192.168.110.130:2380
    - --initial-cluster=k8scloude1=https://192.168.110.130:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.168.110.130:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.168.110.130:2380
    - --name=k8scloude1
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.aliyuncs.com/google_containers/etcd:3.4.13-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        ephemeral-storage: 100Mi
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priorityClassName: system-node-critical
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}
```



可以发现挂载了数据卷，数据目录在/var/lib/etcd/

```shell
[root@k8scloude1 ~]# ls /var/lib/etcd/
member

[root@k8scloude1 ~]# ls /var/lib/etcd/member/
snap  wal
```



# Node

## Node Manage

### cordon & uncordon

cordon 节点会使其停止调度，会将node状态调为SchedulingDisabled，之后再创建新pod，新pod不会被调度到该节点，原有的pod不会受到影响，仍正常对外提供服务。

创建目录存放yaml文件

```shell
[root@k8scloude1 ~]# mkdir deploy

[root@k8scloude1 ~]# cd deploy/
```

使用--dry-run生成deploy配置文件

```shell
[root@k8scloude1 deploy]# kubectl create deploy nginx --image=nginx --dry-run=client -o yaml >nginx.yaml

[root@k8scloude1 deploy]# cat nginx.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}
```

修改deploy配置文件，`replicas: 5表示副本数为 5，deploy将创建5个pod`

```shell
[root@k8scloude1 deploy]# vim nginx.yaml 

#修改配置文件：
# replicas: 5  副本数修改为5
#terminationGracePeriodSeconds: 0  宽限期修改为0
# imagePullPolicy: IfNotPresent  镜像下载策略为存在镜像就不下载
[root@k8scloude1 deploy]# cat nginx.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      terminationGracePeriodSeconds: 0
      containers:
      - image: nginx
        name: nginx
        imagePullPolicy: IfNotPresent
        resources: {}
status: {}
```

创建deploy和使用pod yaml文件创建pod

```shell
[root@k8scloude1 deploy]# cat pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  terminationGracePeriodSeconds: 0
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: n1
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

[root@k8scloude1 deploy]# kubectl apply -f pod.yaml 
pod/pod1 created

[root@k8scloude1 deploy]# kubectl apply -f nginx.yaml 
deployment.apps/nginx created
```

查看pod，可以看到deploy生成5个pod（nginx-6cf858f6cf-XXXXXXX），还有一个pod1。

```shell
[root@k8scloude1 deploy]# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
nginx-6cf858f6cf-fwhmh   1/1     Running   0          52s   10.244.251.217   k8scloude3   <none>           <none>
nginx-6cf858f6cf-hr6bn   1/1     Running   0          52s   10.244.251.218   k8scloude3   <none>           <none>
nginx-6cf858f6cf-j2ccs   1/1     Running   0          52s   10.244.112.161   k8scloude2   <none>           <none>
nginx-6cf858f6cf-l7n4w   1/1     Running   0          52s   10.244.112.162   k8scloude2   <none>           <none>
nginx-6cf858f6cf-t6qxq   1/1     Running   0          52s   10.244.112.163   k8scloude2   <none>           <none>
pod1                     1/1     Running   0          60s   10.244.251.216   k8scloude3   <none>           <none>
```

**假设某天要对k8scloude2进行维护测试，不希望k8scloude2节点上被分配新的pod**，**可以对某个节点执行cordon之后，此节点就不会再调度新的pod了**

cordon k8scloude2节点，k8scloude2节点变为SchedulingDisabled状态

```shell
[root@k8scloude1 deploy]# kubectl cordon k8scloude2
node/k8scloude2 cordoned

[root@k8scloude1 deploy]# kubectl get nodes
NAME         STATUS                     ROLES                  AGE     VERSION
k8scloude1   Ready                      control-plane,master   8d      v1.21.0
k8scloude2   Ready,SchedulingDisabled   <none>                 7d23h   v1.21.0
k8scloude3   Ready                      <none>                 7d23h   v1.21.0
```

kubectl scale deploy命令使nginx deploy的副本数扩展为10个

```shell
[root@k8scloude1 deploy]# kubectl scale deploy nginx --replicas=10
deployment.apps/nginx scaled
```

查看pod，可以发现新生成的pod都被调度到到k8scloude3上，**某个节点被cordon之后，新的pod将不被调度到该节点，原先的pod不变**。

```shell
[root@k8scloude1 deploy]# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
nginx-6cf858f6cf-7fdnr   1/1     Running   0          4s      10.244.251.221   k8scloude3   <none>           <none>
nginx-6cf858f6cf-fwhmh   1/1     Running   0          9m9s    10.244.251.217   k8scloude3   <none>           <none>
nginx-6cf858f6cf-g92ls   1/1     Running   0          4s      10.244.251.219   k8scloude3   <none>           <none>
nginx-6cf858f6cf-hr6bn   1/1     Running   0          9m9s    10.244.251.218   k8scloude3   <none>           <none>
nginx-6cf858f6cf-j2ccs   1/1     Running   0          9m9s    10.244.112.161   k8scloude2   <none>           <none>
nginx-6cf858f6cf-l7n4w   1/1     Running   0          9m9s    10.244.112.162   k8scloude2   <none>           <none>
nginx-6cf858f6cf-lsvsg   1/1     Running   0          4s      10.244.251.223   k8scloude3   <none>           <none>
nginx-6cf858f6cf-mpwjl   1/1     Running   0          4s      10.244.251.222   k8scloude3   <none>           <none>
nginx-6cf858f6cf-s8x6b   1/1     Running   0          4s      10.244.251.220   k8scloude3   <none>           <none>
nginx-6cf858f6cf-t6qxq   1/1     Running   0          9m9s    10.244.112.163   k8scloude2   <none>           <none>
pod1                     1/1     Running   0          9m17s   10.244.251.216   k8scloude3   <none>           <none>
```

来个极端的例子，先把deploy的副本数变为0，再变为10，此时所有的pod都运行在k8scloude3节点了。

```shell
[root@k8scloude1 deploy]# kubectl scale deploy nginx --replicas=0
deployment.apps/nginx scaled

[root@k8scloude1 deploy]# kubectl get pod -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          10m   10.244.251.216   k8scloude3   <none>           <none>

[root@k8scloude1 deploy]# kubectl scale deploy nginx --replicas=10
deployment.apps/nginx scaled

[root@k8scloude1 deploy]# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
nginx-6cf858f6cf-5cx9s   1/1     Running   0          8s    10.244.251.231   k8scloude3   <none>           <none>
nginx-6cf858f6cf-6cblj   1/1     Running   0          8s    10.244.251.228   k8scloude3   <none>           <none>
nginx-6cf858f6cf-827cz   1/1     Running   0          8s    10.244.251.233   k8scloude3   <none>           <none>
nginx-6cf858f6cf-b989n   1/1     Running   0          8s    10.244.251.229   k8scloude3   <none>           <none>
nginx-6cf858f6cf-kwxhn   1/1     Running   0          8s    10.244.251.224   k8scloude3   <none>           <none>
nginx-6cf858f6cf-ljjxz   1/1     Running   0          8s    10.244.251.225   k8scloude3   <none>           <none>
nginx-6cf858f6cf-ltrpr   1/1     Running   0          8s    10.244.251.227   k8scloude3   <none>           <none>
nginx-6cf858f6cf-lwf7g   1/1     Running   0          8s    10.244.251.230   k8scloude3   <none>           <none>
nginx-6cf858f6cf-xw84l   1/1     Running   0          8s    10.244.251.226   k8scloude3   <none>           <none>
nginx-6cf858f6cf-zpwhq   1/1     Running   0          8s    10.244.251.232   k8scloude3   <none>           <none>
pod1                     1/1     Running   0          11m   10.244.251.216   k8scloude3   <none>           <none>
```



要让节点恢复调度pod，uncordon即可。

uncordon k8scloude2节点，k8scloude2节点状态变为Ready，恢复调度。

```shell
#需要uncordon
[root@k8scloude1 deploy]# kubectl uncordon k8scloude2
node/k8scloude2 uncordoned

[root@k8scloude1 deploy]# kubectl get nodes
NAME         STATUS   ROLES                  AGE   VERSION
k8scloude1   Ready    control-plane,master   8d    v1.21.0
k8scloude2   Ready    <none>                 8d    v1.21.0
k8scloude3   Ready    <none>                 8d    v1.21.0
```



### drain & uncordon

在对节点执行维护（例如内核升级、硬件维护等）之前， 可以使用 kubectl drain 从节点安全地逐出所有 Pods。 安全的驱逐过程允许 Pod 的容器 体面地终止， 并确保满足指定的 PodDisruptionBudgets，PodDisruptionBudget 是一个对象，用于定义可能对一组 Pod 造成的最大干扰。。
说明： 默认情况下， kubectl drain 将忽略节点上不能杀死的特定系统 Pod； 'drain' 驱逐或删除除镜像 pod 之外的所有 pod（不能通过 API 服务器删除）。如果有 daemon set-managed pods，drain 不会在没有 --ignore-daemonsets 的情况下继续进行，并且无论如何它都不会删除任何 daemon set-managed pods，因为这些 pods 将立即被 daemon set 控制器替换，它会忽略不可调度的标记。如果有任何 pod 既不是镜像 pod，也不是由复制控制器、副本集、守护程序集、有状态集或作业管理的，那么除非您使用 --force，否则 drain 不会删除任何 pod。如果一个或多个 pod 的管理资源丢失， --force 也将允许继续删除。

kubectl drain 的成功返回，表明所有的 Pods（除了上一段中描述的被排除的那些）， 已经被安全地逐出（考虑到期望的终止宽限期和你定义的 PodDisruptionBudget）。 然后就可以安全地关闭节点， 比如关闭物理机器的电源，如果它运行在云平台上，则删除它的虚拟机。



查看node状态和pod

```shell
[root@k8scloude1 deploy]# kubectl get nodes
NAME         STATUS   ROLES                  AGE   VERSION
k8scloude1   Ready    control-plane,master   8d    v1.21.0
k8scloude2   Ready    <none>                 8d    v1.21.0
k8scloude3   Ready    <none>                 8d    v1.21.0

[root@k8scloude1 deploy]# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
nginx-6cf858f6cf-58wnd   1/1     Running   0          65s   10.244.112.167   k8scloude2   <none>           <none>
nginx-6cf858f6cf-5rrk4   1/1     Running   0          65s   10.244.112.164   k8scloude2   <none>           <none>
nginx-6cf858f6cf-86wxr   1/1     Running   0          65s   10.244.251.237   k8scloude3   <none>           <none>
nginx-6cf858f6cf-89wj9   1/1     Running   0          65s   10.244.112.168   k8scloude2   <none>           <none>
nginx-6cf858f6cf-9njrj   1/1     Running   0          65s   10.244.251.236   k8scloude3   <none>           <none>
nginx-6cf858f6cf-hchtb   1/1     Running   0          65s   10.244.251.234   k8scloude3   <none>           <none>
nginx-6cf858f6cf-mb2ft   1/1     Running   0          65s   10.244.112.166   k8scloude2   <none>           <none>
nginx-6cf858f6cf-nq6zv   1/1     Running   0          65s   10.244.112.169   k8scloude2   <none>           <none>
nginx-6cf858f6cf-pl7ww   1/1     Running   0          65s   10.244.251.235   k8scloude3   <none>           <none>
nginx-6cf858f6cf-sf2w6   1/1     Running   0          65s   10.244.112.165   k8scloude2   <none>           <none>
pod1                     1/1     Running   0          36m   10.244.251.216   k8scloude3   <none>           <none>
```

drain驱逐节点：drain=cordon+evicted

drain k8scloude2节点，--delete-emptydir-data删除数据，--ignore-daemonsets忽略daemonsets

```shell
[root@k8scloude1 deploy]# kubectl drain k8scloude2
node/k8scloude2 cordoned
error: unable to drain node "k8scloude2", aborting command...

There are pending nodes to be drained:
 k8scloude2
cannot delete Pods with local storage (use --delete-emptydir-data to override): kube-system/metrics-server-bcfb98c76-k5dmj
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-nsbfs, kube-system/kube-proxy-lpj8z

[root@k8scloude1 deploy]# kubectl get node
NAME         STATUS                     ROLES                  AGE   VERSION
k8scloude1   Ready                      control-plane,master   8d    v1.21.0
k8scloude2   Ready,SchedulingDisabled   <none>                 8d    v1.21.0
k8scloude3   Ready                      <none>                 8d    v1.21.0

[root@k8scloude1 deploy]# kubectl drain k8scloude2 --ignore-daemonsets
node/k8scloude2 already cordoned
error: unable to drain node "k8scloude2", aborting command...

There are pending nodes to be drained:
 k8scloude2
error: cannot delete Pods with local storage (use --delete-emptydir-data to override): kube-system/metrics-server-bcfb98c76-k5dmj

[root@k8scloude1 deploy]# kubectl drain k8scloude2 --ignore-daemonsets --force --delete-emptydir-data
node/k8scloude2 already cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-nsbfs, kube-system/kube-proxy-lpj8z
evicting pod pod/nginx-6cf858f6cf-sf2w6
evicting pod pod/nginx-6cf858f6cf-5rrk4
evicting pod kube-system/metrics-server-bcfb98c76-k5dmj
evicting pod pod/nginx-6cf858f6cf-58wnd
evicting pod pod/nginx-6cf858f6cf-mb2ft
evicting pod pod/nginx-6cf858f6cf-89wj9
evicting pod pod/nginx-6cf858f6cf-nq6zv
pod/nginx-6cf858f6cf-5rrk4 evicted
pod/nginx-6cf858f6cf-mb2ft evicted
pod/nginx-6cf858f6cf-sf2w6 evicted
pod/nginx-6cf858f6cf-58wnd evicted
pod/nginx-6cf858f6cf-nq6zv evicted
pod/nginx-6cf858f6cf-89wj9 evicted
pod/metrics-server-bcfb98c76-k5dmj evicted
node/k8scloude2 evicted
```

查看pod，k8scloude2节点被drain之后，pod都调度到了k8scloude3节点。

**节点被drain驱逐的本质就是删除节点上的pod**，k8scloude2节点被drain驱逐之后，k8scloude2上运行的pod会被删除。

deploy是一个控制器，会监控pod的副本数，当k8scloude2上的pod被驱逐之后，副本数少于10，于是在可调度的节点创建pod,补足副本数。

单独的pod不具备再生性，删除之后就真删除了，如果k8scloude3被驱逐，则pod pod1会被删除，其他可调度节点也不会再生一个pod1。

```shell
[root@k8scloude1 deploy]# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES
nginx-6cf858f6cf-7gh4z   1/1     Running   0          84s     10.244.251.240   k8scloude3   <none>           <none>
nginx-6cf858f6cf-7lmfd   1/1     Running   0          85s     10.244.251.238   k8scloude3   <none>           <none>
nginx-6cf858f6cf-86wxr   1/1     Running   0          6m14s   10.244.251.237   k8scloude3   <none>           <none>
nginx-6cf858f6cf-9bn2b   1/1     Running   0          85s     10.244.251.243   k8scloude3   <none>           <none>
nginx-6cf858f6cf-9njrj   1/1     Running   0          6m14s   10.244.251.236   k8scloude3   <none>           <none>
nginx-6cf858f6cf-bqk2w   1/1     Running   0          84s     10.244.251.241   k8scloude3   <none>           <none>
nginx-6cf858f6cf-hchtb   1/1     Running   0          6m14s   10.244.251.234   k8scloude3   <none>           <none>
nginx-6cf858f6cf-hjddp   1/1     Running   0          84s     10.244.251.244   k8scloude3   <none>           <none>
nginx-6cf858f6cf-pl7ww   1/1     Running   0          6m14s   10.244.251.235   k8scloude3   <none>           <none>
nginx-6cf858f6cf-sgxfg   1/1     Running   0          84s     10.244.251.242   k8scloude3   <none>           <none>
pod1                     1/1     Running   0          41m     10.244.251.216   k8scloude3   <none>           <none>
```

查看node节点状态

```shell
[root@k8scloude1 deploy]# kubectl get nodes
NAME         STATUS                     ROLES                  AGE   VERSION
k8scloude1   Ready                      control-plane,master   8d    v1.21.0
k8scloude2   Ready,SchedulingDisabled   <none>                 8d    v1.21.0
k8scloude3   Ready                      <none>                 8d    v1.21.0
```



要取消drain某个节点，直接uncordon即可，没有undrain操作。

```shell
[root@k8scloude1 deploy]# kubectl undrain k8scloude2
Error: unknown command "undrain" for "kubectl"

Did you mean this?
        drain

Run 'kubectl --help' for usage.
```

uncordon k8scloude2节点，节点恢复调度

```shell
[root@k8scloude1 deploy]# kubectl uncordon k8scloude2
node/k8scloude2 uncordoned

[root@k8scloude1 deploy]# kubectl get nodes
NAME         STATUS   ROLES                  AGE   VERSION
k8scloude1   Ready    control-plane,master   8d    v1.21.0
k8scloude2   Ready    <none>                 8d    v1.21.0
k8scloude3   Ready    <none>                 8d    v1.21.0
```

把deploy副本数变为0，再变为10，再观察pod分布

```shell
[root@k8scloude1 deploy]# kubectl scale deploy nginx --replicas=0
deployment.apps/nginx scaled

[root@k8scloude1 deploy]# kubectl get pods -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          52m   10.244.251.216   k8scloude3   <none>           <none>

[root@k8scloude1 deploy]# kubectl scale deploy nginx --replicas=10
deployment.apps/nginx scaled
```

k8scloude2节点恢复可调度pod状态

```shell
[root@k8scloude1 deploy]# kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
nginx-6cf858f6cf-4sqj8   1/1     Running   0          6s    10.244.112.172   k8scloude2   <none>           <none>
nginx-6cf858f6cf-cjqxv   1/1     Running   0          6s    10.244.112.176   k8scloude2   <none>           <none>
nginx-6cf858f6cf-fk69r   1/1     Running   0          6s    10.244.112.175   k8scloude2   <none>           <none>
nginx-6cf858f6cf-ghznd   1/1     Running   0          6s    10.244.112.173   k8scloude2   <none>           <none>
nginx-6cf858f6cf-hnxzs   1/1     Running   0          6s    10.244.251.246   k8scloude3   <none>           <none>
nginx-6cf858f6cf-hshnm   1/1     Running   0          6s    10.244.112.171   k8scloude2   <none>           <none>
nginx-6cf858f6cf-jb5sh   1/1     Running   0          6s    10.244.112.170   k8scloude2   <none>           <none>
nginx-6cf858f6cf-l9xlm   1/1     Running   0          6s    10.244.112.174   k8scloude2   <none>           <none>
nginx-6cf858f6cf-pgjlb   1/1     Running   0          6s    10.244.251.247   k8scloude3   <none>           <none>
nginx-6cf858f6cf-rlnh6   1/1     Running   0          6s    10.244.251.245   k8scloude3   <none>           <none>
pod1                     1/1     Running   0          52m   10.244.251.216   k8scloude3   <none>           <none>
```

删除deploy，删除pod。

```shell
[root@k8scloude1 deploy]# kubectl delete -f nginx.yaml 
deployment.apps "nginx" deleted

[root@k8scloude1 deploy]# kubectl delete pod pod1 --force
warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "pod1" force deleted

[root@k8scloude1 deploy]# kubectl get pods -o wide
No resources found in pod namespace.
```



### delete

delete 删除节点就直接把一个节点就k8s集群中删除了，delete 节点之前需要先drain 节点。

kubectl drain 安全驱逐节点上面所有的 pod，--ignore-daemonsets往往需要指定的，这是因为deamonset会忽略SchedulingDisabled标签（使用kubectl drain时会自动给节点打上不可调度SchedulingDisabled标签）,因此deamonset控制器控制的pod被删除后，可能马上又在此节点上启动起来,这样就会成为死循环。因此这里忽略daemonset。



```shell
[root@k8scloude1 ~]# kubectl drain k8scloude3 --ignore-daemonsets 
node/k8scloude3 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-wmz4r, kube-system/kube-proxy-84gcx
evicting pod kube-system/calico-kube-controllers-6b9fbfff44-rl2mh
pod/calico-kube-controllers-6b9fbfff44-rl2mh evicted
node/k8scloude3 evicted
```

k8scloude3变为SchedulingDisabled

```shell
[root@k8scloude1 ~]# kubectl get nodes 
NAME         STATUS                     ROLES                  AGE   VERSION
k8scloude1   Ready                      control-plane,master   64m   v1.21.0
k8scloude2   Ready                      <none>                 56m   v1.21.0
k8scloude3   Ready,SchedulingDisabled   <none>                 56m   v1.21.0
```



删除节点k8scloude3

```shell
[root@k8scloude1 ~]# kubectl delete nodes k8scloude3
node "k8scloude3" deleted

[root@k8scloude1 ~]# kubectl get nodes 
NAME         STATUS   ROLES                  AGE   VERSION
k8scloude1   Ready    control-plane,master   65m   v1.21.0
k8scloude2   Ready    <none>                 57m   v1.21.0
```



# Label

Kubernetes使客户端（用户或内部组件）将称为“标签”的键值对附加到系统中的任何API对象，如pod和节点。相应地，“标签选择器”是针对匹配对象的标签的查询。

标签和选择器是Kubernetes中的主要分组机制，用于确定操作适用的组件。

例如，如果应用程序的Pods具有系统的标签 tier ("front-end", "back-end", for example) 和一个 release_track ("canary", "production", for example)，那么对所有"back-end" 和 "canary" 节点的操作可以使用如下所示的标签选择器：

 `tier=back-end AND release_track=canary `



Lable类似Docker中的tag，一个是对“特殊”镜像、容器、卷组等各种资源做标记，一个是attach到各种诸如Node、Pod、Server、RC资源对象上。不同的是Lable是一对键值对！Lable类似Tag，可使用K8s专有的标签选择器（Label Selector）进行组合查询。

一个Label是一组键值对，key、value都由用户定义，Lable可以附加在各种资源上，如Pod、ReplicaSet、Service等。一个资源对象可以定义任意数量的Label，同一个Label也能附加到任意数量的资源上，我们通过Lable可以实现多维度的资源分组管理能力，也可结合Label Selector实现类似SQL的对象查询机制。



# Annotation

Annotation与Label类似，也使用键值对形式进行定义，相比于Label，Annotation属于用户自定义的附加信息，主要方便外部工具进行获取或查找。



# Replication Controller

控制器是将实际集群状态转移到所需集群状态的对帐循环。它通过管理一组**pod**来实现。一种控制器是一个“复制控制器”，它通过在集群中运行指定数量的pod副本来处理复制和缩放。如果基础节点出现故障，它还可以处理创建替换pod。

其它控制器，是核心Kubernetes系统的一部分包括一个“DaemonSet控制器”为每一台机器（或机器的一些子集）上运行的恰好一个pod，和一个“作业控制器”用于运行pod运行到完成，例如作为批处理作业的一部分。控制器管理的一组pod由作为控制器定义的一部分的标签选择器确定。



RC是K8s集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。

  即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有1个Pod在运行。



Replication Controller，简称RC，她用来干啥呢？就是通过她来实现Pod副本数量的自动控制！RC确保任意时间都有指定数量的Pod“副本”在运行。

> 如果为某个Pod创建了RC并且指定3个副本，它会创建3个Pod，并且持续监控它们。如果某个Pod不响应，那么Replication Controller会替换它，保持总数为3。如果之前不响应的Pod恢复了，现在就有4个Pod了，那么Replication Controller会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Replication Controller会立刻启动2个新Pod，保证总数为5。还可以按照这样的方式缩小Pod，这个特性在执行滚动升级时很有用。
>
> 注意：删除RC，不会影响该RC已经创建好的Pod。在逻辑上Pod副本和RC是解耦和的！创建RC时，需要指定Pod模板（用来创建Pod副本的模板）和Label（RC需要监控的Pod标签）。
> 由Replication Controller衍生出Deployment，与RC相似90%，目的是更好地解决Pod编排。暂时不讨论。
>
> Horizontal Pod Autoscaler，简称HPA，Pod横向自动扩容智能控件。与RC，Deployment一样，也属于K8s的一种资源对象。她的实现原理是通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否针对性地调整目标Pod的副本数。



## 简单rc配置

```
[root@k8s-master ~]# kubectl get  rc
```

始终保证有一个在活着

更新rc文件

```
[root@k8s-master ~]# kubectl replace -f  nginx.yml
```

  nginx.yml文件信息

```
[root@k8s-master ~]# vim nginx.yml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-2
spec:
  restartPolicy: Never
  containers:
  - name: nginx
    image: "docker.io/nginx:latest"
```

对现有已创建资源直进行修改

```
[root@k8s-master ~]# kubectl edit rc nginx
```

  可以调整数量即使生效



## rs实现灰度发布

RS是新一代RC，提供同样的高可用能力，区别主要在于RS后来居上，能支持更多中的匹配模式。副本集对象一般不单独使用，而是作为部署的理想状态参数使用。

是K8S 1.2中出现的概念，是RC的升级。一般和Deployment共同使用。

>  部署表示用户对K8s集群的一次更新操作。部署是一个比RS应用模式更广的API对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新RS中副本数增加到理想状态，将旧RS中的副本数减小到0的复合操作；
>
>  　这样一个复合操作用一个RS是不太好描述的，所以用一个更通用的Deployment来描述。
>
> 　　以K8s的发展方向，未来对所有长期伺服型的的业务的管理，都会通过Deployment来管理。
>
>  　Deployment是对RC的升级，与RC的相似度超过90%。

web-rc.yaml文件内容

```
[root@k8s-master ~]# cat web-rc.yaml 
apiVersion: v1
kind: ReplicationController
metadata:
  name: myweb
spec:
  replicas: 3
  selector:
    app: myweb
  template:
    metadata:
      labels:
        app: myweb
    spec:
      containers:
      - name: myweb
        image: kubeguide/tomcat-app:v1
        ports:
        - containerPort: 8080
        env:
        - name: MYSQL_SERVICE_HOST
          value: 'mysql'
        - name: MYSQL_SERVICE_PORT
          value: '3306'
```

**创建集群**

```
[root@k8s-master ~]# kubectl create -f web-rc.yaml
```

**对集群进行升级操作**

  将集群内容器自动升级到新版本的容器

```
[root@k8s-master ~]# kubectl rolling-update  myweb  -f web-rc2.yaml 
```



web-rc2.yaml配置文件内容

```
[root@k8s-master ~]# cat web-rc2.yaml 
```



> web-rc2.yaml

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: myweb-2
spec:
  replicas: 3
  selector:
    app: myweb-2
  template:
    metadata:
      labels:
        app: myweb-2
    spec:
      containers:
      - name: myweb-2
        image: kubeguide/tomcat-app:v2
        ports:
        - containerPort: 8080
        env:
        - name: MYSQL_SERVICE_HOST
          value: 'mysql'
        - name: MYSQL_SERVICE_PORT
          value: '3306'
```



**升级后的回滚**

  使用新的文件，进行升级操作可达到回滚的目的，参考：https://github.com/kubeguide/samplecode

```
[root@k8s-master ~]# kubectl rolling-update  myweb-2  -f web-rc.yaml 
```



## rc小结

🍢 RC里包括完整的POD定义模板

🍢 RC通过Label Selector机制实现对POD副本的自动控制。

🍢 通过改变RC里的POD副本以实现POD的扩容和缩容

🍢 通过改变RC里POD模块中的镜像版本，可以实现POD的滚动升级。



# ReplicaSet

ReplicaSet （加强版的RC）的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合，简单的说就是保障用户应用集群的高可用性，当集群Pod数小于设定值则补充，大于设定值则剔除多余的Pod。其实ReplicaSet就是加强版的RC，它与RC用法基本相同，不同的是ReplicaSet支持集合式的Label Selector。

这里需要注意：我们在实际使用时应该避免直接使用ReplicaSet，而是应该通过Deployment来创建ReplicaSet和Pod。



# Deploments

实际应用中，我们不会去直接创建Pod，我们一般通过Kubernetes提供的工作负载（Deployment、DeamonSet、StatefulSet、Job等）完成对一组Pod全生命周期的控制。

Deploment是Kubernetes 1.2开始引入的新概念，其目的是为了更好解决Pod编排问题，为此Deploment在内部使用ReplicaSet来实现目的，Deployment通过管理ReplicaSet间接管理Pod，即Deployment管理ReplicaSet，ReplicaSet管理Pod。Deployment有如下几种使用场景：

- 创建一个Deployment对象来生成对应的ReplicaSet并完成Pod副本的创建过程；
- 通过检查Deployment的状态来判断部署是否完成；
- 更新Deployment以创建新的Pod；
- 如果当前Deployment不稳定则会滚到先前的一个版本；
- 挂起或恢复一个Deployment。



Deployment 的控制器，实际上控制的是 ReplicaSet 的数目，以及每个 ReplicaSet 的属性。

而一个应用的版本，对应的正是一个 ReplicaSet；这个版本应用的 Pod 数量，则由 ReplicaSet 通过它自己的控制器（ReplicaSet Controller）来保证。

通过这样的多个 ReplicaSet 对象，Kubernetes 项目就实现了对多个“应用版本”的描述。

![img](../Image/2022/221206-44.jpg)



## YML Configuration

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: webapp
  name: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: webapp
    spec:
      # 宽限期修
      terminationGracePeriodSeconds: 0
      containers:
      - name: webapp
        image: 172.16.194.135:5000/webapp:latest
        imagePullPolicy: IfNotPresent
        resources: {}
        ports:
        - containerPort: 4567
status: {}
```

对于这个文件，这里有几个属性需要说明下：

- replicas：定义Pod的副本数量
- selector：目标Pod的标签选择器
- template：用于自动创建新Pod副本的模板



## Application Example

### 使用RC实现滚动升级

```
#kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2
```

　　使用kubia-v2版本应用来替换运行着kubia-v1的RC，将新的复制控制器命名为kubia-v2，并使用luksa/kubia:v2最为镜像。

　　升级完成后使kubectl describe rc kubia-v2查看升级后的状态。

　　为什么现在不再使用rolling-update？

　　1.直接更新pod和RC的标签并不是一个很的方案；

　　2.kubectl只是执行升级中的客户端，但如果执行kubectl过程中是去了网络连接，升级将会被中断，pod和RC将会处于一个中间的状态，所以才有了Deployment资源的引入。



### 使用Deployment声明式的升级应用

　Rs替代Rc来复制个管理pod。

　　创建Deployment前确保删除所有的RC和pod，但是暂时保留Service，

　　kubectl delete rc --all

　　创建Deployment

```
#kubectl create -f kubectl.depl-v1.yaml --record  //--record可以记录历史版本
 
#查看Deployment的相关信息
#kubectl get deployment
#kubectl describe deployment
 
#查看部署状态：
#kubectl rollout status deployment kubia
```



### 触发deployment升级

```
#kubectl edit deployment kubia //修改完后资源对象会被更新
#kubectl patch deployment kubia -p '{...}' //只能包含想要更新的字段
#kubectl apply -f kubia-deploy-v2.yml //如果yml中定义的资源不存在，会自动被创建
#kubectl replace -f kubia-deploy-v2.yml //如果yml中定义的资源不存在，则会报错
```

　修改configmap并不会触发升级，如果想要触发，可以创建新的configmap并修改pod模板引用新的configmap。



### 回滚deployment

在上述升级deployment过程中可以使用如下命令来观察升级的过程

```
#kubectl rollout status deployment kubia
```



如果出现报错，如何进行停止？可以使用如下命令进行回滚到先前部署的版本

```
#kubectl rollout undo deployment kubia
```



　如何显示deployment的历史版本？

```
#kubectl rollout history deployment kubia
```



　　如何回滚到特定的版本？

```
#kubectl rollout undo deployment kubia --to-revision=1
```



### 控制滚动升级的速率

在deployment的滚动升级过程中，有两个属性决定一次替换多少个pod：maxSurge、maxUnavailable,可以通过strategy字段下的rollingUpdate的属性来配置，

　　maxSurge：决定期望的副本数，默认值为25%，如果副本数设置为4个，则在滚动升级过程中，不会运行超过5个pod。

　　maxUnavaliable: 决定允许多少个pod处于不可用状态，默认值为25%，如果副本数为4，那么只能有一个pod处于不可用状态，

​    默认情况下如果10分钟内没有升级完成，将被视为失败，如果要修改这个参数可以使用kubectl describe deploy kubia 查看到一条ProgressDeadline-Exceeded的记录，可以修改此项参数修改判断时间。



### 金丝雀发布和蓝绿发布

金丝雀部署：优先发布一台或少量机器升级，等验证无误后再更新其他机器。优点是用户影响范围小，不足之处是要额外控制如何做自动更新。
蓝绿部署：2组机器，蓝代表当前的V1版本，绿代表已经升级完成的V2版本。通过LB将流量全部导入V2完成升级部署。优点是切换快速，缺点是影响全部用户。



## Application

### Deploy Nginx

Deployments可以自动部署一个容器应用的多个副本，监控其副本数量并始终维持这一数量。创建并维持三个nginx副本，通过kubectl create创建此Deployment：

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```



```sh
$ kubectl create -f demo_deployment.yml
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           2m40s
$ kubectl get pods
NAME                               READY   STATUS    RESTARTS   AGE
nginx-deployment-585449566-b6v88   1/1     Running   0          2m4s
nginx-deployment-585449566-p4vb7   1/1     Running   0          2m4s
nginx-deployment-585449566-s95sr   1/1     Running   0          2m4s
```



三个nginx的Pod启动完毕了，手动杀掉一个Pod，Deployment会自动将其恢复：

```sh
$ kubectl delete pod nginx-deployment-585449566-b6v88
$ kubectl get pods
NAME                               READY   STATUS              RESTARTS   AGE
nginx-deployment-585449566-b6v88   0/1     Terminating         0          7m23s
nginx-deployment-585449566-nbrdm   0/1     ContainerCreating   0          6s
nginx-deployment-585449566-p4vb7   1/1     Running             0          7m23s
nginx-deployment-585449566-s95sr   1/1     Running             0          7m23s
```



Deployment是自动启动并调度Pod的，这三个Pod最终在哪些节点上运行，完全由master的Scheduler来控制，通过-o wide来看看这三个Pod都分布在哪里：

```sh
$ kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE     IP           NODE      NOMINATED NODE   READINESS GATES
nginx-deployment-585449566-nbrdm   1/1     Running   0          3m12s   10.244.1.8   kubevm2   <none>           <none>
nginx-deployment-585449566-p4vb7   1/1     Running   0          10m     10.244.2.5   kubevm3   <none>           <none>
nginx-deployment-585449566-s95sr   1/1     Running   0          10m     10.244.1.7   kubevm2   <none>           <none>
```



# DaemonSet

DaemonSet是kubernetes 1.2新增的资源对象，它可以确保在每个Node上仅运行一份Pod的副本，DaemonSet可用于以下场景：

- 在每个节点上运行集群守护进程
- 在每个节点上运行日志收集守护进程
- 在每个节点上运行监控守护进程



## Application

创建一个DaemonSet看看效果，还是以Nginx为例，下面的例子定义的Daemon将会为每个Node都启动一个Nginx容器：

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      name: nginx-daemonset
  template:
    metadata:
      labels:
        name: nginx-daemonset
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```



```sh
$ kubectl create -f demo_daemonset.yml
daemonset.apps/nginx-daemonset created
$ kubectl get DaemonSet
NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
nginx-daemonset   2         2         2       2            2           <none>          11m
$ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
nginx-daemonset-49tcp   1/1     Running   0          14m   10.244.1.13   kubevm2   <none>           <none>
nginx-daemonset-w2xc2   1/1     Running   0          14m   10.244.2.11   kubevm3   <none>           <none>
```



与Deployment不同的是，在yaml中没有指定replicas，但是DaemonSet自动为除Master节点外每一个Node都创建了一个副本。Master没有Pod副本，缺省状态下，DaemonSet只会在Node中创建副本，如果需要在Master中也启动Pod，则需要设置容忍度：

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      name: nginx-daemonset
  template:
    metadata:
      labels:
        name: nginx-daemonset
    spec:
      # 配置容忍度
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```



重新创建DaemonSet，看看效果：

```sh
$ kubectl get pods -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
nginx-daemonset-45z95   1/1     Running   0          46s   10.244.2.13   kubevm3   <none>           <none>
nginx-daemonset-9z2lt   1/1     Running   0          46s   10.244.0.5    kubevm1   <none>           <none>
nginx-daemonset-cjf6k   1/1     Running   0          46s   10.244.1.15   kubevm2   <none>           <none>
```






# Service

微服务架构中的一个“微服务”，她是正真的新娘，而之前的Pod，RC等资源对象其实都是嫁衣。
每个Pod都会被分配一个单独的IP地址，而且每个Pod都提供了一个独立的Endpoint（Pod lP + ContainerPort）以被客户端访问，现在多个Pod副本组成了一个集群来提供服务，客户端要想访问集群，一般的做法是部署一个负载均衡器（软件或硬件），为这组Pod开启一个对外的服务端口如8000端口，并且将这些Pod的Endpoint列表加入8000端口的转发列表中，客户端就可以通过负载均衡器的对外IP地址 + 服务端口来访问此服务，而客户端的请求最后会被转发到哪个Pod，则由负载均衡器的算法所决定。

![img](../Image/2022/221206-16.jpg)

K8s的server定义了一个服务的访问入口地址，前端（Pod）通过入口地址访问其背后的一组由Pod副本组成的集群实例，service与其后端Pod副本集群之间通过Label Selector 实现“无缝对接”。

> 可以将Server抽象成特殊的扁平的双向管道，Service借助Label Selector保证了前端容器正确可靠地指向对应的后台容器。 RC的作用保证了Service的服务能力和服务质量始终处于预期的标准。

Kubemetes也遵循了上述常规做法，运行在每个Node上的kube-proxy进程其实就是一个智能的软件负载均衡器，它负责把对Service的请求转发到后端的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。但Kubernetes发明了一种很巧妙又影响深远的设计：Service不是共用一个负载均衡器的IP地址，而是每个Service分配了一个全局唯一的虚拟IP地址，这个虚拟IP被称为ClusterIP。这样一来，每个服务就变成了具备唯一IP地址的“通信节点”，服务调用就变成了最基础的TCP网络通信问题。

> Pod的Endpoint地址会随着Pod的销毁和重新创建而发生改变，因为新Pod的IP地址与之前旧Pod的不同。而Service一旦创建，Kubemetes就会自动为它分配一个可用的Cluster IP，而且在Service的整个生命周期内，它的ClusterIP不会发生改变。于是，服务发现这个棘手的问题Kubemetes的架构里也得以轻松解决：只要用Service的Name与Service的Cluster IP地址做一个DNS域名映射即可完美解决问题。



Service定义了外界访问一组Pod的方式，Service有自己的IP和端口，Service为这些Pod提供负载均衡。比如有一个文件上传应用，其后端应用有三个Pod副本，这三个副本由于会经常性被销毁重启，所以前端必然不能时刻维护这三个副本的地址，这时候Service的作用就体现出来了，它能解耦前端应用与后端应用的这种关联。



## Service Introduction

Kubernetes服务是一组协同工作的pod，就像多层架构应用中的一层。构成服务的pod组通过标签选择器来定义。

Kubernetes通过给服务分配静态IP地址和域名来提供服务发现机制，并且以轮询调度的方式将流量负载均衡到能与选择器匹配的pod的IP地址的网络连接上（即使是故障导致pod从一台机器移动到另一台机器）。默认情况下，一个服务会暴露在集群中（例如，多个后端pod可能被分组成一个服务，前端pod的请求在它们之间负载平衡）；但是，一个服务也可以暴露在集群外部（例如，从客户端访问前端pod）。

service是k8s中的一个重要概念，主要是提供负载均衡和服务自动发现。Service 是由 kube-proxy 组件，加上 iptables 来共同实现的。



### Service作用

RC、RS和Deployment只是保证了支撑服务的POD的数量，但是没有解决如何访问这些服务的问题。一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以确定的IP和端口号提供服务。

要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。

在K8集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的**虚拟IP**，集群内部通过虚拟IP访问一个服务。

在K8s集群中微服务的负载均衡是由Kube-proxy实现的。Kube-proxy是K8s集群内部的负载均衡器。它是一个分布式代理服务器，在K8s的每个节点上都有一个；这一设计体现了它的***伸缩性\***优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。



### service原理图

![img](../Image/2022/221206-5.jpg)

网访问node ip 转到cluster ip上 在进行pod 分发 rr轮询

```
kubectl create -f web-svc.yaml
    [root@k8s-master ~]# kubectl get service
    
    NAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
    kubernetes   10.254.0.1      <none>        443/TCP          4h
    myweb        10.254.168.71   <nodes>       8080:30001/TCP   15s
```



## Create And Test

### 创建Service

创建Service的方法有两种：



#### 通过kubectl expose创建

这一步说是将服务暴露出去，实际上是在服务前面加一个负载均衡，因为pod可能分布在不同的结点上。

- –port：暴露出去的端口
- –type=NodePort：使用结点+端口方式访问服务
- –target-port：容器的端口
- –name：创建service指定的名称



```
#kubectl expose deployment nginx --port=88 --type=NodePort --target-port=80 --name=nginx-service
```



#### 通过yaml文件创建

创建一个名为hostnames-yaohong的服务，将在端口80接收请求并将链接路由到具有标签选择器是app=hostnames的pod的9376端口上。使用kubectl creat来创建serivice：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: hostnames-yaohong
spec:
  selector:
    app: hostnames
  ports:
  - name: default
    protocol: TCP
    port: 80     //该服务的可用端口
    targetPort: 9376    //具有app=hostnames标签的pod都属于该服务
```



### 测试service

```yaml
[root@k8s-master ~]# vim myweb-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: myweb
spec:
  type: NodePort
  ports:
    - port: 8080
      nodePort: 30001
  selector:
    app: myweb
```

启动集群

```
[root@k8s-master ~]# kubectl create -f myweb-svc.yaml 
service "myweb" created
[root@k8s-master ~]# kubectl get svc
NAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes   10.254.0.1      <none>        443/TCP          6h
myweb        10.254.247.21   <nodes>       8080:30001/TCP   12s
```

浏览器访问测试

![img](../Image/2022/221206-4.jpg)





## 服务暴露

有3种方式在外部访问服务：

　　1.将服务的类型设置成NodePort；

　　2.将服务的类型设置成LoadBalance；

　　3.创建一个Ingress资源。



### nodeport

NodePort 服务是引导外部流量到你的服务的最原始方式。NodePort，正如这个名字所示，在所有节点（虚拟机）上开放一个特定端口，任何发送到该端口的流量都被转发到对应服务。

![img](../Image/2022/221206-9.jpg)



```yaml
apiVersion: v1
kind: Service
metadata:
  name: Service-yaohong
spec:
  type: NodePort  //为NodePort设置服务类型
  ports:
  - port: 80 
    targetPort: 8080
    nodeport: 30123    //通过集群节点的30123端口可以访问服务
  selector:
    app: yh
```



这种方法有许多缺点：

　　1.每个端口只能是一种服务

　　2.端口范围只能是 30000-32767

如果节点/VM 的 IP 地址发生变化，需要能处理这种情况。基于以上原因，不建议在生产环境上用这种方式暴露服务。如果你运行的服务不要求一直可用，或者对成本比较敏感，你可以使用这种方法。这样的应用的最佳例子是 demo 应用，或者某些临时应用。



### Loadbalance

LoadBalancer 服务是暴露服务到 internet 的标准方式。在 GKE 上，这种方式会启动一个 Network Load Balancer[2]，它将给你一个单独的 IP 地址，转发所有流量到你的服务。

![img](../Image/2022/221206-10.jpg)

通过如下方法来定义服务使用负载均衡：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: loadBalancer-yaohong
spec:
  type: LoadBalancer  //该服务从k8s集群的基础架构获取负载均衡器
  ports:
  - port: 80 
    targetPort: 8080
  selector:
    app: yh
```



如果你想要直接暴露服务，这就是默认方式。所有通往你指定的端口的流量都会被转发到对应的服务。它没有过滤条件，没有路由等。这意味着你几乎可以发送任何种类的流量到该服务，像 HTTP，TCP，UDP，Websocket，gRPC 或其它任意种类。

这个方式的最大缺点是每一个用 LoadBalancer 暴露的服务都会有它自己的 IP 地址，每个用到的 LoadBalancer 都需要付费，这将是非常昂贵的。



## Endpoint

服务并不是和pod直接相连的，介于他们之间的就是Endpoint资源。

Endpoint资源就是暴露一个服务的IP地址和端口列表。

通过service查看endpoint方法如下：



```bash
$ kubectl -n kube-system get svc kube-dns
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE
kube-dns   ClusterI
P   10.187.0.2   <none>        53/UDP,53/TCP   19d
 
$ kubectl -n kube-system describe svc kube-dns
Name:              kube-dns
Namespace:         kube-system
Labels:            addonmanager.kubernetes.io/mode=Reconcile
                   k8s-app=kube-dns
                   kubernetes.io/cluster-service=true
                   kubernetes.io/name=CoreDNS
Annotations:       kubectl.kubernetes.io/last-applied-configuration:
                     {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{"prometheus.io/scrape":"true"},"labels":{"addonmanager.kubernetes.io/mode":...
                   prometheus.io/scrape: true
Selector:          k8s-app=kube-dns
Type:              ClusterIP
IP:                10.187.0.2
Port:              dns  53/UDP
TargetPort:        53/UDP
Endpoints:         10.186.0.2:53,10.186.0.3:53     //代表服务endpoint的pod的ip和端口列表
Port:              dns-tcp  53/TCP
TargetPort:        53/TCP
Endpoints:         10.186.0.2:53,10.186.0.3:53
Session Affinity:  None
Events:            <none>
```



直接查看endpoint信息方法如下：

```bash
#kubectl -n kube-system get endpoints kube-dns
NAME       ENDPOINTS                                               AGE
kube-dns   10.186.0.2:53,10.186.0.3:53,10.186.0.2:53 + 1 more...   19d
 
#kubectl -n kube-system describe  endpoints kube-dns
Name:         kube-dns
Namespace:    kube-system
Labels:       addonmanager.kubernetes.io/mode=Reconcile
              k8s-app=kube-dns
              kubernetes.io/cluster-service=true
              kubernetes.io/name=CoreDNS
Annotations:  <none>
Subsets:
  Addresses:          10.186.0.2,10.186.0.3
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    dns      53    UDP
    dns-tcp  53    TCP
 
Events:  <none>
```



### 手动配置服务的endpoint

 如果创建pod时不包含选择器，则k8s将不会创建endpoint资源。这样就需要创建endpoint来指的服务的对应的endpoint列表。

service中创建endpoint资源，其中一个作用就是用于service知道包含哪些pod。

![img](../Image/2022/221206-8.jpg)

### 为外部服务创建别名

除了手动配置来访问外部服务外，还可以使用完全限定域名（FQDN）访问外部服务。



```yaml
apiVersion: v1
kind: Service
metadata:
  name: Service-yaohong
spec:
  # 代码的type被设置成了ExternalName  externalName: someapi.somecompany.com
  # 实际服务的完全限定域名（FQDN）<br>port: - port: 80
  type: ExternalName                     
```



服务创建完成后，pod可以通过external-service.default.svc.cluster.local域名（甚至是external-service）连接外部服务。



## Headless Service

`Headless Service`也是一种`Service`，但不同的是会定义`spec:clusterIP: None`，也就是不需要`Cluster IP`的`Service`。

顾名思义，`Headless Service`就是没头的`Service`。有什么使用场景呢？

- 第一种：自主选择权，有时候`client`想自己来决定使用哪个`Real Server`，可以通过查询`DNS`来获取`Real Server`的信息。
- 第二种：`Headless Services`还有一个用处（PS：也就是我们需要的那个特性）。`Headless Service`的对应的每一个`Endpoints`，即每一个`Pod`，都会有对应的`DNS`域名；这样`Pod`之间就可以互相访问。



# Ingress

为什么使用Ingress，一个重要的原因是LoadBalancer服务都需要创建自己的负载均衡器，以及独有的公有Ip地址，而Ingress只需要一个公网Ip就能为许多服务提供访问。

![img](../Image/2022/221206-11.jpg)



## Ingress原理

![img](../Image/2022/221206-13.jpg)

Ingress 可能是暴露服务的最强大方式，但同时也是最复杂的。Ingress 控制器有各种类型，包括 Google Cloud Load Balancer， Nginx，Contour，Istio，等等。它还有各种插件，比如 cert-manager[5]，它可以为你的服务自动提供 SSL 证书。

如果你想要使用同一个 IP 暴露多个服务，这些服务都是使用相同的七层协议（典型如 HTTP），那么Ingress 就是最有用的。如果你使用本地的 GCP 集成，你只需要为一个负载均衡器付费，且由于 Ingress是“智能”的，你还可以获取各种开箱即用的特性（比如 SSL、认证、路由等等）。



## 创建Ingress资源

Ingress 事实上不是一种服务类型。相反，它处于多个服务的前端，扮演着“智能路由”或者集群入口的角色。

你可以用 Ingress 来做许多不同的事情，各种不同类型的 Ingress 控制器也有不同的能力。

![img](../Image/2022/221206-12.jpg)

编写如下ingress.yml文件：

```yaml
kind: Ingress
metadata:
  name: ingressyaohong
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
```



通过如下命令进行查看ingress：

```
# kubectl create  -f ingress.yml
```



### 通过Ingress访问服务

通过kubectl get ing命令进行查看ingress：

```
# kubectl get ing
NAME             HOSTS               ADDRESS   PORTS   AGE
ingressyaohong   kubia.example.com             80      2m
```



### 通过相同的Ingress暴露多少服务

**将不同的服务映射到相同的主机不同的路径**：

```yaml
apiVersion: v1
kind: Ingress
metadata:
  name: Ingress-yaohong
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /yh                //对kubia.example.com/yh请求转发至kubai服务
        backend:
          serviceName: kubia
          servicePort：80
      - path: /foo              //对kubia.example.com/foo请求转发至bar服务
        backend:
          serviceName: bar
          servicePort：80
```



**将不同的服务映射到不同的主机上**：

```yaml
apiVersion: v1
kind: Ingress
metadata:
  name: Ingress-yaohong
spec:
  rules:
  - host: yh.example.com
    http:
      paths:
      - path: /                //对yh.example.com请求转发至kubai服务
        backend:
          serviceName: kubia
          servicePort：80
  - host: bar.example.com
    http:
      paths:
      - path: /                //对bar.example.com请求转发至bar服务
        backend:
          serviceName: bar
          servicePort：80
```



### 配置Ingress处理TLS传输

客户端和控制器之间的通信是加密的，而控制器和后端pod之间的通信则不是。

```yaml
apiVersion: v1
kind: Ingress
metadata:
  name: Ingress-yaohong
spec:
 tls:                     //在这个属性中包含所有的TLS配置
 - hosts:
   - yh.example.com       //将接收来自yh.example.com的TLS连接
   serviceName: tls-secret     //从tls-secret中获得之前创立的私钥和证书
  rules:
  - host: yh.example.com
    http:
      paths:
      - path: /                //对yh.example.com请求转发至kubai服务
        backend:
          serviceName: kubia
          servicePort：80
```












# Deploy Applications

## Statefulset

StatefulSet是Kubernetes提供的管理有状态应用的负载管理控制器API。

​    特点：

　　1.具有固定的网络标记（主机名）

　　2.具有持久化存储

　　3.需要按顺序部署和扩展

　　4.需要按顺序终止和删除

　　5.需要按顺序滚动和更新

　　有状态应用：这种实例之间有不对等关系，以及实例对外部数据有依赖关系的应用，就被称为“有状态应用”（Stateful Application）。



### statefulset的创建

statefulset的创建顺序从0到N-1,终止顺序则相反，如果需要对satateful扩容，则之前的n个pod必须存在，如果要终止一个pod，则他的后续pod必须全部终止。

​    创建statefulset

```
#kubectl create -f ss-nginx.yml
```



查看statefulset

```
#kubectl get statefulset
```



　statefulset会使用一个完全一致的pod来替换被删除的pod。

　　statefulset扩容和缩容时，都会删除最高索引的pod，当这个pod完全被删除后，才回删除拥有次高索引的pod。



### statefulset中发现伙伴的节点

　通过DNS中SRV互相发现。



### 更新statefulset

```
#kuebctl edit statefulset kubia
```

但修改后pod不会自动 被更新，需要手动delete pod后会重新调度更新。












# References

- [Kubernetes教程](https://k8s.easydoc.net/)
- [GETTING STARTED](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-strong-getting-started-strong-)
