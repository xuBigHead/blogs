---
layout: post
title: K8S 容器.md
categories: [Kubernetes 容器编排]
description: Kubernetes 容器编排
keywords: Kubernetes 容器编排
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---
# Pod

## Introduction

docker可以创建容器，Kubernetes不能直接创建容器，Kubernetes创建的是pod，pod里面包含了一个或者多个容器。

Kubernetes的基本调度单元称为“pod”。它可以把更高级别的抽象内容增加到容器化组件。一个pod一般包含一个或多个容器，这样可以保证它们一直位于主机上，并且可以共享资源。Kubernetes中的每个pod都被分配一个唯一的（在集群内的）IP地址这样就可以允许应用程序使用端口，而不会有冲突的风险。

Pod可以定义一个卷，例如本地磁盘目录或网络磁盘，并将其暴露在pod中的一个容器之中。pod可以通过Kubernetes API手动管理，也可以委托给控制器来管理。

Pod是K8s最重要也是最基础的概念！每个Pod都有一个特殊的被称为“根容器”的Pause容器，此容器与引入业务无关并且不易死亡。且以它的状态代表整个容器组的状态！Pause容器对应的镜像属于K8s平台的一部分，除了Pause容器，每个Pod还包含一个或多个用户业务容器。Pod其实有两种类型：普通的Pod及静态Pod（static Pod）,static Pod并不存放在Kubemetes的eted存储里，而是存放在某个具体的Node上的一个具体文件中，并且只在此Node上启动运行。而普通的Pod一旦被创建，就会被放入到etcd中存储，确后会被KubernetesMaster调度到某个具体的Node上并进行绑定（Binding），随后该Pod被对应的Node上的kubelet进程实例化成一组相关的Docker容器并启动起来。在默认情况下，当Pod里的某个容器停止时，Kubemetes会自动检测到这个问题并且重新启动这个Pod（重启Podel）的所有容器），如果Pod所在的Node完机，则会将这个Node上的所有Pod重新调度到其他节点上。Pod（上图绿色方框）安排在节点上，包含一组容器和卷。同一个Pod里的容器共享同一个网络命名空间，可以使用localhost互相通信。



Endpoint（Pod IP + ContainerPort） pod ip：一个Pod里多个容器共享Pod IP地址。K8s要求底层网络支持集群内任意两个Pod之间的TCP/IP直接通信，使用虚拟二层网络技术（Flannel（没有接触过 ），OpenvSwitch）实现。在Vmware中类似的二层交换技术是VSwitch，当然了，现在整个数据中心网络二层逐步从vSwitch—>OpenvSwitch



Pod是Kubernetes中创建和管理的、最小的可部署的计算单元。它其实是由一组容器组成（最少一个），这一组容器共享存储、网络、以及怎样运行这些容器的声明。一个Pod的组成示意图如下：

<img src="../Image/2022/221215-1.jpg" alt="img" style="zoom:50%;" />

我们可以看出一个Pod有一个我们称之为“根容器”的Pause容器和多个用户业务容器组成，为什么要这么设计呢？

- 当将一组容器作为一个基本单元时，可利用Pause容器这种业务无关的容器描述当前整个单元的状态；
- Pod里多个业务容器共享Pause容器的IP和Volume，简化了业务容器间通信问题与文件共享问题。



Pod 是一组紧密关联的容器集合，它们共享IPC、Network和UTS namespace，是 Kubernetes 调度的基本单元。Pod 的设计理念是支持多个容器在一个 Pod 中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。

Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。

Pod（就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） 容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的 “逻辑主机”，其中包含一个或多个应用容器， 这些容器相对紧密地耦合在一起。 在非云环境中，在相同的物理机或虚拟机上运行的应用类似于在同一逻辑主机上运行的云应用。

除了应用容器，Pod 还可以包含在 Pod 启动期间运行的 Init 容器。 你也可以在集群中支持临时性容器 的情况下，为调试的目的注入临时性容器。

pod里面有一个或者多个容器，常见的容器有docker容器，containerd容器，除了 Docker 之外，Kubernetes 支持 很多其他容器运行时， Docker 是最有名的容器运行时， 使用 Docker 的术语来描述 Pod 会很有帮助。

Pod 的共享上下文包括一组 Linux 命名空间、控制组（cgroup）和可能一些其他的隔离方面， 即用来隔离 Docker 容器的技术。 在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。

就 Docker 概念的术语而言，Pod 类似于共享命名空间和文件系统卷的一组 Docker 容器。

Pod是K8s集群中所有业务类型的基础

Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。

Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统。

POD控制器Deployment、Job、DaemonSet和PetSet



## Core Concept

### Pod Status

Pod在整个生命周期中被系统标示为各种状态，熟悉Pod的各种状态对于理解Pod的调度策略、重启策略很有必要。



Pod的所处阶段信息保存在PodStatus.Phase，Phase有如下几种值：

- **Pending**: API Server已经创建该Pod，但一个或多个容器还没有被创建，包括通过网络下载镜像的过程。
- **Running:** Pod中的所有容器都已经被创建且已经调度到 Node 上面，但至少有一个容器还在运行或者正在启动。
- **Succeeded**: Pod 调度到 Node 上面后均成功运行结束，并且不会重启。
- **Failed**: Pod中的所有容器都被终止了，但至少有一个容器退出失败（即退出码不为 0 或者被系统终止）。
- **Unknonwn**: 状态未知，因为一些原因Pod无法被正常获取，通常是由于 apiserver 无法与 kubelet 通信导致。

Pod的生命周期示意图如下：

![img](../Image/2022/221215-2.jpg)



Phase记录的是Pod在其生命周期中的简单宏观概述。该阶段并不是对容器或Pod的综合汇总，也不是为了作为综合状态机。



Pod 遵循一个预定义的生命周期，起始于 Pending 阶段，如果至少 其中有一个主要容器正常启动，则进入 Running，之后取决于 Pod 中是否有容器以 失败状态结束而进入 Succeeded 或者 Failed 阶段。

在 Pod 运行期间，kubelet 能够重启容器以处理一些失效场景。 在 Pod 内部，Kubernetes 跟踪不同容器的状态 并确定使 Pod 重新变得健康所需要采取的动作。

在 Kubernetes API 中，Pod 包含规约部分和实际状态部分。 Pod 对象的状态包含了一组 Pod 状况（Conditions）。 如果应用需要的话，你也可以向其中注入自定义的就绪性信息。

Pod 在其生命周期中只会被调度一次。 一旦 Pod 被调度（分派）到某个节点，Pod 会一直在该节点运行，直到 Pod 停止或者 被终止。

Pod 阶段：Pod 的 status 字段是一个 PodStatus 对象，其中包含一个 phase 字段。Pod 的阶段（Phase）是 Pod 在其生命周期中所处位置的简单宏观概述。 该阶段并不是对容器或 Pod 状态的综合汇总，也不是为了成为完整的状态机。

Pod 阶段的数量和含义是严格定义的。 下面是 phase 可能的值：

| 取值      | 描述                                                         |
| --------- | ------------------------------------------------------------ |
| Pending   | Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜像的时间。 |
| Running   | Pod 已经绑定到了某个节点，Pod 中所有的容器都已被创建。至少有一个容器仍在运行，或者正处于启动或重启状态。 |
| Succeeded | Pod 中的所有容器都已成功终止，并且不会再重启。               |
| Failed    | Pod 中的所有容器都已终止，并且至少有一个容器是因为失败终止。也就是说，容器以非 0 状态退出或者被系统终止。 |
| Unknown   | 因为某些原因无法取得 Pod 的状态。这种情况通常是因为与 Pod 所在主机通信失败。 |

如果某节点死掉或者与集群中其他节点失联，Kubernetes 会实施一种策略，将失去的节点上运行的所有 Pod 的 phase 设置为 Failed。

一般情况下，删除pod很慢，要等30秒才能删除pod

```shell
[root@k8scloude1 pod]# kubectl get pod
NAME    READY   STATUS             RESTARTS   AGE
nginx   1/1     Running            0          45m
pod1    1/2     CrashLoopBackOff   11         34m

#kubectl delete pod pod1 :删除很慢，要等30秒才能删除
[root@k8scloude1 pod]# kubectl delete pod pod1
pod "pod1" deleted
```



```sh
$ sudo kubectl create -f webapp_pod.yaml
pod/webapp created
$ sudo kubectl get pod
NAME     READY   STATUS              RESTARTS   AGE
webapp   0/2     ContainerCreating   0          3s
$ sudo kubectl get pod webapp -o jsonpath="{.status.phase}"
Pending
```



当创建Pod时，通过get pod能看到Pod状态为**ContainerCreating**，通过phase看到Pod所处的阶段是**Pending**，所以Phase描述Pod更为宏观的状态。



### Restart Strategy

Pod的重启策略应用与Pod内所有容器，并且仅在Pod所处的Node上由kubelet进行判断和操作，当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy进行对应操作。

restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅针对同一节点上 kubelet 的容器重启动作。当 Pod 中的容器退出时，kubelet 会按指数回退方式计算重启的延迟（10s、20s、40s、...），其最长延迟为 5 分钟。 一旦某容器执行了 10 分钟并且没有出现问题，kubelet 对该容器的重启回退计时器执行重置操作。



Pod重启策略如下：

- Always：当容器退出时，无论失败与否都重启
- OnFailure：故障了就重启，sleep 10只是正常到期不是故障，所以不会重启；
- Never：无论容器状态如何，都不重启



Pod重启策略与控制方式息息相关，可管理Pod的控制器有：RC、Job、DaemonSet、kubelet（静态Pod）等，每种控制器对Pod的重启策略要求如下：

- RC和DaemonSet：必须设置为Always，确保容器持续运行
- Job：可设置为OnFailure或Never，确保容器执行完成后释放资源不再重启
- kubelet：不论设置什么样的重启策略，其在Pod失效时都会自动重启



结合Pod的生命周期和重启策略，我们可得出下表中Pod状态切换过程：

| Pod容器数 | Pod当前阶段 | 发生事件        | Always重启策略 | OnFailure重启策略 | Never重启策略 |
| --------- | ----------- | --------------- | -------------- | ----------------- | ------------- |
| 1个容器   | Running     | 容器正常退出    | Running        | Successed         | Successed     |
| 1个容器   | Running     | 容器退出失败    | Running        | Running           | Failed        |
| 2个容器   | Running     | 1个容器退出失败 | Running        | Running           | Running       |
| 2个容器   | Running     | 容器被OOM杀掉   | Running        | Running           | Failed        |



### Health Check

Kubernetes对Pod的健康检查可通过三类探针来完成：

- **LivenessProbe**：探测容器是否正在运行。如果探测容器不健康，则 kubelet 会杀死容器， 并且容器将根据其重启策略决定后续操作。如果容器不包含此探针， 则默认状态为 Success。
- **ReadinessProbe**：探测容器是否准备好为请求提供服务。如果就探测失败， 系统将从与 Pod 匹配的所有服务的EndPoint列表中删除该 Pod 的 IP 地址。如果容器不包含此探针，则默认状态为 Success。
- **StartupProbe**: 探测容器中的应用是否已经启动。如果提供了此探针，则所有其他探针都会被禁用，直到此探针成功为止。如果探测失败，kubelet 将杀死容器，然后容器按照其重启策略进行后续操作。如果容器不包含此探测，则默认状态为 Success。

以上探针均可配置以下三种实现方式：

- **ExecAction**：在容器内执行指定命令。如果命令退出码为 0 则认为诊断成功。
- **TCPSockerAction**：对容器的 IP 地址上的指定端口执行 TCP 检查。如果端口打开，则诊断被认为是成功的。
- **HTTPGetAction**：对容器的 IP 地址上指定端口和URL执行 Get 请求。如果响应的状态码大于等于 200 且小于 400，则诊断被认为是成功的。

每次探测都将获得以下三种结果之一：

- **Success（成功）**：容器通过了诊断。
- **Failure（失败）**：容器未通过诊断。
- **Unknown（未知）**：诊断失败，因此不会采取任何行动。



### Pod Hook

为容器的生命周期事件设置处理函数，Kubernetes 支持 postStart 和 preStop 事件。 当一个容器启动后，Kubernetes 将立即发送 postStart 事件；在容器被终结之前， Kubernetes 将发送一个 preStop 事件。容器可以为每个事件指定一个处理程序。

pod hook：目前pod3容器里运行的是nginx进程，在启动容器的时候，除了主进程，还想启动一个进程，这时候就需要使用pod hook(pod 钩子)，pod hook有两个选项：

- postStart:容器启动之后执行XXXX，和主进程是同时运行起来的，并没有先后顺序；
- preStop:在容器关闭之前执行XXXX



#### postStart

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod3
  name: pod3
spec:
  terminationGracePeriodSeconds: 0
  containers:
  - image: nginx
    command: ["sh","-c","date > /tmp/aa.txt ; sleep 10000"]
    imagePullPolicy: IfNotPresent
    name: n1
    resources: {}
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh","-c","date >> /tmp/bb.txt"]
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```



```sh
$ kubectl apply -f pod3.yaml 
pod/pod3 created
```



文件/tmp/aa.txt /tmp/bb.txt 时间是一致的，就说明两个命令是同时运行的，证明了postStart：容器启动之后执行XXXX,和主进程是同时运行起来的，并没有先后顺序。

```sh
$ kubectl exec -it pod3 -- bash
root@pod3:/# cat /tmp/aa.txt /tmp/bb.txt 
Thu Jan 13 07:40:24 UTC 2022
Thu Jan 13 07:40:24 UTC 2022
root@pod3:/# exit
exit
```



#### preStop

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod3
  name: pod3
spec:
  terminationGracePeriodSeconds: 600
  containers:
  - image: nginx
    command: ["sh","-c","date > /tmp/aa.txt ; sleep 10000"]
    imagePullPolicy: IfNotPresent
    name: n1
    resources: {}
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh","-c","date >> /tmp/bb.txt"]
      preStop:
        exec:
          command: ["/bin/sh","-c","date >> /tmp/bb.txt ; sleep 100"]
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```



运行一段时间后，删除pod，在容器关闭之前执行preStop的命令，preStop执行完成之后，主程序还要运行10000秒，但是宽限期terminationGracePeriodSeconds只有600s，所以600秒之后pod被删除。



## Init Containers

初始化容器Init Containers是一种特殊容器，在 Pod 内的应用容器启动之前运行。Init 容器可以包括一些应用镜像中不存在的实用工具和安装脚本可以在 Pod 的规约中与用来描述应用容器的 containers 数组平行的位置指定 Init 容器。



每个 Pod 中可以包含多个容器， 应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。Init 容器与普通的容器非常像，除了如下两点：

- 它们总是运行到完成。
- 每个都必须在下一个启动之前成功完成。

如果 Pod 的 Init 容器失败，kubelet 会不断地重启该 Init 容器直到该容器成功为止。 然而，如果 Pod 对应的 restartPolicy 值为 "Never"，并且 Pod 的 Init 容器失败， 则 Kubernetes 会将整个 Pod 状态设置为失败。

为 Pod 设置 Init 容器需要在 Pod 规约中添加 initContainers 字段， 该字段以 Container 类型对象数组的形式组织，和应用的 containers 数组同级相邻。 Init 容器的状态在 status.initContainerStatuses 字段中以容器状态数组的格式返回 （类似 status.containerStatuses 字段）。



Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。 然而，Init 容器对资源请求和限制的处理稍有不同，同时 Init 容器不支持 lifecycle、livenessProbe、readinessProbe 和 startupProbe， 因为它们必须在 Pod 就绪之前运行完成。

如果为一个 Pod 指定了多个 Init 容器，这些容器会按顺序逐个运行。 每个 Init 容器必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时， Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。



因为 Init 容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：

- Init 容器可以包含一些安装过程中应用容器中不存在的实用工具或个性化代码。 例如，没有必要仅为了在安装过程中使用类似 sed、awk、python 或 dig 这样的工具而去 FROM 一个镜像来生成一个新的镜像。
- 应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。
- 与同一 Pod 中的多个应用容器相比，Init 容器能以不同的文件系统视图运行。因此，Init 容器可以被赋予访问应用容器不能访问的 Secret 的权限。
- 由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。 一旦前置条件满足，Pod 内的所有的应用容器会并行启动。
- Init 容器可以安全地运行实用程序或自定义代码，而在其他方式下运行这些实用程序或自定义代码可能会降低应用容器镜像的安全性。 通过将不必要的工具分开，你可以限制应用容器镜像的被攻击范围。



### Create InitContainers

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: initc
  name: initc
spec:
  terminationGracePeriodSeconds: 0
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: n1
    resources: {}
  initContainers:
  - name: initc
    image: nginx
    imagePullPolicy: IfNotPresent
    command: ["sh","-c","sleep 15"]
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
```



```sh
$ kubectl apply -f initcontainer.yaml    
pod/initc created
```



查看pod可以发现，初始化容器休眠了15秒之后，才执行应用容器。

```sh
$ kubectl get pod
NAME    READY   STATUS     RESTARTS   AGE
initc   0/1     Init:0/1   0          1s

$ kubectl get pod
NAME    READY   STATUS     RESTARTS   AGE
initc   0/1     Init:0/1   0          15s

$ kubectl get pod
NAME    READY   STATUS            RESTARTS   AGE
initc   0/1     PodInitializing   0          17s

$ kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
initc   1/1     Running   0          18s
```



### Modify Core Param

在容器里修改内核参数，实际上修改的就是物理机的内核参数，为了安全性，一般不允许在容器里修改内核参数，seccom控制了容器能做哪些操作，添加securityContext参数之后就可以修改内核参数了。

创建一个pod，initContainers初始化容器里的securityContext:privileged: true表示该容器具有特权，可以执行命令`"sh","-c","/sbin/sysctl -w vm.swappiness=0"`，vm.swappiness设置为0表示尽量少使用swap内存。

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: initc
  name: initc
spec:
  terminationGracePeriodSeconds: 0
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: n1
    resources: {}
  initContainers:
  - name: initc
    image: alpine
    imagePullPolicy: IfNotPresent
    command: ["sh","-c","/sbin/sysctl -w vm.swappiness=0"]
    securityContext:
      privileged: true
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
```



```shell
$ kubectl apply -f initcontainer2.yaml 
pod/initc created

$ kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
initc   1/1     Running   0          7s

#可以看到pod运行在k8scloude2节点
$ kubectl get pods -o wide
NAME    READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
initc   1/1     Running   0          22s   10.244.112.147   k8scloude2   <none>           <none>
```

查看k8scloude2机器的swappiness ，此时k8scloude2机器上的swappiness变为了0，swappiness修改为0之后，nginx容器才可以运行

```shell
$ cat /proc/sys/vm/swappiness 
0
```



### Default InitContainer Pause

**在k8s中，每一个pod都会有一个掩藏的初始化容器pause**。

创建一个普通的Nginx pod，查看该pod运行在哪个node，然后通过在该node机器上执行docker ps | grep nginx，可以看到初始化容器pause。

```sh
#这是pause镜像
$ docker images | grep pause
registry.aliyuncs.com/google_containers/pause        3.4.1     0f8457a4c2ec   12 months ago   683kB

$ docker ps | grep nginx
d00a5991dd5b   605c77e624dd                                          "/docker-entrypoint.…"   27 seconds ago   Up 27 seconds             k8s_nginx_nginx_pod_8aaab455-5e27-4c65-815d-2a23f76e758b_0
595918a44478   registry.aliyuncs.com/google_containers/pause:3.4.1   "/pause"                 28 seconds ago   Up 27 seconds             k8s_POD_nginx_pod_8aaab455-5e27-4c65-815d-2a23f76e758b_0
```



## Static Pod

静态 Pod 在指定的节点上由 kubelet 守护进程直接管理，不需要 API 服务器监管。 与由控制面管理的 Pod（例如，Deployment） 不同；kubelet 监视每个静态 Pod（在它失败之后重新启动）。**静态 Pod 始终都会绑定到特定节点的 Kubelet 上**。

kubelet 会尝试通过 Kubernetes API 服务器为每个静态 Pod 自动创建一个镜像 Pod。 这意味着节点上运行的静态 Pod 对 API 服务来说是可见的，但是不能通过 API 服务器来控制。 Pod 名称将把以连字符开头的节点主机名作为后缀。

**说明**：如果你在运行一个 Kubernetes 集群，并且在每个节点上都运行一个静态 Pod， 就可能需要考虑使用 DaemonSet 替代这种方式。静态 Pod 的 spec 不能引用其他 API 对象 （如：ServiceAccount、 ConfigMap、 Secret 等）。



### Create Static Pod

静态pod的**应用场景**为：1.使master能正常启动 2.如果某天我们的master崩溃了，如何让别人知道我们的服务器在维护？
目前该命名空间是没有pod运行的

```shell
$ kubectl get pods
No resources found in pod namespace.
```

静态pod的创建方法为：写一个yaml文件，然后把yaml文件放在指定目录，会自动根据yaml文件创建pod。有两种方法来指定这个目录：

1. --pod-manifest-path
2. /etc/kubernetes/manifests



#### Create By Args

查看kubelet的配置文件位置，可以看到kubelet的配置文件在/usr/lib/systemd/system/kubelet.service

**注意**：我们是在k8s集群的worker节点k8scloude2上创建静态pod的

```shell
[root@k8scloude2 ~]# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since 六 2022-01-15 12:27:34 CST; 5h 30min ago
     Docs: https://kubernetes.io/docs/
 Main PID: 947 (kubelet)
   Memory: 122.6M
   CGroup: /system.slice/kubelet.service
           └─947 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infr...
```

修改kubelet的配置文件/usr/lib/systemd/system/kubelet.service，使用--pod-manifest-path=/etc/kubernetes/kubelet.d指定静态pod目录。

```shell
[root@k8scloude2 ~]# vim /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 

#--pod-manifest-path=/etc/kubernetes/kubelet.d表示静态pod的目录为/etc/kubernetes/kubelet.d
[root@k8scloude2 ~]# cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --pod-manifest-path=/etc/kubernetes/kubelet.d"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/sysconfig/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
```

创建静态pod目录，并使kubelet配置文件生效

```shell
[root@k8scloude2 ~]# mkdir /etc/kubernetes/kubelet.d

[root@k8scloude2 ~]# systemctl daemon-reload 

[root@k8scloude2 ~]# systemctl restart kubelet

[root@k8scloude2 ~]# systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /usr/lib/systemd/system/kubelet.service.d
           └─10-kubeadm.conf
   Active: active (running) since 六 2022-01-15 18:02:15 CST; 6s ago
     Docs: https://kubernetes.io/docs/
 Main PID: 108844 (kubelet)
   Memory: 30.7M
   CGroup: /system.slice/kubelet.service
           ├─108844 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --pod-manifest-path=/etc/kubernetes/kubelet.d --config=/var/lib/ku...
           └─108999 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --pod-manifest-path=/etc/kubernetes/kubelet.d --config=/var/lib/ku...
```

进入静态pod的目录，然后创建pod yaml文件

```shell
[root@k8scloude2 ~]# cd /etc/kubernetes/kubelet.d/

[root@k8scloude2 kubelet.d]# ls

[root@k8scloude2 kubelet.d]# vim pod.yaml

[root@k8scloude2 kubelet.d]# cat pod.yaml 
```

```yml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod
  name: pod
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod
    resources: {}
    ports:
    - name: http
      containerPort: 80
      protocol: TCP
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```



在k8s集群的master节点上查看pod，在master上可以看到该pod，由于pod.yaml没有指定namespace，默认在default下，**可以看到Pod 名称（pod-k8scloude2）是以连字符开头的节点主机名作为后缀**。

```shell
[root@k8scloude1 pod]# kubectl get pods -n default -o wide
NAME             READY   STATUS    RESTARTS   AGE    IP               NODE         NOMINATED NODE   READINESS GATES
pod-k8scloude2   1/1     Running   0          109s   10.244.112.153   k8scloude2   <none>           <none>
```

当把这个pod.yaml文件从静态pod目录移走，pod就消失了

```shell
[root@k8scloude2 kubelet.d]# mv pod.yaml ~/

[root@k8scloude2 kubelet.d]# ls

#当把这个yaml文件移走,pod消失
[root@k8scloude1 pod]# kubectl get pods -n default -o wide
No resources found in default namespace.
```

现在指定静态pod的namespace为pod

```shell
[root@k8scloude2 kubelet.d]# vim pod.yaml 

#namespace: pod:指定pod的命名空间
[root@k8scloude2 kubelet.d]# cat pod.yaml 
[root@k8scloude2 kubelet.d]# ls
pod.yaml
```



```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod
  name: pod
  namespace: pod
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod
    resources: {}
    ports:
    - name: http
      containerPort: 80
      protocol: TCP
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```



查看pod

```shell
[root@k8scloude1 pod]# kubectl get pods -n pod
NAME             READY   STATUS    RESTARTS   AGE
pod-k8scloude2   1/1     Running   0          6s
```



#### Create By Default Dir

进行这一步的时候，先还原kubelet配置文件/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

查看配置文件，可以发现静态pod默认目录为/etc/kubernetes/manifests

```shell
[root@k8scloude2 kubelet.d]# cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf 
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/sysconfig/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS

[root@k8scloude2 kubelet.d]# ls /var/lib/kubelet/config.yaml
/var/lib/kubelet/config.yaml

[root@k8scloude2 kubelet.d]# cat /var/lib/kubelet/config.yaml | grep manifest
staticPodPath: /etc/kubernetes/manifests

#默认的静态pod的目录为 ls /etc/kubernetes/manifests
[root@k8scloude2 kubelet.d]# ls /etc/kubernetes/manifests
```

在默认的静态pod目录/etc/kubernetes/manifests/下创建pod yaml文件

```shell
#namespace: pod:指定pod的命名空间
[root@k8scloude2 kubelet.d]# vim ~/pod.yaml 

[root@k8scloude2 kubelet.d]# cat ~/pod.yaml 
[root@k8scloude2 kubelet.d]# cp ~/pod.yaml /etc/kubernetes/manifests/
```

```yml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod
  name: pod
  namespace: pod
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod
    resources: {}
    ports:
    - name: http
      containerPort: 80
      protocol: TCP
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```



可以发现，静态pod已经创建了

```shell
[root@k8scloude1 pod]# kubectl get pods -n pod
NAME             READY   STATUS    RESTARTS   AGE
pod-k8scloude2   1/1     Running   0          6s
```

删除yaml文件，静态pod消失

```shell
#删除yaml文件
[root@k8scloude2 kubelet.d]# rm -rf /etc/kubernetes/manifests/pod.yaml 

#pod消失
[root@k8scloude1 pod]# kubectl get pods -n pod
No resources found in pod namespace.
```

前面几步，静态pod是在k8s集群的worker节点上做的，现在在k8s集群的master节点上做。

**注意**：如果在/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf里添加了--pod-manifest-path=/etc/kubernetes/kubelet.d，则相应的/etc/kubernetes/manifests/下的yaml文件也要移动到/etc/kubernetes/kubelet.d目录下，不然k8s集群的master节点启动不起来。

可以看到k8s集群的master节点有很多静态pod。

```shell
[root@k8scloude1 pod]# ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
```



## Yaml Comfiguration

```yaml
apiVersion: v1
# 资源类型
kind: Pod
metadata:
  creationTimestamp: null
  # 资源标签
  labels:
    run: nginx
  # 资源名称
  name: nginx
  # pod命名空间
  namespace: pod
spec:
  # 优雅的终止pod需要时间，默认是30s。修改为0，可以立马删除pod
  terminationGracePeriodSeconds: 30
  # 定义容器
  containers:
  # 镜像名称
  - image: nginx
    # 镜像下载策略
    imagePullPolicy: IfNotPresent
    # 启动容器命令，没有则执行镜像中指定的CMD命令
    command: ["sh","-c","sleep 1000"]
    # 容器名
    name: nginx
    # 容器资源（CPU、内存等）
    resources: {}
    lifecycle:
      # 容器启动后执行钩子
      postStart:
        exec: 
          command: ["/bin/sh","-c","date >> /tmp/bb.txt"]
      # 容器结束后执行钩子
      preStop:
        exec:
          command: ["/bin/sh","-c","date >> /tmp/bb.txt ; sleep 100"]
    # 容器端口
    ports:
    - name: http
      containerPort: 80
      protocol: TCP
    # 容器中的环境变量
    env:
    - name: xx
      value: "12"
    - name: yy
      value: "21"
    - name: zz
      value: hello
  # 初始容器
  initContainers:
  - name: initc
    image: nginx
    imagePullPolicy: IfNotPresent
    command: ["sh","-c","sleep 15"]
    # 表示该容器具有特权，可以修改内核参数
    securityContext:
      privileged: true
  # DNS策略
  dnsPolicy: ClusterFirst
  # 重启策略，其可能取值包括 Always（默认）、OnFailure和Never
  restartPolicy: Always
status: {}
```



**当pod里有多个容器的时候，containers可以定义多个，每个容器是一个对象，每个容器的第一个变量就加-。**



## Basic Use

在使用Pod前我们需要注意，在Kubernetes中对于长时间运行的容器的要求是：其主程序需要一直在前台运行。如果主程序运行在后台，则Kubernetes会认为Pod执行结束，将会销毁Pod。以webapp镜像为例，它的Dockerfile如下：

```dockerfile
FROM java:8
WORKDIR /opt/soft/
EXPOSE 4567
COPY webapp-1.0-SNAPSHOT.jar /opt/soft/webapp.jar
ENTRYPOINT ["java", "-jar", "/opt/soft/webapp.jar"]
```



Pod包含两个容器：webapp和busybox，Pod定义如下：

```yml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  containers:
  - name: webapp
    image: 172.16.194.135:5000/webapp:1.0
    ports:
    - containerPort: 5000
  - name: busybox
    image: busybox
    command: ["sh", "-c", "top"]
```



busybox容器中我们定义了启动命令top，这样做就是为了确保busybox容器始终在前台运行top命令，避免容器直接被销毁。

创建Pod，并通过describe可以清楚看到这两个容器的创建过程：

```sh
$ sudo kubectl create -f webapp_pod.yaml
pod/webapp created
$ sudo kubectl describe pod webapp
Name:         webapp
Namespace:    default
Priority:     0
Node:         ayato/172.16.194.135
Start Time:   Sat, 08 Jan 2022 05:49:38 +0000
Labels:       app=webapp
Annotations:  <none>
Status:       Running
IP:           172.17.0.6
IPs:
  IP:  172.17.0.6
Containers:
  webapp:
    Container ID:   docker://9c68ef7019126b65e2feba5d4d69e55997a9e573ce585b0bbb6a7cfe2fe20b31
    Image:          172.16.194.135:5000/webapp:1.0
    Image ID:       docker-pullable://172.16.194.135:5000/webapp@sha256:df3a447a013ada0642dec67bb31976f42f1a0699a68873d0452f514fa24e5c77
    Port:           5000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Sat, 08 Jan 2022 05:49:40 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pcr2h (ro)
  busybox:
    Container ID:  docker://0dfd00b5fa8e419bfe0b4a43595c83cb1d4986980914865ae3371e1724c7f568
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:5acba83a746c7608ed544dc1533b87c737a0b0fb730301639a0179f9344b1678
    Port:          <none>
    Host Port:     <none>
    Command:
      sh
      -c
      top
    State:          Running
      Started:      Sat, 08 Jan 2022 05:49:45 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pcr2h (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-pcr2h:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-pcr2h
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  3m50s  default-scheduler  Successfully assigned default/webapp to ayato
  Normal  Pulled     3m49s  kubelet            Container image "172.16.194.135:5000/webapp:1.0" already present on machine
  Normal  Created    3m48s  kubelet            Created container webapp
  Normal  Started    3m48s  kubelet            Started container webapp
  Normal  Pulling    3m48s  kubelet            Pulling image "busybox"
  Normal  Pulled     3m44s  kubelet            Successfully pulled image "busybox" in 4.516692787s
  Normal  Created    3m44s  kubelet            Created container busybox
  Normal  Started    3m43s  kubelet            Started container busybox
```



同一个Pod中的容器共享网络，也就是说我们在busybox容器中可以通过localhost访问webapp的接口，我们尝试一下：

```sh
$ sudo kubectl exec -it webapp -c busybox /bin/sh
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
/ # wget http://localhost:4567/api/hello
Connecting to localhost:4567 (127.0.0.1:4567)
saving to 'hello'
hello                100% |******************************************************************************************|    15  0:00:00 ETA
'hello' saved
/ # cat hello
Hello my friend/ #
```



## Sharing Volume

同一个Pod中的容器能够共享Pod级别的Volume，Volume可以被定义为各种类型，多个容器分别进行挂载操作。我们还是以webapp和busybox为例，webapp向volume中写log，busybox通过tail命令读log，Pod定义如下：

```yml
apiVersion: v1
kind: Pod
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  containers:
  - name: webapp
    image: 172.16.194.135:5000/webapp:1.0
    ports:
    - containerPort: 5000
    volumeMounts:
    - name: webapp-logs
      mountPath: /tmp
  - name: busybox
    image: busybox
    command: ["sh", "-c", "tail -f /logs/log.out"]
    volumeMounts:
    - name: webapp-logs
      mountPath: /logs
  volumes:
  - name: webapp-logs
    emptyDir: {}
```



我们通过Pod定义中可以看到：我们设置了一个Volume，名称为webapp-logs，type为emptyDir。容器webapp将Volume挂载到/tmp目录，因为webapp配置了logback并会向/tmp中写日志。容器busybox将Volume挂载到/logs目录，并通过tail命令持续读日志。我们启动Pod，并使用kubectl logs命令从busybox中读取tail的输出：

```sh
$ sudo kubectl create -f webapp_pod.yaml
pod/webapp created
$ sudo kubectl logs webapp -c busybox
06:30:27.810 [INFO ] [main] [org.demo.webapp.todolist.TodoListApplication] Starting TodoListApplication v1.0-SNAPSHOT using Java 1.8.0_111 on webapp with PID 1 (/opt/soft/webapp.jar started by root in /opt/soft)
06:30:27.821 [INFO ] [main] [org.demo.webapp.todolist.TodoListApplication] No active profile set, falling back to default profiles: default
06:30:30.060 [INFO ] [main] [org.springframework.boot.web.embedded.tomcat.TomcatWebServer] Tomcat initialized with port(s): 4567 (http)
06:30:30.079 [INFO ] [main] [org.apache.coyote.http11.Http11NioProtocol] Initializing ProtocolHandler ["http-nio-4567"]
06:30:30.088 [INFO ] [main] [org.apache.catalina.core.StandardService] Starting service [Tomcat]
06:30:30.089 [INFO ] [main] [org.apache.catalina.core.StandardEngine] Starting Servlet engine: [Apache Tomcat/9.0.41]
06:30:30.359 [INFO ] [main] [org.apache.catalina.core.ContainerBase.[Tomcat].[localhost].[/]] Initializing Spring embedded WebApplicationContext
06:30:30.359 [INFO ] [main] [org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext] Root WebApplicationContext: initialization completed in 2407 ms
06:30:30.913 [INFO ] [main] [org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor] Initializing ExecutorService 'applicationTaskExecutor'
06:30:32.634 [WARN ] [main] [org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration$DefaultTemplateResolverConfiguration] Cannot find template location: classpath:/templates/ (please add some templates or check your Thymeleaf configuration)
06:30:32.956 [INFO ] [main] [org.apache.coyote.http11.Http11NioProtocol] Starting ProtocolHandler ["http-nio-4567"]
06:30:33.096 [INFO ] [main] [org.springframework.boot.web.embedded.tomcat.TomcatWebServer] Tomcat started on port(s): 4567 (http) with context path ''
06:30:33.131 [INFO ] [main] [org.demo.webapp.todolist.TodoListApplication] Started TodoListApplication in 6.387 seconds (JVM running for 7.205)
```



关于Volume的类型，有如下几种：

1）emptyDir：emptyDir是在Pod分配到Node时创建的，它的初始内容为空，并且无须指定host上对应的目录文件，它是由Kubernetes自动分配的目录，当Pod销毁后，emptyDir中的数据也会被删除。一般可用作临时空间，存放应用程序临时数据。

2）hostPath：将宿主机中的文件或目录挂载到Pod中。通常用于应用永久数据的存储。

3）iscsi：将iSCSI存储设备上的目录挂载到Pod中。

4）nfs：将NFS上的目录挂载到Pod中。

5）glusterfs：将GlusterFS网络文件系统的目录挂载到Pod中。

6）rbd：将Ceph块设备共享存储挂载到Pod中。

7）gitRepo：通过挂载一个空目录，并从Git中克隆一个git仓库供Pod使用。

8）configmap：将配置数据挂载为容器中的文件。

9）secret：将Secret数据挂载为容器中的文件。



## Multi Container

一个pod里可以有一个容器，也可以有多个容器。一个pod里运行两个容器的意义在于一个容器是主容器，一个是副容器sidecar。比如nginx容器用来提供服务，另外一个容器使用工具来进行日志分析，两个容器挂载同一个数 据卷，日志分析容器读取数据卷即可分析日志。



### Create Multi Container Pod

对于containers，如果没有指定command参数，则运行的进程为nginx镜像里指定的CMD那个进程，如果有command参数，则运行command指定的进程，不运行nginx镜像里CMD的进程。



## Downward API

Kubernetes在创建Pod时，会为Pod和容器设置一些额外的信息，比如Pod名称、Pod IP、Node IP、Label、Annotation、资源限制等，我们经常会在应用程序中使用到这些数据，比如利用Pod名称作为应用日志的字段，方便分析日志。为了能在容器内获取这些信息，我们可以使用Downward API机制来实现。

我们可以通过Downward API向容器注入如下信息：

1）可通过fieldRef获得的信息：

- metadata.name：Pod 名称
- metadata.namespace：Pod 名字空间
- metadata.uid：Pod 的 UID
- metadata.labels['<KEY>']：Pod标签 <KEY> 的值 (例如, metadata.labels['mylabel']）
- metadata.annotations['<KEY>']：Pod 的注解 <KEY> 的值（例如, metadata.annotations['myannotation']）
- metadata.labels：获取所有标签
- metadata.annotations：获取所有注解
- status.podIP：节点 IP
- spec.serviceAccountName：Pod 服务帐号名称, 版本要求v1.4.0-alpha.3
- spec.nodeName：节点名称, 版本要求 v1.4.0-alpha.3
- status.hostIP：节点 IP, 版本要求 v1.7.0-alpha.1

2）可通过 resourceFieldRef 获得的信息：

- 容器的 CPU 约束值
- 容器的 CPU 请求值
- 容器的内存约束值
- 容器的内存请求值
- 容器的巨页限制值（前提是启用了DownwardAPIHugePages 特性门控）
- 容器的巨页请求值（前提是启用了DownwardAPIHugePages 特性门控）
- 容器的临时存储约束值
- 容器的临时存储请求值



Downward API可以通过**环境变量**和**Volume挂载**这两种方式将Pod信息注入容器。



### 环境变量方式

我们还是以Busybox为例进行演示，我们将Pod信息和Container信息以环境变量方式注入容器，在容器启动后通过env命令打印出来，我们Yaml文件内容如下：

```yml
apiVersion: v1
kind: Pod
metadata:
  name: busybox-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh", "-c", "env | grep VAR_"]
    resources:
      requests:
        memory: "16Mi"
        cpu: "125m"
      limits:
        memory: "32Mi"
        cpu: "250m"
    env:
    - name: VAR_NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: VAR_POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name
    - name: VAR_POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: VAR_POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: VAR_SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          fieldPath: spec.serviceAccountName
    - name: VAR_CPU_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: busybox
          resource: requests.cpu
    - name: VAR_CPU_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: busybox
          resource: limits.cpu
    - name: VAR_MEM_REQUEST
      valueFrom:
        resourceFieldRef:
          containerName: busybox
          resource: requests.memory
    - name: VAR_MEM_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: busybox
          resource: limits.memory
  restartPolicy: Never
```



我们创建Pod并使用kubtctl logs命令打印下输出：

```sh
$ sudo kubectl apply -f busy_pod.yaml
pod/busybox-pod created
$ sudo kubectl logs busybox-pod
VAR_MEM_REQUEST=16777216      # 容器内存请求值
VAR_NODE_NAME=ayato           # 节点名称
VAR_SERVICE_ACCOUNT=default   # Pod使用的ServiceAccount名称
VAR_CPU_REQUEST=1             # 容器cpu请求值
VAR_POD_NAME=busybox-pod      # pod名称
VAR_MEM_LIMIT=33554432        # 容器内存限制值
VAR_POD_NAMESPACE=default     # Pod所在命名空间
VAR_POD_IP=172.17.0.6         # Pod ip地址
VAR_CPU_LIMIT=1               # 容器cpu请求值
```



### Volume挂载方式

我们接下来尝试使用Volume挂载方式，将Pod信息注入容器。还是以Busybox为例，由于Pod信息都是以文件方式注入容器，所以我们修改容器启动后执行命令：我们使用cat不断打印注入的文件，修改后的Yaml文件如下：

```yml
apiVersion: v1
kind: Pod
metadata:
  name: busybox-pod
  labels:
    cluster: demo-cluster
    type: tool-pod
  annotations:
    builder: alalazy
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh", "-c"]
    args:
    - while true; do
        if [[ -e /etc/podinfo/labels ]]; then
          echo -en '\n\n'; cat /etc/podinfo/labels; fi;
        if [[ -e /etc/podinfo/annotations ]]; then
          echo -en '\n\n'; cat /etc/podinfo/annotations; fi;
        if [[ -e /etc/podinfo/cpu_limit ]]; then
          echo -en '\n\n'; cat /etc/podinfo/cpu_limit; fi;
        if [[ -e /etc/podinfo/cpu_request ]]; then
          echo -en '\n\n'; cat /etc/podinfo/cpu_request; fi;
        if [[ -e /etc/podinfo/mem_limit ]]; then
          echo -en '\n\n'; cat /etc/podinfo/mem_limit; fi;
        if [[ -e /etc/podinfo/mem_request ]]; then
          echo -en '\n\n'; cat /etc/podinfo/mem_request; fi;
        sleep 5;
      done;
    volumeMounts:
      - name: podinfo
        mountPath: /etc/podinfo
    resources:
      requests:
        memory: "16Mi"
        cpu: "125m"
      limits:
        memory: "32Mi"
        cpu: "250m"
  volumes:
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations
          - path: "cpu_limit"
            resourceFieldRef:
              containerName: busybox
              resource: limits.cpu
              divisor: 1m
          - path: "cpu_request"
            resourceFieldRef:
              containerName: busybox
              resource: requests.cpu
              divisor: 1m
          - path: "mem_limit"
            resourceFieldRef:
              containerName: busybox
              resource: limits.memory
              divisor: 1Mi
          - path: "mem_request"
            resourceFieldRef:
              containerName: busybox
              resource: requests.memory
              divisor: 1Mi
```



我们创建此Pod，并通过kubectl logs查看输出：

```sh
$ sudo kubectl apply -f busy_pod.yaml
pod/busybox-pod created
$ sudo kubectl logs busybox-pod
cluster="demo-cluster"
type="tool-pod"

builder="alalazy"
kubectl.kubernetes.io/last-applied-configuration="{\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{\"builder\":\"alalazy\"},\"labels\":{\"cluster\":\"demo-cluster\",\"type\":\"tool-pod\"},\"name\":\"busybox-pod\",\"namespace\":\"default\"},\"spec\":{\"containers\":[{\"args\":[\"while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en '\\\\n\\\\n'; cat /etc/podinfo/labels; fi; if [[ -e /etc/podinfo/annotations ]]; then echo -en '\\\\n\\\\n'; cat /etc/podinfo/annotations; fi; if [[ -e /etc/podinfo/cpu_limit ]]; then echo -en '\\\\n\\\\n'; cat /etc/podinfo/cpu_limit; fi; if [[ -e /etc/podinfo/cpu_request ]]; then echo -en '\\\\n\\\\n'; cat /etc/podinfo/cpu_request; fi; if [[ -e /etc/podinfo/mem_limit ]]; then echo -en '\\\\n\\\\n'; cat /etc/podinfo/mem_limit; fi; if [[ -e /etc/podinfo/mem_request ]]; then echo -en '\\\\n\\\\n'; cat /etc/podinfo/mem_request; fi; sleep 5; done;\"],\"command\":[\"/bin/sh\",\"-c\"],\"image\":\"busybox\",\"name\":\"busybox\",\"resources\":{\"limits\":{\"cpu\":\"250m\",\"memory\":\"32Mi\"},\"requests\":{\"cpu\":\"125m\",\"memory\":\"16Mi\"}},\"volumeMounts\":[{\"mountPath\":\"/etc/podinfo\",\"name\":\"podinfo\"}]}],\"volumes\":[{\"downwardAPI\":{\"items\":[{\"fieldRef\":{\"fieldPath\":\"metadata.labels\"},\"path\":\"labels\"},{\"fieldRef\":{\"fieldPath\":\"metadata.annotations\"},\"path\":\"annotations\"},{\"path\":\"cpu_limit\",\"resourceFieldRef\":{\"containerName\":\"busybox\",\"divisor\":\"1m\",\"resource\":\"limits.cpu\"}},{\"path\":\"cpu_request\",\"resourceFieldRef\":{\"containerName\":\"busybox\",\"divisor\":\"1m\",\"resource\":\"requests.cpu\"}},{\"path\":\"mem_limit\",\"resourceFieldRef\":{\"containerName\":\"busybox\",\"divisor\":\"1Mi\",\"resource\":\"limits.memory\"}},{\"path\":\"mem_request\",\"resourceFieldRef\":{\"containerName\":\"busybox\",\"divisor\":\"1Mi\",\"resource\":\"requests.memory\"}}]},\"name\":\"podinfo\"}]}}\n"
kubernetes.io/config.seen="2022-01-15T05:33:53.379386410Z"
kubernetes.io/config.source="api"

250

125

32

16
```



进入容器查看下挂载的文件：

```sh
$  sudo kubectl exec -it busybox-pod -- sh
/ # cd /etc/podinfo/
/etc/podinfo # ls
annotations  cpu_limit    cpu_request  labels       mem_limit    mem_request
```



## Scheduling Strategy

### NodeSelector

节点定向调度，Kubernetes的Scheduler服务在调度Pod的时候会通过一系列复杂的算法自动计算出每一个Pod的最佳目标节点，但有的时候，我们需要将Pod指定的到一些Node上，比如我们有的Node安装了SSD，磁盘读写高，可以部署一些IO密集型应用，这个时候，我们就需要给这些Node打标签，然后在Pod中定义NodeSelector就可以实现。接下来我们来看一些如何实现：



1、我们通过kubectl label命令给集群中kubevm2打标签：disk=ssd，标示我们kubevm2的磁盘使用SSD。

```sh
$ kubectl get nodes -o wide
NAME      STATUS   ROLES    AGE   VERSION    INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME
kubevm1   Ready    master   33d   v1.19.16   192.168.56.120   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://1.13.1
kubevm2   Ready    <none>   33d   v1.19.16   192.168.56.121   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://1.13.1
kubevm3   Ready    <none>   33d   v1.19.16   192.168.56.122   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://1.13.1
$ kubectl label nodes kubevm2 disk=ssd
```



2、我们在之前deployment的例子中的Pod定义中增加nodeSelector设置：

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
      nodeSelector:
        disk: ssd
```



3、创建此Deployment，查看pod部署位置：我们发现三个副本都部署在kubevm2上。

```sh
[root@kubevm1 workspace]# kubectl create -f demo_deployment.yml
deployment.apps/nginx-deployment created
[root@kubevm1 workspace]# kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE    IP            NODE      NOMINATED NODE   READINESS GATES
nginx-deployment-758754644-d2qg5   1/1     Running   0          101s   10.244.1.26   kubevm2   <none>           <none>
nginx-deployment-758754644-tsf9f   1/1     Running   0          101s   10.244.1.27   kubevm2   <none>           <none>
nginx-deployment-758754644-vxdkm   1/1     Running   0          101s   10.244.1.25   kubevm2   <none>           <none>
```



如果我们给多个Node都定义了相同的标签，则Scheduler会从这组Node中挑选一个可用的Node。通过给Node打标签的方式，可以描述集群中具有不同特点的Node，在部署应用时通过结合应用的需求设置NodeSelector进行指定Node范围的调度。如果指定了NodeSelector条件，但集群中不存在包含相应标签的Node，即使集群中还有其他可供使用的node，这个Pod也无法被成功调度。

就那上面的例子来说，希望应用部署在SSD节点上，但如果集群中没有SSD的节点了，使用机械硬盘的节点也能部署，这样的话，使用NodeSelector就无法满足这一需求了。这个时候就需要使用节点亲和性调度了。



### NodeAffinity

节点亲和性调度，NodeAffinity是用于替换NodeSelector的全新调度策略，目前该策略有两种表达方式：

- **RequiredDuringSchedulingIgnoredDuringExecution**：调度器只有在规则被满足的时候才能执行调度。此功能类似于 nodeSelector，但其语法表达能力更强
- **PreferredDuringSchedulingIgnoredDuringExecution**：调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。



我们可以看到RequiredDuringSchedulingIgnoredDuringExecution 的特性跟NodeSelector很像，而PreferredDuringSchedulingIgnoredDuringExecution更加灵活一些，我们接下来看一下二者的用法：

1、RequiredDuringSchedulingIgnoredDuringExecution使用

```sh
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: disk
                operator: In
                values:
                - ssd
```



我们创建此Deployment，可以看到Pod豆部署在kubevm2上：

```sh
$ kubectl create -f demo_deployment.yml
deployment.apps/nginx-deployment created
$ kubectl get pods -o wide
NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
nginx-deployment-6ff7bf994d-9dhl2   1/1     Running   0          63s   10.244.1.28   kubevm2   <none>           <none>
nginx-deployment-6ff7bf994d-c8xzh   1/1     Running   0          63s   10.244.1.29   kubevm2   <none>           <none>
nginx-deployment-6ff7bf994d-xlpg8   1/1     Running   0          63s   10.244.1.30   kubevm2   <none>           <none>
```



2、PreferredDuringSchedulingIgnoredDuringExecution的使用

我们将kubevm2、kubevm3打标签：disk=sata：

```sh
$ kubectl label nodes kubevm2 disk=sata
node/kubevm2 labeled
$ kubectl label nodes kubevm3 disk=sata
node/kubevm3 labeled
```



修改deployment配置：

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: disk
                operator: In
                values:
                - ssd
              - key: disk
                operator: In
                values:
                - ssd
```



我们创建此Deployment可以看到，即使当前集群中没有ssd的node，Pod仍然可以部署：

```sh
$ kubectl create -f demo_deployment.yml
deployment.apps/nginx-deployment created
$ kubectl get pods -o wide
NAME                               READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
nginx-deployment-cff9b7cf7-26hww   1/1     Running   0          81s   10.244.2.15   kubevm3   <none>           <none>
nginx-deployment-cff9b7cf7-k9x47   1/1     Running   0          81s   10.244.1.31   kubevm2   <none>           <none>
nginx-deployment-cff9b7cf7-xtrq4   1/1     Running   0          81s   10.244.1.32   kubevm2   <none>           <none>
```



在上面的例子中，我们使用到了In操作符，NodeAffinity支持的操作符包括In、NotIn、Exists、DoesNotExist、Gt、Lt等。



### Affinity & AntiAffinity

在实际应用中，我们往往会遇到特殊的Pod调度需求：存在某些相互依赖、频繁调用的Pod，他们需要尽可能部署在同一个节点、网段、机柜或区域中，这就是Pod间亲和性，反之，出于避免竞争或容错需求，我们需要使某些Pod尽可能远离某些特定Pod时，这就是Pod间反亲和性。**简单的说，就是相关的两种或多种Pod是否可以在同一个拓扑域中共存或互斥。**

**拓扑域**：一个拓扑域由一些Node组成，这些Node通常有相同的地理空间坐标，如部署在同一个机架、机房或地区，我们一般用region表示机架或机房的拓扑区域，用Zone表示地区这种跨度大的拓扑区域。



Pod的亲和性与反亲和性设置是通过在Pod的定义上增加topologyKey属性来实现的，我们来看一下具体的例子：我们部署web应用极其对应的redis缓存服务，一般情况下我们希望redis多个实例尽可能分散部署，而web应用和redis缓存尽可能部署在一台机器上，但多个web应用副本应当分散部署，这该如何实现呢？

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:latest
```



上面是redis部署配置，包含三个副本以及 app=store 标签选择器。Deployment 中配置了 PodAntiAffinity，通过设置topologyKey为kbernetes.io/hostname确保调度器不会将三个副本调度到一个节点上。

```sh
$ kubectl create -f redis_deployment.yml
deployment.apps/redis-cache created
$ kubectl get pods -o wide
NAME                           READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
redis-cache-5d67cf4885-4vj4q   1/1     Running   0          15m   10.244.0.7    kubevm1   <none>           <none>
redis-cache-5d67cf4885-bn7dt   1/1     Running   0          15m   10.244.2.18   kubevm3   <none>           <none>
redis-cache-5d67cf4885-rx2b9   1/1     Running   0          15m   10.244.1.35   kubevm2   <none>           <none>
```



接下来是web应用的部署配置：通过配置 podAntiAffinity 以及 podAffinity。要求将其副本与 包含 app=store 标签的 Pod 放在同一个节点上；同时也要求 web-app 的副本不被调度到同一个节点上。

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-server
spec:
  selector:
    matchLabels:
      app: web-app
  replicas: 3
  template:
    metadata:
      labels:
        app: web-app
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - web-app
            topologyKey: "kubernetes.io/hostname"
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-app
        image: nginx:latest
```



我们来看一下部署结果：

```sh
$ kubectl get pods -o wide
NAME                           READY   STATUS             RESTARTS   AGE     IP            NODE      NOMINATED NODE   READINESS GATES
redis-cache-5d67cf4885-4vj4q   1/1     Running            0          20m     10.244.0.7    kubevm1   <none>           <none>
redis-cache-5d67cf4885-bn7dt   1/1     Running            0          20m     10.244.2.18   kubevm3   <none>           <none>
redis-cache-5d67cf4885-rx2b9   1/1     Running            0          20m     10.244.1.35   kubevm2   <none>           <none>
web-server-f98798775-87fdn     1/1     Running            0          3m33s   10.244.1.36   kubevm2   <none>           <none>
web-server-f98798775-wd7vk     1/1     Running            0          3m33s   10.244.0.8    kubevm1   <none>           <none>
web-server-f98798775-whlbr     1/1     Running            0          3m33s   10.244.2.19   kubevm3   <none>           <none>
```



### Taints & Tolerances

节点亲和性使得Pod能够被调度到某些Node上运行，**污点**则相反，它可以让Node拒绝Pod的运行，比如有些Node磁盘满了，需要清理，或者它的内存或CPU占用较高，这个时候我们不希望将新的Pod调度过去。当然，被标记为污点的Node并非故障不可用的节点，我们还可以通过设置**容忍度**将某些Pod调度过来。默认情况下，在Node上设置一个或多个污点标签后除非Pod明确声明能够容忍这些污点，否则无法调度到这些Node上运行，我们用一个例子演示一下。

```sh
$ kubectl taint nodes kubevm2 disk=full:NoSchedule
node/kubevm2 tainted
```



我们这里给kubevm2设置污点标签，此标签的键为disk，值为full，其效果是NoSchedule，这样除非Pod明确声明可以容忍磁盘满的Node，否则将不会调度到kubevm2上，我们还是以上面redis的部署为例，当不设置容忍度时的情况是如何的：

```sh
$ kubectl create -f redis_deployment.yml
$ kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
redis-cache-5d67cf4885-2xfdt   0/1     Pending   0          28s
redis-cache-5d67cf4885-j2zxh   1/1     Running   0          28s
redis-cache-5d67cf4885-mbk5g   1/1     Running   0          28s
```



由于redis的部署中设置了Pod反亲和性，即每一个副本必须部署在不同节点上，所以可以看到当kubevm2设置污点后，redis其中一个Pod一直处于Pending状态无法调度。ok，接下来我们声明一下容忍度试试：

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-cache
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  template:
    metadata:
      labels:
        app: store
    spec:
      tolerations:
      - key: "disk"
        operator: "Exists"
        effect: "NoSchedule"
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "kubernetes.io/hostname"
      containers:
      - name: redis-server
        image: redis:latest
```



我们创建此deployment看看效果：

```sh
$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP            NODE      NOMINATED NODE   READINESS GATES
redis-cache-d67c79fd7-r2mw9   1/1     Running   0          44s   10.244.2.21   kubevm3   <none>           <none>
redis-cache-d67c79fd7-t2vsh   1/1     Running   0          44s   10.244.0.10   kubevm1   <none>           <none>
redis-cache-d67c79fd7-xppql   1/1     Running   0          44s   10.244.1.37   kubevm2   <none>           <none>
```

我们可以看到，通过设置容忍度，Pod也可以调度到kubevm2上了。



这里要注意，在Pod的容忍度声明中，key和effect需要与污点的设置保持一致，并满足以下两个条件之一：

- operator的值时Exists，则无需指定value；
- operator的值是Equal则需要指定value并且和污点的value相同。



所以我们也可以这样设置容忍度：

```yml
tolerations:
- key: "disk"
  operator: "Equal"
  value: "full"
  effect: "NoSchedule"
```

除此之外还有两种特殊情况需要注意：

- 如果一个容忍度的 key 为空且 operator 为 Exists， 则能容忍所有污点；
- 如果 effect 为空，则能匹配所有effect。



### Priority Preemptive

在实际应用中，我们往往需要提高Kubernetes集群的资源利用率，即允许集群中所有负载所需的资源总量超过集群所提供的资源量，这个时候，当资源不足时，系统可以选择释放一些不重要的负载，保障最重要的负载能够运行，这就是我们今天要学习的**优先级抢占调度策略**。

要使用优先级抢占，需要做以下两件事：

- 定义PriorityClass；
- 在Pod配置中声明PriorityClassName并设置为上面所定义的PriorityClass。



1、定义PriorityClass

```yml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: priority-class-demo
value: 100000
globalDefault: false
description: "This is a demo priority class."
```



PriorityClass不属于任何命名空间，在上面这个例子中，定义了一个叫做priority-cass-demo的PrioriryClass，其优先级为100000，这个值越大，优先级越高，而超出10亿的值，其实是被Kubernetes保留下来分配给系统 Pod使用的。显然，这样做就是为了保证系统 Pod 不会被业务Pod抢占掉。设置globalDefault为false意味着，创建这个PriorityClass之后，所创建的Pod的默认优先级都是100000。我们创建此PriorityClass：

```sh
$ kubectl apply -f priority_class.yml
priorityclass.scheduling.k8s.io/priority-class-demo created
```



2、为Pod指定PriorityClass

我们修改nginx的deployment，在其中pod配置中增加PriorityClassName的设置：

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      priorityClassName: priority-class-demo
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```



当 Pod 拥有了优先级之后，高优先级的 Pod 就可能会比低优先级的 Pod 提前出队，从而尽早完成调度过程。但如果由于集群负载较大，一个高优先级Pod调度失败时，调度器的抢占能力就会被触发。此时调度器就会试图从当前集群里寻找一个Node，使得当这个Node上的一个或者多个低优先级 Pod 被删除后，待调度的高优先级 Pod 就可以被调度到这个节点上。当然在实际操作过程中，如果恰好此时又出现了一个优先级更高的Pod，调度器就会优先调度这个更高优先级的Pod，然后在重新调度之前等待的次高优先级Pod。

大家可以看出来，这个抢占能力其实对于服务性Pod影响不大，但对于执行批处理任务的Pod来说是一个灾难，这有可能导致Job没有执行完任务就被驱逐，从而导致部分任务没有被执行完成。为了避免这个问题，PriorityClass增加了一个属性**preemptionPolicy**，这个属性默认值为**PreemptLowerPriority**，即抢占资源，当我们设置为**Never**时，则不会抢占资源，而是默默排队，等待被调度。

以上就是优先级抢占调度的执行过程，其实我们不难看出，这个调度策略有可能会增加系统复杂性，以及部署在集群上的业务系统的稳定性，所以如果出现资源不足的情况，最好优先考虑扩容方案，实在不行在考虑优先级抢占调度。



## 就绪探针

**就绪探针有三种类型：**

1.Exec探针，执行进程的地方。容器的状态由进程的退出状态代码确定。

2.HTTP GET探针，向容器发送HTTP GET请求，通过响应http状态码判断容器是否准备好。

3.TCP socket探针，它打开一个TCP连接到容器的指定端口，如果连接建立，则认为容器已经准备就绪。



启动容器时，k8s设置了一个等待时间，等待时间后才会执行一次准备就绪检查。之后就会周期性的进行调用探针，并根据就绪探针的结果采取行动。

如果某个pod未就绪成功，则会从该服务中删除该pod，如果pod再次就绪成功，则从新添加pod。



**与存活探针区别：**

就绪探针如果容器未准备就绪，则不会终止或者重启启动。

存活探针通过杀死异常容器，并用新的正常的容器来替代他保证pod正常工作。

就绪探针只有准备好处理请求pod才会接收他的请求。



**重要性;**

确保客户端只与正常的pod进行交互，并且永远不会知道系统存在问题。



### 向pod添加就绪探针

```yaml
apiVersion: v1
kind: deployment
...
spec:
  ...
  port:
    containers:
    - name: kubia-yh
        imgress: luksa/kubia
        readinessProbe:
          failureThreshold: 2
          httpGet:
            path: /ping
            port: 80
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 3
```



相关参数解释如下：

- initialDelaySeconds：容器启动和探针启动之间的秒数。
- periodSeconds：检查的频率（以秒为单位）。默认为10秒。最小值为1。
- timeoutSeconds：检查超时的秒数。默认为1秒。最小值为1。
- successThreshold：失败后检查成功的最小连续成功次数。默认为1.活跃度必须为1。最小值为1。
- failureThreshold：当Pod成功启动且检查失败时，Kubernetes将在放弃之前尝试failureThreshold次。放弃生存检查意味着重新启动Pod。而放弃就绪检查，Pod将被标记为未就绪。默认为3.最小值为1。

HTTP探针在httpGet上的配置项：

- host：主机名，默认为pod的IP。
- scheme：用于连接主机的方案（HTTP或HTTPS）。默认为HTTP。
- path：探针的路径。
- httpHeaders：在HTTP请求中设置的自定义标头。 HTTP允许重复的请求头。
- port：端口的名称或编号。数字必须在1到65535的范围内



### **模拟就绪探针**

```
# kubectl   exec <pod_name> -- curl http://10.187.0.139:80/ping
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
```



## Create Pod

Kubernetes创建pod一般有两种方式：

1. 使用命令行的方式创建pod；
2. 使用yaml文件创建pod



### Create By Command

首先拉取需要的镜像，先在worker节点拉取nginx镜像

```shell
[root@k8scloude2 ~]# docker pull nginx

[root@k8scloude3 ~]# docker pull nginx
```



使用nginx镜像创建一个pod

```shell
#nginx为pod名字     --image=nginx表示使用Nginx镜像
[root@k8scloude1 pod]# kubectl run nginx --image=nginx --image-pull-policy=IfNotPresent --env "xx=1" --env "yy=2" --labels="xx=1,yy=2"
pod/nginx created
```



查看pod，STATUS为Running就表示pod创建成功

```shell
[root@k8scloude1 pod]# kubectl get pod
NAME    READY   STATUS              RESTARTS   AGE
nginx   0/1     ContainerCreating   0          6s

[root@k8scloude1 pod]# kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          54s
```



进入到pod里，可以看到变量xx,yy

```shell
[root@k8scloude1 pod]# kubectl exec -it nginx -- bash
root@nginx:/# echo $xx
1
root@nginx:/# echo $yy
2
root@nginx:/# exit
exit
```



查看pod的标签，--show-labels参数显示标签

```shell
[root@k8scloude1 pod]# kubectl get pod -o wide --show-labels
NAME    READY   STATUS    RESTARTS   AGE     IP               NODE         NOMINATED NODE   READINESS GATES   LABELS
nginx   1/1     Running   0          4m59s   10.244.112.136   k8scloude2   <none>           <none>            xx=1,yy=2
```



删除pod

```shell
[root@k8scloude1 pod]# kubectl delete pod nginx
pod "nginx" deleted
```



### Create By Yaml

生成创建pod的yaml文件

```shell
[root@k8scloude1 pod]# kubectl run nginx --image=nginx --image-pull-policy=IfNotPresent --dry-run=client -o yaml >nginx.yaml
```



kubectl apply -f 应用配置文件，创建pod

```shell
[root@k8scloude1 pod]# kubectl apply -f nginx.yaml 
pod/nginx created

[root@k8scloude1 pod]# kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          26s
```



删除pod

```shell
[root@k8scloude1 pod]# kubectl delete -f nginx.yaml 
pod "nginx" deleted

[root@k8scloude1 pod]# kubectl get pods
No resources found in pod namespace.
```



#### 写一个编排yaml格式

kubenetes里面的创建service、rc、pod都是这种形式(另外一种是json)。



```
[root@k8s-master ~]# cat  /etc/kubernetes/apiserver 
KUBE_API_ADDRESS="--insecure-bind-address=0.0.0.0"
KUBE_API_PORT="--port=8080"
KUBE_ETCD_SERVERS="--etcd-servers=http://10.0.0.11:2379"
KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"
KUBE_ADMISSION_CONTROL="--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ResourceQuota"
KUBE_API_ARGS=""
```



#### 启动一个pod

```
[root@k8s-master ~]# kubectl create -f hello.yaml 
pod "hello-world" created
```

查看默认namespace下的pods

```
[root@k8s-master ~]# kubectl get pods
NAME          READY     STATUS              RESTARTS   AGE
hello-world   0/1       ContainerCreating   0          8s
```

查看pod的详细信息

```
[root@k8s-master ~]# kubectl  describe pod  hello-world 
Events:
  FirstSeen    LastSeen    Count    From            SubObjectPath    Type       Reason        Message
  ---------    --------    -----    ----            -------------    --------   ------        -------
  4m        4m        1    {default-scheduler}      Normal         Scheduled    Successfully assigned hello-world to 10.0.0.13
  4m        1m        5    {kubelet 10.0.0.13}      Warning        FailedSync    Error syncing pod, skipping: failed to "StartContainer" for "POD" with ErrImagePull: "image pull failed for registry.access.redhat.com/rhel7/pod-infrastructure:latest, this may be because there are no credentials on this request.  details: (open /etc/docker/certs.d/registry.access.redhat.com/redhat-ca.crt: no such file or directory)"
  3m        14s       13   {kubelet 10.0.0.13}      Warning        FailedSync    Error syncing pod, skipping: failed to "StartContainer" for "POD" with ImagePullBackOff: "Back-off pulling image \"registry.access.redhat.com/rhel7/pod-infrastructure:latest\""
```

该错误的解决方法： yum install python-rhsm* -y

获取指定pods详细信息

```
kubectl describe pods yourpodname
```

获取已运行pod状态

```
kubectl get pods -o wide
```

下载pod-infrastructure镜像包

```
docker tag docker.io/tianyebj/pod-infrastructure:latest registry.access.redhat.com/rhel7/pod-infrastructure:lates
```



#### 使用同一个yaml文件创建多个pod

生成yaml文件

```shell
[root@k8scloude1 pod]# kubectl run pod1 --image=nginx --image-pull-policy=IfNotPresent --dry-run=client -o yaml >pod1.yaml
```

使用同一个yaml文件创建2个pod

```shell
[root@k8scloude1 pod]# sed 's/pod1/pod2/' pod1.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod2
  name: pod2
spec:
  containers:
  - image: nginx
    imagePullPolicy: IfNotPresent
    name: pod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

[root@k8scloude1 pod]# sed 's/pod1/pod2/' pod1.yaml | kubectl apply -f -
pod/pod2 created

[root@k8scloude1 pod]# sed 's/pod1/pod3/' pod1.yaml | kubectl apply -f -
pod/pod3 created

[root@k8scloude1 pod]# kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
pod2    1/1     Running   0          30s
pod3    1/1     Running   0          9s

[root@k8scloude1 pod]# ls
pod1.yaml
```