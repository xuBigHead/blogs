---
layout: post
title: RocketMQ 其他.md
categories: [RocketMQ]
description: 
keywords: RocketMQ 其他.md
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---
# 前言

[Rocket Github地址](https://github.com/apache/rocketmq/tree/master/docs/cn)



# 面试相关

RocketMQ是一个纯Java、分布式、队列模型的开源消息中间件，前身是MetaQ，是阿里参考Kafka特点研发的一个队列模型的消息中间件，后开源给apache基金会成为了apache的顶级开源项目，具有高性能、高可靠、高实时、分布式特点。



RocketMQ优点：

- 单机吞吐量：十万级
- 可用性：非常高，分布式架构
- 消息可靠性：经过参数优化配置，消息可以做到0丢失
- 功能支持：MQ功能较为完善，还是分布式的，扩展性好
- 支持10亿级别的消息堆积，不会因为堆积导致性能下降
- 源码是java，我们可以自己阅读源码，定制自己公司的MQ，可以掌控



RocketMQ缺点：

- 支持的客户端语言不多，目前是java及c++，其中c++不成熟
- 社区活跃度不是特别活跃那种
- 没有在 mq 核心中去实现**JMS**等接口，有些系统要迁移需要修改大量代码



**为什么选择RocketMQ？**

ActiveMQ较老；RabbitMQ是erlang开发的，不方便看源码和定制；Kafka更多用于大数据。

RocketMQ是Java开发的，经过了阿里的超高并发和高吞吐的考验。



**MQ设计思路**

可伸缩；数据持久化；可用性；数据不丢失。



## 消息队列

**优点**：**解耦**、**异步**、**削峰**。

**缺点**：**系统可用性降低**；**系统复杂度提高**；**一致性问题**。



## 组成角色

![图片](https://oss.xubighead.top/oss/image/202506/1930159361100976129.png)

| 角色       | 作用                                                         |
| ---------- | ------------------------------------------------------------ |
| Nameserver | 无状态，动态列表；这也是和zookeeper的重要区别之一。zookeeper是有状态的。<br />名字服务是一个非常简单的 Topic 路由注册中心，其角色类似 Dubbo 中的zookeeper，支持Broker的动态注册与发现。 |
| Producer   | 消息生产者，负责发消息到Broker。<br />消息发布的角色，Producer 通过 MQ 的负载均衡模块选择相应的 Broker 集群队列进行消息投递，投递的过程支持快速失败并且低延迟。 |
| Broker     | 就是MQ本身，负责收发消息、持久化消息等。<br />Broker主要负责消息的存储、投递和查询以及服务高可用保证。 |
| Consumer   | 消息消费者，负责从Broker上拉取消息进行消费，消费完进行ack。<br />消息消费的角色，支持以 push 推，pull 拉两种模式对消息进行消费。 |



### Nameserver

类似一个注册中心，底层由netty实现，提供了路由管理、服务注册、服务发现的功能，是一个无状态节点。

nameserver是服务发现者，集群中各个角色（producer、broker、consumer等）都需要定时向nameserver上报自己的状态，以便互相发现彼此，超时不上报的话，nameserver会把它从列表中剔除。

nameserver可以部署多个，当多个nameserver存在的时候，其他角色同时向他们上报信息，以保证高可用。NameServer集群间互不通信，没有主备的概念。nameserver内存式存储，nameserver中的broker、topic等信息默认不会持久化，所以他是无状态节点。



**NameServer**是一个功能齐全的服务器，其角色类似Dubbo中的Zookeeper，但NameServer与Zookeeper相比更轻量。主要是因为每个NameServer节点互相之间是独立的，没有任何信息交互。

**NameServer**压力不会太大，平时主要开销是在维持心跳和提供Topic-Broker的关系数据。

但有一点需要注意，Broker向NameServer发心跳时， 会带上当前自己所负责的所有**Topic**信息，如果**Topic**个数太多（万级别），会导致一次心跳中，就Topic的数据就几十M，网络情况差的话， 网络传输失败，心跳失败，导致NameServer误认为Broker心跳失败。

**NameServer** 被设计成几乎无状态的，可以横向扩展，节点之间相互之间无通信，通过部署多台机器来标记自己是一个伪集群。

每个 Broker 在启动的时候会到 NameServer 注册，Producer 在发送消息前会根据 Topic 到 **NameServer** 获取到 Broker 的路由信息，Consumer 也会定时获取 Topic 的路由信息。



NameServer 是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。Broker 启动之后会向所有 NameServer 定期（每 30s）发送**心跳包**(**路由信息**)，NameServer 会定期扫描 Broker 存活列表，如果超过 120s 没有心跳则移除此 Broker 相关信息，代表下线。



### Producer

消息的生产者，Producer随机选择其中一个NameServer节点建立长连接，获得Topic路由信息（包括topic下的queue，这些queue分布在哪些broker上等等）。接下来向提供topic服务的master建立长连接（因为rocketmq只有master才能写消息），且定时向master发送心跳。

**Producer**由用户进行分布式部署，消息由**Producer**通过多种负载均衡模式发送到**Broker**集群，发送低延时，支持快速失败。



1、获得 Topic-Broker 的映射关系。

Producer 启动时，也需要指定 Namesrv 的地址，从 Namesrv 集群中选一台建立长连接。

生产者每 30 秒从 Namesrv 获取 Topic 跟 Broker 的映射关系，更新到本地内存中。然后再跟 Topic 涉及的所有 Broker 建立长连接，每隔 30 秒发一次心跳。

2、生产者端的负载均衡。

生产者发送时，会自动轮询当前所有可发送的broker，一条消息发送成功，下次换另外一个broker发送，以达到消息平均落到所有的broker上。



#### 消息发送方式

**RocketMQ** 提供了三种方式发送消息：同步、异步和单向。



##### 同步发送

同步发送指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。一般用于重要通知消息，例如重要通知邮件、营销短信。



##### 异步发送

异步发送指发送方发出数据后，不等接收方发回响应，接着发送下个数据包，一般用于可能链路耗时较长而对响应时间敏感的业务场景，例如用户视频上传后通知启动转码服务。



##### 单向发送

单向发送是指只负责发送消息而不等待服务器回应且没有回调函数触发，适用于某些耗时非常短但对可靠性要求并不高的场景，例如日志收集。



#### Queue选择算法

分为两种，一种是直接发消息，client内部有选择queue的算法，不允许外界改变。还有一种是可以自定义queue的选择算法，内置了三种算法，不喜欢的话可以自定义算法实现。

有时候我们不希望默认的queue选择算法，而是需要自定义，一般最常用的场景在顺序消息，顺序消息的发送一般都会指定某组特征的消息都发当同一个queue里，这样才能保证顺序，因为单queue是有序的。



内置了三种算法，三种算法都实现了一个共同的接口MessageQueueSelector：

- `SelectMessageQueueByRandom`
- `SelectMessageQueueByHash`
- `SelectMessageQueueByMachineRoom`
- 要想自定义逻辑的话，直接实现接口重写select方法即可。



###### SelectMessageQueueByRandom

> org.apache.rocketmq.client.producer.selector.SelectMessageQueueByRandom

```java
public class SelectMessageQueueByRandom implements MessageQueueSelector {
    private Random random = new Random(System.currentTimeMillis());

    @Override
    public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
        // mqs.size()：队列的个数。假设队列个数是4，那么这个value就是0-3之间随机。
        int value = random.nextInt(mqs.size());
        return mqs.get(value);
    }
}
```



###### SelectMessageQueueByHash

> org.apache.rocketmq.client.producer.selector.SelectMessageQueueByHash

```java
public class SelectMessageQueueByHash implements MessageQueueSelector {

    @Override
    public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
        int value = arg.hashCode();
        if (value < 0) {
            // 防止出现负数，取个绝对值，这也是我们平时开发中需要注意到的点
            value = Math.abs(value);
        }
        // 直接取余队列个数。
        value = value % mqs.size();
        return mqs.get(value);
    }
}
```



###### SelectMessageQueueByMachineRoom

> org.apache.rocketmq.client.producer.selector.SelectMessageQueueByMachineRoom

```java
public class SelectMessageQueueByMachineRoom implements MessageQueueSelector {
    private Set<String> consumeridcs;

    @Override
    public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
        return null;
    }

    public Set<String> getConsumeridcs() {
        return consumeridcs;
    }

    public void setConsumeridcs(Set<String> consumeridcs) {
        this.consumeridcs = consumeridcs;
    }
}
```



#### 扩展

##### Topic和Queue的区别

queue就是来源于数据结构的FIFO队列。而Topic是个抽象的概念，每个Topic底层对应N个queue，而数据也真实存在queue上的。



### Broker

broker主要用于producer和consumer接收和发送消息，是消息中间件的消息存储、转发服务器。每个Broker节点，在启动时，都会遍历NameServer列表，与每个NameServer建立长连接，注册自己的信息，之后定时上报提交自己的信息。

- **Broker**是具体提供业务的服务器，单个Broker节点与所有的NameServer节点保持长连接及心跳，并会定时将**Topic**信息注册到NameServer，顺带一提底层的通信和连接都是**基于Netty实现**的。
- **Broker**负责消息存储，以Topic为纬度支持轻量级的队列，单机可以支撑上万队列规模，支持消息推拉模型。
- 官网上有数据显示：具有**上亿级消息堆积能力**，同时可**严格保证消息的有序性**。



#### 接收消息

##### Queue的分布

一个topic的queue可以分布到多个Broker上。比如一个topic有4个queue，他可能分配到broker-a上三个queue，broker-b上1个queue，这个queue的分配是由broker端决定的。

**每个queue的消息都是不一样的，也就是比如你发N条消息，他可能一部分在broker-a上一部分在broker-b上，不管他在哪，消息都是不一样的，不要理解成M-S那种复制。他只是负载均衡将queue分配到了不同的broker上。**



#### 持久化消息

消息持久化的地方其实是磁盘上，在如下目录里的commitlog文件夹里。文件大小是1.0G，超过1.0G再写入消息的话会自动创建新的commitlog文件。

```java
// 数据存储根目录
private String storePathRootDir = System.getProperty("user.home") + File.separator + "store";
// commitlog目录
private String storePathCommitLog = System.getProperty("user.home") + File.separator + "store" + File.separator + "commitlog";
// 每个commitlog文件大小为1GB，超过1GB则创建新的commitlog文件
private int mappedFileSizeCommitLog = 1024 * 1024 * 1024;
```



先将消息写入到ByteBuffer缓冲区，然后默认是每隔500毫秒刷一次盘。有两种刷盘方式，同步和异步，一般选择异步，同步效率低，但是更可靠。



|         类名          |                 描述                 | 刷盘性能 |
| :-------------------: | :----------------------------------: | :------: |
| CommitRealTimeService |      异步刷盘 &&开启字节缓冲区       |   最高   |
| FlushRealTimeService  |     异步刷盘&&关闭内存字节缓冲区     |   较高   |
|  GroupCommitService   | 同步刷盘，刷完盘才会返回消息写入成功 |   最低   |



Broker的**Buffer**通常指的是Broker中一个队列的内存Buffer大小，这类**Buffer**通常大小有限。

另外，RocketMQ没有内存**Buffer**概念，RocketMQ的队列都是持久化磁盘，数据定期清除。

RocketMQ同其他MQ有非常显著的区别，RocketMQ的内存**Buffer**抽象成一个无限长度的队列，不管有多少数据进来都能装得下，这个无限是有前提的，Broker会定期删除过期的数据。

例如Broker只保存3天的消息，那么这个**Buffer**虽然长度无限，但是3天前的数据会被从队尾删除。



##### 同步刷盘



##### 异步刷盘



#### 扩展

##### 消息被消费后会立即删除吗？

不会，每条消息都会持久化到CommitLog中，每个Consumer连接到Broker后会维持消费进度信息，当有消息消费后只是当前Consumer的消费进度（CommitLog的offset）更新了。



##### 那么消息会堆积吗？什么时候清理过期消息？

4.6版本默认48小时后会删除不再使用的CommitLog文件，避免消息堆积。

- 检查这个文件最后访问时间
- 判断是否大于过期时间
- 指定时间删除，默认凌晨4点



##### 任何一台Broker突然宕机了怎么办？

Broker主从架构以及多副本策略。Master收到消息后会同步给Slave，这样一条消息就不止一份了，Master宕机了还有slave中的消息可用，保证了MQ的可靠性和高可用性。

而且Rocket MQ4.5.0开始就支持了Dlegder模式，基于raft的，做到了真正意义的HA。



##### RocketMQ 是如何保证数据的高容错性的?

- 在不开启容错的情况下，轮询队列进行发送，如果失败了，重试的时候过滤失败的Broker
- 如果开启了容错策略，会通过RocketMQ的预测机制来预测一个Broker是否可用
- 如果上次失败的Broker可用那么还是会选择该Broker的队列
- 如果上述情况失败，则随机选择一个进行发送
- 在发送消息的时候会记录一下调用的时间与是否报错，根据该时间去预测broker的可用时间



### Consumer

消息的消费者，通过NameServer集群获得Topic的路由信息，连接到对应的Broker上消费消息。由于Master和Slave都可以读取消息，因此Consumer会与Master和Slave都建立连接进行消费消息。

- **Consumer**也由用户部署，支持PUSH和PULL两种消费模式，支持**集群消费**和**广播消息**，提供**实时的消息订阅机制**。
- **Pull**：拉取型消费者（Pull Consumer）主动从消息服务器拉取信息，只要批量拉取到消息，用户应用就会启动消费过程，所以 Pull 称为主动消费型。
- **Push**：推送型消费者（Push Consumer）封装了消息的拉取、消费进度和其他的内部维护工作，将消息到达时执行的回调接口留给用户应用程序来实现。所以 Push 称为被动消费类型，但从实现上看还是从消息服务器中拉取消息，不同于 Pull 的是 Push 首先要注册消费监听器，当监听器处触发后才开始消费消息。



1、获得 Topic-Broker 的映射关系。

Consumer 启动时需要指定 Namesrv 地址，与其中一个 Namesrv 建立长连接。消费者每隔 30 秒从 Namesrv 获取所有Topic 的最新队列情况，

Consumer 跟 Broker 是长连接，会每隔 30 秒发心跳信息到Broker .

2、消费者端的负载均衡。根据消费者的消费模式不同，负载均衡方式也不同。



#### 消费原理

- Consumer端发心跳给Broker，Broker收到后存到consumerTable里（就是个Map），key是GroupName，value是ConsumerGroupInfo。
- ConsumerGroupInfo里面是包含topic等信息的，但是问题就出在上一步骤，key是groupName，你同GroupName的话Broker心跳最后收到的Consumer会覆盖前者的。



这样同key，肯定产生了覆盖。所以Consumer1不会收到任何消息，但是Consumer2为什么只收到了一半（不固定）消息呢？

那是因为：你是集群模式消费，它会负载均衡分配到各个节点去消费，所以一半消息（不固定个数）跑到了Consumer1上，结果Consumer1订阅的是tag1，所以不会任何输出。

**如果换成BROADCASTING，那绝逼后者会收到全部消息，而不是一半，因为广播是广播全部Consumer。**



#### 消费模式

##### 集群消费

1.一条消息只会被同Group中的一个Consumer消费一次

2.多个Group同时消费一个Topic时，每个Group都会有一个Consumer消费到数据

3.在消息重投时，不能保证路由到同一台机器上

4.消费状态由broker维护



##### 广播消费

消息将对一个Consumer Group 下的各个 Consumer 实例都消费一遍。即使这些 Consumer 属于同一个Consumer Group ，消息也会被 Consumer Group 中的每个 Consumer 都消费一次。



- 消费进度由consumer维护

- 保证每个消费者都消费一次消息

- 消费失败的消息不会重投

	

#### 消费方式

RocketMQ没有真正意义的push，都是pull，虽然有push类，但实际底层实现采用的是**长轮询机制**，即拉取方式。



> broker端属性 longPollingEnable 标记是否开启长轮询，默认开启。



##### 为什么要主动拉取消息而不使用事件监听方式？

事件驱动方式是建立好长连接，由事件（发送数据）的方式来实时推送。

如果broker主动推送消息的话有可能push速度快，消费速度慢的情况，那么就会造成消息在consumer端堆积过多，同时又不能被其他consumer消费的情况。而pull的方式可以根据当前自身情况来pull，不会造成过多的压力而造成瓶颈。所以采取了pull的方式。



##### 实现原理

Consumer首次请求Broker

- Broker中是否有符合条件的消息
- 有 ->
- - 响应Consumer
	- 等待下次Consumer的请求
- 没有
- - DefaultMessageStore#ReputMessageService#run方法
	- PullRequestHoldService 来Hold连接，每个5s执行一次检查pullRequestTable有没有消息，有的话立即推送
	- 每隔1ms检查commitLog中是否有新消息，有的话写入到pullRequestTable
	- 当有新消息的时候返回请求
	- 挂起consumer的请求，即不断开连接，也不返回数据
	- 使用consumer的offset
	



### 总结

核心流程如下：

- Broker都注册到Nameserver上
- Producer发消息的时候会从Nameserver上获取发消息的topic信息
- Producer向提供服务的所有master建立长连接，且定时向master发送心跳
- Consumer通过NameServer集群获得Topic的路由信息
- Consumer会与所有的Master和所有的Slave都建立连接进行监听新消息



## 工作流程

Producer 与 NameServer集群中的其中一个节点（随机选择）建立长连接，定期从 NameServer 获取 **Topic** 路由信息，并向提供 Topic 服务的 **Broker Master** 建立长连接，且定时向 **Broker** 发送心跳。

**Producer** 只能将消息发送到 Broker master，但是 **Consumer** 则不一样，它同时和提供 Topic 服务的 Master 和 Slave建立长连接，既可以从 Broker Master 订阅消息，也可以从 Broker Slave 订阅消息。



**执行流程**：

- 启动 Namesrv，Namesrv起 来后监听端口，等待 Broker、Producer、Consumer 连上来，相当于一个路由控制中心。
- Broker 启动，跟所有的 Namesrv 保持长连接，定时发送心跳包。
- 收发消息前，先创建 Topic 。创建 Topic 时，需要指定该 Topic 要存储在 哪些 Broker上。也可以在发送消息时自动创建Topic。
- Producer 发送消息。
- Consumer 消费消息。



### NameServer

NameServer启动流程如下：

- 第一步是初始化配置
- 创建**NamesrvController**实例，并开启两个定时任务：每隔10s扫描一次**Broker**，移除处于不激活的**Broker**；每隔10s打印一次KV配置。
- 第三步注册钩子函数，启动服务器并监听Broker。



NameServer 会有每 10s 一次的定时任务检查 Broker 是否下线了，如果 120s 内有没有收到 Broker 心跳，则关闭 channel，把 Broker 信息从本地缓存移除。消费者则默认每隔 30s 向 NameServer 拉取路由信息来刷新本地缓存的 Broker 列表。也就是说可能会有最多 150s 的时间消费者拉取消息失败。



### Producer

![img](https://oss.xubighead.top/oss/image/202506/1930159643256000514.png)



### Broker

**Broker**在RocketMQ中是进行处理Producer发送消息请求，Consumer消费消息的请求，并且进行消息的持久化，以及HA策略和服务端过滤，就是集群中很重的工作都是交给了**Broker**进行处理。

**Broker**模块是通过BrokerStartup进行启动的，会实例化BrokerController，并且调用其初始化方法。

Broker初始化，会根据配置创建很多线程，主要用来**发送消息**、**拉取消息**、**查询消息**、**客户端管理**和**消费者管理**，也有很多**定时任务**，同时也注册了很多**请求处理器**，用来发送拉取消息查询消息的。



### Consumer

![img](https://oss.xubighead.top/oss/image/202506/1930159468097671169.png)



消费端会通过**RebalanceService**线程，10秒钟做一次基于**Topic**下的所有队列负载。



### 扩展

##### ACK

**ACK机制是发生在Consumer端的，不是在Producer端的**。也就是说Consumer消费完消息后要进行ACK确认，如果未确认则代表是消费失败，这时候Broker会进行重试策略（仅集群模式会重试）。ACK的意思就是：Consumer说：ok，我消费成功了。这条消息给我标记成已消费吧。



## 负载均衡

RocketMQ通过Topic在多Broker中分布式存储实现负载均衡。



### producer端

**Producer**通过轮训某个**Topic**下面的所有队列实现发送方的负载均衡。

发送端指定message queue发送消息到相应的broker，来达到写入时的负载均衡。

- 提升写入吞吐量，当多个producer同时向一个broker写入数据的时候，性能会下降
- 消息分布在多broker中，为负载消费做准备



**默认策略是随机选择：**

- producer维护一个index
- 每次取节点会自增
- index向所有broker个数取余
- 自带容错策略



**其他实现：**

- SelectMessageQueueByHash
- SelectMessageQueueByRandom
- SelectMessageQueueByMachineRoom 没有实现

也可以自定义实现**MessageQueueSelector**接口中的select方法



### consumer端

采用的是平均分配算法来进行负载均衡。



**其他负载均衡算法**

- 平均分配策略(默认)(AllocateMessageQueueAveragely) 

- 环形分配策略(AllocateMessageQueueAveragelyByCircle)
-  手动配置分配策略(AllocateMessageQueueByConfig) 
- 机房分配策略(AllocateMessageQueueByMachineRoom) 
- 一致性哈希分配策略(AllocateMessageQueueConsistentHash) 
- 靠近机房策略(AllocateMachineRoomNearby)



#### 扩展

##### 当消费负载均衡consumer和queue不对等的时候会发生什么？

- queue个数大于Consumer个数，且queue个数能整除Consumer个数的话， 那么Consumer会平均分配queue。
- queue个数大于Consumer个数，且queue个数不能整除Consumer个数的话， 那么会有一个Consumer多消费1个queue，其余Consumer平均分配。
- queue个数小于Consumer个数，那么会有Consumer闲置，就是浪费掉了，其余Consumer平均分配到queue上。



##### Consumer掉线或上线

当一个consumer出现宕机后，默认最多20s，其它机器将重新消费已宕机的机器消费的queue，同样当有新的Consumer连接上后，20s内也会完成rebalance使得新的Consumer有机会消费queue里的msg。

等等，好像有问题：新上线一个Consumer要等20s才能负载均衡？这不是搞笑呢吗？肯定有猫腻。

确实，新启动Consumer的话会立即唤醒沉睡的线程， 让他立马进行this.mqClientFactory.doRebalance()。








## 实际场景

### 重复消费

影响消息正常发送和消费的**重要原因是网络的不确定性。**



**引起重复消费的原因**

- ACK

正常情况下在consumer真正消费完消息后应该发送ack，通知broker该消息已正常消费，从queue中剔除

当ack因为网络原因无法发送到broker，broker会认为词条消息没有被消费，此后会开启消息重投机制把消息再次投递到consumer。



- 消费模式

在CLUSTERING模式下，消息在broker中会保证相同group的consumer消费一次，但是针对不同group的consumer会推送多次。



**解决方案**

- 数据库表

处理消息前，使用消息主键在表中带有约束的字段中insert

- Map

单机时可以使用map *ConcurrentHashMap* -> putIfAbsent  guava cache

- Redis

分布式锁搞起来。



### 消息去重

使用业务端逻辑保持幂等性，保证每条消息都有唯一编号。



### 顺序消费

首先多个queue只能保证单个queue里的顺序，queue是典型的FIFO，天然顺序。多个queue同时消费是无法绝对保证消息的有序性的。所以总结如下：

同一topic，同一个QUEUE，发消息的时候一个线程去发送消息，消费的时候 一个线程去消费一个queue里的消息。

但问题是1个topic有N个queue，作者这么设计的好处也很明显，天然支持集群和负载均衡的特性，将海量数据均匀分配到各个queue上，你发了10条消息到同一个topic上，这10条消息会自动分散在topic下的所有queue中，所以消费的时候不一定是先消费哪个queue，后消费哪个queue，这就导致了无序消费。

一个Producer发送了m1、m2、m3、m4四条消息到topic上，topic有四个队列，由于自带的负载均衡策略，四个队列上分别存储了一条消息。queue1上存储的m1，queue2上存储的m2，queue3上存储的m3，queue4上存储的m4，Consumer消费的时候是多线程消费，所以他无法保证先消费哪个队列或者哪个消息，比如发送的时候顺序是m1，m2，m3，m4，但是消费的时候由于Consumer内部是多线程消费的，所以可能先消费了queue4队列上的m4，然后才是m1，这就导致了无序。



#### 解决方案

##### 消息发到同一个queue

Rocket MQ提供了MessageQueueSelector接口，可以重写里面的接口，实现自己的算法，举个最简单的例子：判断`i % 2 == 0`，那就都放到queue1里，否则放到queue2里。



##### 单线程消费

比如你新需求：把未支付的订单都放到queue1里，已支付的订单都放到queue2里，支付异常的订单都放到queue3里，然后你消费的时候要保证每个queue是有序的，不能消费queue1一条直接跑到queue2去了，要逐个queue去消费。

这时候思路是发消息的时候利用自定义参数arg，消息体里肯定包含支付状态，判断是未支付的则选择queue1，以此类推。这样就保证了每个queue里只包含同等状态的消息。那么消费者目前是多线程消费的，肯定乱序。三个queue随机消费。解决方案更简单，直接将**消费端的线程数改为1个**，这样队列是FIFO，他就逐个消费了。

这种方式的缺点在于并行度就会成为消息系统的瓶颈。



### 消息丢失

![图片](https://oss.xubighead.top/oss/image/202506/1930159531481993217.png)



消息流程分为如下三大部分，每一部分都有可能会丢失数据。

- 生产阶段：Producer通过网络将消息发送给Broker，这个发送可能会发生丢失，比如网络延迟不可达等。
- 存储阶段：Broker肯定是先把消息放到内存的，然后根据刷盘策略持久化到硬盘中，刚收到Producer的消息，再内存中了，但是异常宕机了，导致消息丢失。
- 消费阶段：消费失败了其实也是消息丢失的一种变体吧。



#### Producer

##### 同步发送消息

采取send()同步发消息，发送结果是同步感知的。

> org.apache.rocketmq.client.producer.DefaultMQProducer

```java
// 同步发送
public SendResult send(Message msg) throws MQClientException, RemotingException,      MQBrokerException, InterruptedException {}

// 异步发送，sendCallback作为回调
public void send(Message msg,SendCallback sendCallback) throws MQClientException, RemotingException, InterruptedException {}

// 单向发送，不关心发送结果，最不靠谱
public void sendOneway(Message msg) throws MQClientException, RemotingException, InterruptedException {}
```



`send` 方法是一个同步操作，只要这个方法不抛出任何异常，就代表消息已经**发送成功**。

消息发送成功仅代表消息已经到了 Broker 端，Broker 在不同配置下，可能会返回不同响应状态:

判断返回状态是否是 `SendStatus.SEND_OK`。若是其他状态，就需要考虑补偿重试。



###### 消息发送状态

- `SendStatus.SEND_OK`

消息发送成功。要注意的是消息发送成功也不意味着它是可靠的。要确保不会丢失任何消息，还应启用同步Master服务器或同步刷盘，即SYNC_MASTER或SYNC_FLUSH。



- `SendStatus.FLUSH_DISK_TIMEOUT`

消息发送成功但是服务器刷盘超时。此时消息已经进入服务器队列（内存），只有服务器宕机，消息才会丢失。消息存储配置参数中可以设置刷盘方式和同步刷盘时间长度，如果Broker服务器设置了刷盘方式为同步刷盘，即FlushDiskType=SYNC_FLUSH（默认为异步刷盘方式），当Broker服务器未在同步刷盘时间内（默认为5s）完成刷盘，则将返回该状态——刷盘超时。



- `SendStatus.FLUSH_SLAVE_TIMEOUT`

消息发送成功，但是服务器同步到Slave时超时。此时消息已经进入服务器队列，只有服务器宕机，消息才会丢失。如果Broker服务器的角色是同步Master，即SYNC_MASTER（默认是异步Master即ASYNC_MASTER），并且从Broker服务器未在同步刷盘时间（默认为5秒）内完成与主服务器的同步，则将返回该状态——数据同步到Slave服务器超时。



- `SendStatus.SLAVE_NOT_AVAILABLE`

消息发送成功，但是此时Slave不可用。如果Broker服务器的角色是同步Master，即SYNC_MASTER（默认是异步Master服务器即ASYNC_MASTER），但没有配置slave Broker服务器，则将返回该状态——无Slave服务器可用。



##### 自动重试机制

发送失败后可以重试，设置重试次数，默认3次。可以根据api进行更改，比如改为10次。

```java
producer.setRetryTimesWhenSendFailed(10);
```



> org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl

```java
// 自动重试次数，this.defaultMQProducer.getRetryTimesWhenSendFailed()默认为2，如果是同步发送，默认重试3次，否则重试1次
int timesTotal = communicationMode == CommunicationMode.SYNC ? 1 + this.defaultMQProducer.getRetryTimesWhenSendFailed() : 1;
int times = 0;
for (; times < timesTotal; times++) {
      // 选择发送的消息queue
    MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName);
    if (mqSelected != null) {
        try {
            // 真正的发送逻辑，sendKernelImpl。
            sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime);
            switch (communicationMode) {
                case ASYNC:
                    return null;
                case ONEWAY:
                    return null;
                case SYNC:
                    // 如果发送失败了，则continue，意味着还会再次进入for，继续重试发送
                    if (sendResult.getSendStatus() != SendStatus.SEND_OK) {
                        if (this.defaultMQProducer.isRetryAnotherBrokerWhenNotStoreOK()) {
                            continue;
                        }
                    }
                    // 发送成功的话，将发送结果返回给调用者
                    return sendResult;
                default:
                    break;
            }
        } catch (RemotingException e) {
            continue;
        } catch (...) {
            continue;
        }
    }
}
```



重试流程如下：

- 重试次数同步是1 + `this.defaultMQProducer.getRetryTimesWhenSendFailed()`，其他方式默认1次。
- this.defaultMQProducer.getRetryTimesWhenSendFailed()默认是2，我们可以手动设置`producer.setRetryTimesWhenSendFailed(10);`
- 调用sendKernelImpl真正的去发送消息
- 如果是sync同步发送，且发送失败了，则continue，意味着还会再次进入for，继续重试发送
- 发送成功的话，将发送结果返回给调用者
- 如果发送异常进入catch了，则continue继续下次重试。



##### 多个Master节点

集群部署，比如发送失败了的原因可能是当前Broker宕机了，重试的时候会发送到其他Broker上。

假设Broker宕机了，但是生产环境一般都是多M多S的，所以还会有其他master节点继续提供服务，这也不会影响到我们发送消息，我们消息依然可达。因为比如恰巧发送到broker的时候，broker宕机了，producer收到broker的响应发送失败了，这时候producer会自动重试，这时候宕机的broker就被踢下线了， 所以producer会换一台broker发送消息。



##### 总结

失败会自动重试，即使重试N次也不行后，那客户端也会知道消息没成功，这也可以自己补偿等，不会盲目影响到主业务逻辑。再比如即使Broker挂了，那还有其他Broker再提供服务了，高可用，不影响。

总结为几个字就是：**同步发送+自动重试机制+多个Master节点。**



#### Broker

Broker是先把消息放到内存的，然后根据刷盘策略持久化到硬盘中，刚收到Producer的消息，再内存中了，但是异常宕机了，导致消息丢失。

默认情况下，消息只要到了 Broker 端，将会优先保存到内存中，然后立刻返回确认响应给生产者。随后 Broker 定期批量的将一组消息从内存异步刷入磁盘。

这种方式减少 I/O 次数，可以取得更好的性能，但是如果发生机器掉电，异常宕机等情况，消息还未及时刷入磁盘，就会出现丢失消息的情况。

若想保证 Broker 端不丢消息，保证消息的可靠性，我们需要将消息保存机制修改为同步刷盘方式，即消息**存储磁盘成功**，才会返回响应。



##### 同步刷盘

MQ持久化消息分为两种：同步刷盘和异步刷盘。默认情况是异步刷盘，Broker收到消息后会先存到cache里然后立马通知Producer说消息我收到且存储成功了，你可以继续你的业务逻辑了，然后Broker起个线程异步的去持久化到磁盘中，但是Broker还没持久化到磁盘就宕机的话，消息就丢失了。同步刷盘的话是收到消息存到cache后并不会通知Producer说消息已经ok了，而是会等到持久化到磁盘中后才会通知Producer说消息完事了。这也保障了消息不会丢失，但是性能不如异步高。看业务场景取舍。

修改刷盘策略为同步刷盘。默认情况下是异步刷盘的。

```bash
# 默认情况为 ASYNC_FLUSH，修改为同步刷盘：SYNC_FLUSH，实际场景看业务，同步刷盘效率肯定不如异步刷盘高。
flushDiskType = SYNC_FLUSH
```

若 Broker 未在同步刷盘时间内（**默认为 5s**）完成刷盘，将会返回 `SendStatus.FLUSH_DISK_TIMEOUT` 状态给生产者。



对应的Java配置类如下：

```java
package org.apache.rocketmq.store.config;

public enum FlushDiskType {
    // 同步刷盘
    SYNC_FLUSH,
    // 异步刷盘（默认）
    ASYNC_FLUSH
}
```



异步刷盘默认10s执行一次，源码如下：

```java
/*
 * {@link org.apache.rocketmq.store.CommitLog#run()}
 */

while (!this.isStopped()) {
    try {
        // 等待10s
        this.waitForRunning(10);
        // 刷盘
        this.doCommit();
    } catch (Exception e) {
        CommitLog.log.warn(this.getServiceName() + " service has exception. ", e);
    }
}
```



##### 集群部署

集群部署，主从模式，高可用。

即使Broker设置了同步刷盘策略，但是Broker刷完盘后磁盘坏了，这会导致盘上的消息全TM丢了。但是如果即使是1主1从了，但是Master刷完盘后还没来得及同步给Slave就磁盘坏了，不也是GG吗？没错！

所以我们还可以配置不仅是等Master刷完盘就通知Producer，而是等Master和Slave都刷完盘后才去通知Producer说消息ok了。

为了保证可用性，Broker 通常采用一主（**master**）多从（**slave**）部署方式。为了保证消息不丢失，消息还需要复制到 slave 节点。

默认方式下，消息写入 **master** 成功，就可以返回确认响应给生产者，接着消息将会异步复制到 **slave** 节点。

```bash
# 默认为 ASYNC_MASTER
brokerRole=SYNC_MASTER
```

此时若 master 突然**宕机且不可恢复**，那么还未复制到 **slave** 的消息将会丢失。

为了进一步提高消息的可靠性，我们可以采用同步的复制方式，**master** 节点将会同步等待 **slave** 节点复制完成，才会返回确认响应。

如果 **slave** 节点未在指定时间内同步返回响应，生产者将会收到 `SendStatus.FLUSH_SLAVE_TIMEOUT` 返回状态。



##### 总结

若想很严格的保证Broker存储消息阶段消息不丢失，则需要如下配置，但是性能肯定远差于默认配置。

```bash
# master 节点配置
flushDiskType = SYNC_FLUSH
brokerRole=SYNC_MASTER

# slave 节点配置
brokerRole=slave
flushDiskType = SYNC_FLUSH
```



上面这个配置含义是：

Producer发消息到Broker后，Broker的Master节点先持久化到磁盘中，然后同步数据给Slave节点，Slave节点同步完且落盘完成后才会返回给Producer说消息ok了。



#### Consumer

消费失败了其实也是消息丢失的一种变体。

消费者从 broker 拉取消息，然后执行相应的业务逻辑。一旦执行成功，将会返回 `ConsumeConcurrentlyStatus.CONSUME_SUCCESS` 状态给 Broker。

如果 Broker 未收到消费确认响应或收到其他状态，消费者下次还会再次拉取到该条消息，进行重试。这样的方式有效避免了消费者消费过程发生异常，或者消息在网络传输中丢失的情况。



##### ACK

消费者会先把消息拉取到本地，然后进行业务逻辑，业务逻辑完成后手动进行ack确认，这时候才会真正的代表消费完成。而不是说pull到本地后消息就算消费完了。

```java
 consumer.registerMessageListener(new MessageListenerConcurrently() {
     @Override
     public ConsumeConcurrentlyStatus consumeMessage(List<MessageExt> msgs, ConsumeConcurrentlyContext consumeConcurrentlyContext) {
         for (MessageExt msg : msgs) {
             String str = new String(msg.getBody());
             System.out.println(str);
         }
         // ack，只有等上面一系列逻辑都处理完后，到这步CONSUME_SUCCESS才会通知broker说消息消费完成，
         // 如果上面发生异常没有走到这步ack，则消息还是未消费状态。
         // 而不是像比如redis的blpop，弹出一个数据后数据就从redis里消失了，
         // 并没有等我们业务逻辑执行完才弹出。
         return ConsumeConcurrentlyStatus.CONSUME_SUCCESS;
     }
 });
```



以上消费消息过程的，我们需要**注意返回消息状态**。只有当业务逻辑真正执行成功，我们才能返回 `ConsumeConcurrentlyStatus.CONSUME_SUCCESS`。否则我们需要返回 `ConsumeConcurrentlyStatus.RECONSUME_LATER`，稍后再重试。



##### 消息重试

消息消费失败自动重试。如果消费消息失败了，没有进行ack确认，则会自动重试，重试策略和次数（默认15次）如下配置。

```java
/**
 * Broker可以配置的所有选项
 */
public class org.apache.rocketmq.store.config.MessageStoreConfig {
    private String messageDelayLevel = "1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h";
}
```



### 消息堆积

首先要找到是什么原因导致的消息堆积，是Producer太多了，Consumer太少了导致的还是说其他情况，总之先定位问题。

然后看下消息消费速度是否正常，正常的话，可以通过上线更多consumer临时解决消息堆积问题



1、如果可以添加消费者解决，就添加消费者的数据量
		2、如果出现了queue，但是消费者多的情况。可以使用准备一个临时的topic，同时创建一些queue，在临时创建一个消费者来把这些消息转移到topic中，让消费者消费。



#### 如果Consumer和Queue不对等，上线了多台也在短时间内无法消费完堆积的消息怎么办？

- 准备一个临时的topic
- queue的数量是堆积的几倍
- queue分布到多Broker中
- 上线一台Consumer做消息的搬运工，把原来Topic中的消息挪到新的Topic里，不做业务逻辑处理，只是挪过去
- 上线N台Consumer同时消费临时Topic中的数据
- 改bug
- 恢复原来的Consumer，继续消费之前的Topic



#### 堆积时间过长消息超时了？

RocketMQ中的消息只会在commitLog被删除的时候才会消失，不会超时。也就是说未被消费的消息不会存在超时删除这情况。



#### 堆积的消息会不会进死信队列？

不会，消息在消费失败后会进入重试队列（%RETRY%+ConsumerGroup），重试16次才会进入死信队（%DLQ%+ConsumerGroup）。



```java
public class MessageStoreConfig {
    // 每隔如下时间会进行重试，到最后一次时间重试失败的话就进入死信队列了，1s和5s被跳过。
 private String messageDelayLevel = "1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h";
}
```



### 消息过滤

- **Broker**端消息过滤　　
	在**Broker**中，按照**Consumer**的要求做过滤，优点是减少了对于**Consumer**无用消息的网络传输。缺点是增加了Broker的负担，实现相对复杂。
- **Consumer**端消息过滤
	这种过滤方式可由应用完全自定义实现，但是缺点是很多无用的消息要传输到**Consumer**端。



### 回溯消费

回溯消费是指Consumer已经消费成功的消息，由于业务上的需求需要重新消费，要支持此功能，Broker在向Consumer投递成功消息后，消息仍然需要保留。并且重新消费一般是按照时间维度。

例如由于Consumer系统故障，恢复后需要重新消费1小时前的数据，那么Broker要提供一种机制，可以按照时间维度来回退消费进度。

**RocketMQ**支持按照时间回溯消费，时间维度精确到毫秒，可以向前回溯，也可以向后回溯。



## 优化

### 开发

- 同一group下，多机部署，并行消费

- 单个Consumer提高消费线程个数

- 批量消费

	- 消息批量拉取
	- 业务逻辑批量处理

	


### 运维

- 网卡调优
- jvm调优
- 多线程与cpu调优
- Page Cache









## 其它

### 分布式消息组件的条件

- 需要考虑能快速扩容、天然支持集群
- 持久化的姿势
- 高可用性
- 数据0丢失的考虑
- 服务端部署简单、client端使用简单









# 样例

## 样例 [(Example)](https://github.com/apache/rocketmq/blob/master/docs/cn/RocketMQ_Example.md) 

介绍RocketMQ的常见用法，包括基本样例、顺序消息样例、延时消息样例、批量消息样例、过滤消息样例、事务消息样例等。

# 最佳实践

## 最佳实践 [(Best Practice)](https://github.com/apache/rocketmq/blob/master/docs/cn/best_practice.md)

介绍RocketMQ的最佳实践，包括生产者、消费者、Broker以及NameServer的最佳实践，客户端的配置方式以及JVM和linux的最佳参数配置。

## 消息轨迹指南 [(Message Trace)](https://github.com/apache/rocketmq/blob/master/docs/cn/msg_trace/user_guide.md)

介绍RocketMQ消息轨迹的使用方法。

## 权限管理[(Auth Management)](https://github.com/apache/rocketmq/blob/master/docs/cn/acl/user_guide.md)

介绍如何快速部署和使用支持权限控制特性的RocketMQ集群。

## Dledger快速搭建[(Quick Start)](https://github.com/apache/rocketmq/blob/master/docs/cn/dledger/quick_start.md)

介绍Dledger的快速搭建方法。

## 集群部署[(Cluster Deployment)](https://github.com/apache/rocketmq/blob/master/docs/cn/dledger/deploy_guide.md)

介绍Dledger的集群部署方式。



# 运维管理

## 安装RocketMQ

### Docker

 ```shell
# 拉取RocketMQ镜像
$ docker pull rocketmqinc/rocketmq

# 运行RocketMQ的NameServer
$ docker run -d -p 9876:9876 
-v /opt/rocket/data/namesrv/logs:/root/logs 
-v /opt/rocket/data/namesrv/store:/root/store
--name rmqnamesrv 
-e "MAX_POSSIBLE_HEAP=100000000" rocketmqinc/rocketmq sh mqnamesrv

# 使用相同的镜像运行RocketMQ的broker
$ docker run -d -p 10911:10911 -p  10909:10909   
-v   /opt/rocket/data/broker/logs:/root/logs   
-v   /opt/rocket/rocketmq/data/broker/store:/root/store   
-v   /opt/rocket/conf/broker.conf:/opt/rocketmq/conf/broker.conf --name  rmqbroker --link rmqnamesrv:namesrv 
-e "NAMESRV_ADDR=namesrv:9876"   
-e "MAX_POSSIBLE_HEAP=200000000" rocketmqinc/rocketmq sh  mqbroker   
-c /opt/rocketmq/conf/broker.conf  

# 拉取RocketMQ控制台镜像
$ docker pull pangliang/rocketmq-console-ng
$ docker run -e "JAVA_OPTS=-Drocketmq.namesrv.addr=47.101.48.22:9876 
-Dcom.rocketmq.sendMessageWithVIPChannel=false"
-p 19876:19876 
-t pangliang/rocketmq-console-ng
 ```



## 集群部署[(Operation)](https://github.com/apache/rocketmq/blob/master/docs/cn/operation.md)

介绍单Master模式、多Master模式、多Master多slave模式等RocketMQ集群各种形式的部署方法以及运维工具mqadmin的使用方式。

# API Reference（待补充）

[DefaultMQProducer API Reference](https://github.com/apache/rocketmq/blob/master/docs/cn/client/java/API_Reference_DefaultMQProducer.md)