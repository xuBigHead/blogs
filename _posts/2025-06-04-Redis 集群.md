---
layout: post
title: Redis 集群.md
categories: [Redis]
description: Redis
keywords: Redis
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
---
# Cluster Mode

## Redis Cluster

主从模式和哨兵模式数据库都存储了相同的数据，比较浪费内存。而且当数据量增加时，在单个数据库上很难实现在线扩容。Redis Cluster将数据分布存储在不同的节点上，每个节点存储不同的数据。添加节点就能解决扩容问题。

`Redis`集群提供分布式解决方案，通过分片将数据拆分到不同的节点上，并提供复制和故障转移功能。使用了水平扩展的方法，将数据分发到不同的数据库中。

![img](https://oss.xubighead.top/oss/image/202506/1930098239643684865.jpg)

每个虚线圆都表示一个节点，每个节点都有一个主数据库和多个从数据库。任意两个节点都是相同的（三个节点画图容易误以为是一个环，四个节点容易理解），节点之间都共享数据。



因为写性能在高并发下会遇到瓶颈和无法无限地纵向扩展，因此需要Redis分片集群。需要解决「数据路由」和「数据迁移」的问题。



Redis V3.0后提供了 Redis Cluster 集群方案。

主从模式下实现读写分离的架构，可以让多个从服务器承载「读流量」，但面对「写流量」时，始终是只有主服务器在抗。

纵向扩展升级Redis服务器硬件能力，但升级至一定程度下，就不划算了。纵向扩展意味着大内存，Redis持久化时的"成本"会加大（Redis做RDB持久化，是全量的，fork子进程时有可能由于使用内存过大，导致主线程阻塞时间过长）

用多个Redis实例来组成一个集群，按照一定的规则把数据分发到不同的Redis实例上。当集群所有的Redis实例的数据加起来，那这份数据就是全的。



- 自动将数据进行分片，每个 master 上放一部分数据
- 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的

在 Redis cluster 架构下，每个 Redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。

16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议， `gossip` 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。



优点：

- 具有高可用，哨兵模式的优点，他都有
- 数据分片，不重复存储数据，占内存小
- 容易实现扩容和缩容



### 节点

Redis 集群中各个节点使用 `CLUSTER MEET` 命令进行连接。

```shell
CLUSTER MEET <ip> <port>
```

向一个节点 node 发送`CLUSTER MEET`命令，可以让 node 节点与 ip 和 port 的指定节点进行**握手**。握手成功后，node 节点会将目标节点添加到 node 节点所在的集群中。重复这个操作，可以让多个节点处于同一集群。



#### 集群数据结构

- **`clusterNode`结构**保存了一个节点的当前状态，包括节点创建时间、节点名字、节点配置纪元、节点 IP 地址和端口号等等。
- **`clusterLink`结构**是`clusterNode`的一个属性，保存了连接节点所需的有关信息，比如套接字描述符、输入缓冲区和输出缓冲区等等。
- **`clusterState`结构**被每个节点所保存，记录了当前节点视角下集群所处状态，例如集群是在线还是下线，集群包含多少个节点，集群当前的配置纪元等等。



#### 实现原理

![image-20220214151205692](https://oss.xubighead.top/oss/image/202506/1930098337727483905.png)

1. 客户端发送命令 CLUSTER MEET 命令到节点A；
2. 节点A收到命令，向命令中指向的节点B发送 MEET 消息；
3. 节点B收到 MEET 消息后向节点A返回 PONG 消息；
4. 节点A收到 PONG 消息后返回节点B PING消息，至此节点A和节点B之间建立了集群关系。



### 哈希槽

#### 槽

`Redis`分片集群，使用了一种类似于一致性哈希的分片技术——哈希槽，每个键都有一个哈希槽的值，Redis 集群有`16384`个哈希嘈，对键的`CRC16`取模`16384`计算出哈希槽，以此决定放置的哈希嘈的位置。

`Redis`集群中每个节点都负责一部分哈希嘈，比如，集群有三个节点，其中：

- 节点 A 包含 0 到 5460 号哈希槽
- 节点 B 包含 5461 到 10922 号哈希槽
- 节点 C 包含 10923 到 16383 号哈希槽

数据根据哈希嘈分配到不同的数据库中，实现数据的分片。这里添加或者减少一个节点就比较容易了。比如，我想添加一个新的`节点D`，需要将`节点A、B、C`一部分数据移动到`节点D`中。而删除一个`节点A`，就将原来`A节点`的数据分发到其它节点上。



Redis 集群通过分片来保存数据库中的键值对，集群的整个数据库被分为 16384 个哈希槽（Hash Slot），数据库中的每个键都属于这 16384 个槽中的一个，集群中的每个节点可以处理 0 个或最多 16384 个槽。

当数据库中的 16384 个槽都有节点在处理，集群处于**上线状态**。如果有任何一个槽没有节点处理，那么集群处于**下线状态**。

通过向节点发送`CLUSTER ADDSLOTS`命令，我们可以将一个或多个槽指派给节点负责。例如以下命令可以将槽 0~5000 指派给节点 7000 负责：

```shell
> CLUSTER ADDSLOTS 0 1 2 3 4 ... 5000
```



**为什么哈希槽是16384个吗？**

嗯，这个。是这样的，Redis实例之间「通讯」会相互交换「槽信息」，那如果槽过多（意味着网络包会变大），网络包变大，那是不是就意味着会「过度占用」网络的带宽。另外一块是，Redis作者认为集群在一般情况下是不会超过1000个实例。那就取了16384个，即可以将数据合理打散至Redis集群中的不同实例，又不会在交换数据时导致带宽占用过多。

16384个既能让Redis实例分配到的数据相对均匀，又不会影响Redis实例之间交互槽信息产生严重的网络性能开销问题



**为什么对数据进行分区在Redis中用的是「哈希槽」这种方式吗？而不是一致性哈希算法**

一致性哈希算法就是有个「哈希环」，当客户端请求时，会对Key进行hash，确定在哈希环上的位置，然后顺时针往后找，找到的第一个真实节点。一致性哈希算法比「传统固定取模」的好处就是：如果集群中需要新增或删除某实例，只会影响一小部分的数据。但如果在集群中新增或者删除实例，在一致性哈希算法下，就得知道是「哪一部分数据」受到影响了，需要进行对受影响的数据进行迁移。

而哈希槽的方式下，在集群中的每个实例都能拿到槽位相关的信息。当客户端对key进行hash运算之后，如果发现请求的实例没有相关的数据，实例会返回「重定向」命令告诉客户端应该去哪儿请求。集群的扩容、缩容都是以「哈希槽」作为基本单位进行操作，总的来说就是「实现」会更加简单（简洁，高效，有弹性）。过程大概就是把部分槽进行重新分配，然后迁移槽中的数据即可，不会影响到集群中某个实例的所有数据。

哈希槽实现相对简单高效，每次扩缩容只需要动对应Solt（槽）的数据，一般不会动整个Redis实例。



#### 记录节点的槽指派信息

`clusterNode`结构的`slots`属性和`numslot`属性记录了节点负责处理哪些槽。

- `slots`长度为 16384，某一位的长度是 1 或者 0，代表这个槽有没有被当前的节点负责处理。
- `numslots`属性记录了当前节点负责处理的槽的数量。



#### 传播节点的槽指派信息

节点会将自己的`slots`数组通过消息发送给集群中的其他节点，告诉他们自己目前负责处理哪些槽。

当节点 A 通过消息从节点 B 那里接收到节点 B 的`slots`数组时，会在自己的`clusterState.nodes`字典中查找节点 B 对应的`clusterNode`结构，并进行更新。

有了这个映射关系以后，客户端也会缓存一份到自己的本地上，那自然客户端就知道去哪个Redis实例上操作了。



#### 执行命令

上线状态的集群可以执行命令。当客户端向节点发送命令时，接受命令的节点会检查命令要处理的数据库键属于哪个槽，并检查这个槽是否指派给了自己。

- 如果是，则执行命令。
- 如果不是，则向客户端返回一个`MOVED`错误，指引客户端转向正确节点，重新发送之前的命令。



##### 执行过程

当客户端请求时，使用CRC16算法算出Hash值并模以16384，自然就能得到哈希槽进而得到所对应的Redis实例位置。



1. 计算给定key所属的槽

redis使用如下算法来计算给定的`key`属于哪个槽

```c
def slot_number(key):
	return CRC16(key) & 16383
```



`CRC16(key)`语句计算出`key`的`CRC-16`校验和，而`& 16383`计算出一个介于 0 和 16383 之间的整数作为key的槽号。`CLUSTER KEYSLOT`命令是根据上面的槽分配算法来实现的。



2. 判断槽是否由当前节点负责

检查自己的`slots`数组中的对应位置。如果有当前节点处理则执行命令，如果不是则返回MOVED，让客户端根据 MOVED 错误转向正确的节点。

MOVED 错误格式：

```shell
MOVED <slot> <ip>:<port>
```



#### 节点数据库的实现

除了将键值对保存在数据里面外，节点还会用`clusterState`结构中的`slots_to_keys`跳跃表来保存槽和键之间的关系。

该跳表的每个节点的分值都是一个槽号。而每个节点的成员都是一个数据库键。使用这个跳表，可以方便地对属于某些槽的所有数据库键进行批量操作。

![image-20220214153936717](https://oss.xubighead.top/oss/image/202506/1930098414130925570.png)



### 集群主从模式

为了保证高可用，`Redis Cluster`也使用了主从模式。节点（上图虚线圆）宕机了，就无法提供继续数据服务了。当节点引入主从模式后，主服务宕机之后，从服务器升级成主服务。但是如果一个节点的所有主从服务服务都宕机了，该节点就无法提供数据服务了。



### 重新分片

#### 重新分片流程

Redis 集群的重新分片操作可以令分派给某个槽的节点重新分派给另一个节点。重新分片可以在线进行，过程中集群不需要下线，且可以正常处理命令请求。

重新分片由 Redis 集群管理软件`redis-trib`负责执行。

![image-20220214154622162](https://oss.xubighead.top/oss/image/202506/1930098454207500289.png)

重新分片流程如上图。如果要分片的槽属于多个节点，就要对多个节点发送命令。



#### ASK错误

重新分片期间可能出现一种情况：属于被迁移槽的一部分键值对保存在源节点里面，另一部分键值对保存在目标节点里面。此时当客户端向源节点发送一个命令，并且要求处理的数据库键恰好正在被迁移：

- 源节点现在自己的数据找，找到就直接发送命令。
- 没找到就向客户端返回 ASK 错误，指引客户端转向目标节点执行命令。

![image-20220214155110855](https://oss.xubighead.top/oss/image/202506/1930098503972917249.png)



一个 ASK 错误如下图所示：

![image-20220214155341074](https://oss.xubighead.top/oss/image/202506/1930098524390789122.png)

收到 ASK 错误的客户端会根据错误提供的 IP 地址和端口号，转向目标节点，先向目标节点发送一个 ASKING 命令，之后再重新发送原本要执行的命令。

![image-20220214155539535](https://oss.xubighead.top/oss/image/202506/1930098542132695042.png)

ASKING 命令可以打开发送该命令的客户端的`REDIS_ASKING`标识。这在服务端接收到后面的命令时有用。

![image-20220214155528944](https://oss.xubighead.top/oss/image/202506/1930098564349923330.png)



正在进行重新分片的服务端判断发送请求的客户端的 ASKING 标识是否打开

- 如果打开，该客户端的请求是一个槽分派时的重定向请求，正常执行。
- 如果没打开，该客户端的请求是一个普通的寻址错误的命令请求，返回 MOVED 错误。



当集群删除或者新增Redis实例时，那总会有某Redis实例所负责的哈希槽关系会发生变化。发生变化的信息会通过消息发送至整个集群中，所有的Redis实例都会知道该变化，然后更新自己所保存的映射关系。但这时候，客户端其实是不感知的，所以，当客户端请求时某Key时，还是会请求到「原来」的Redis实例上。而原来的Redis实例会返回「moved」命令，告诉客户端应该要去新的Redis实例上去请求啦。客户端接收到「moved」命令之后，就知道去新的Redis实例请求了，并且更新「缓存哈希槽与实例之间的映射关系」，总结起来就是：数据迁移完毕后被响应，客户端会收到「moved」命令，并且会更新本地缓存。

如果数据还没完全迁移完，那这时候会返回客户端「ask」命令。也是让客户端去请求新的Redis实例，但客户端这时候不会更新本地缓存。

如果已经迁移完毕了，那就返回「move」命令告诉客户端应该去找哪个Redis实例要数据，并且客户端应该更新自己的缓存(映射关系)，如果正在迁移中，那就返回「ack」命令告诉客户端应该去找哪个Redis实例要数据。



### 复制和故障转移

Redis 集群中的主节点用于处理槽，从节点用于复制某个主节点，并且在被复制的主节点下线时接替它。

假如某个场景下服务器间的状态如下：

假如主服务器 7000 下线，那么剩余的主服务器即 7001、7002、7003 会从 7000 的两个从服务器 7004、7005 选择一个接管 7000 负责的槽。另一个从服务器会变成新主服务器的从服务器。

![image-20220215191830836](https://oss.xubighead.top/oss/image/202506/1930098668439965698.png)

![image-20220215192915987](https://oss.xubighead.top/oss/image/202506/1930098685888270338.png)

如果后续 7000 重新上线，它会变成 7004 的新从节点。

![image-20220215192942362](https://oss.xubighead.top/oss/image/202506/1930098705467281409.png)



#### 故障检测

集群中每个节点会定时向集群中的其它节点发送 PING 消息，以此检测对方是否下线。如果接收 PING 的节点没有在规定时间内返回 PONG 消息，就会被标记为**疑似下线**。

集群中的节点在别的节点的视角下有三种状态：**在线、疑似下线、已下线。**

当一个主节点 A 通过消息得知主节点 B 认为主节点 C 进入疑似下线时，主节点 A 会在自己的`clusterState.nodes`字典中找到主节点 C 对应的`clusterNode`结构，并将主节点 B 的下线报告添加到`clusterNode`结构的`fail_reports`链表中。

如果在集群中，有一个主节点发现半数以上的主节点都将某个主节点 x 标记为意思下线，那么他将把这个主节点标记为已下线，并向其他所有节点广播 FAIL 消息。

![image-20220215195303379](https://oss.xubighead.top/oss/image/202506/1930098813026013185.png)



#### 故障转移

当一个从节点发现自己正在复制的主节点进入已下线状态时，从节点将开始对下线主节点进行故障转移操作：

1. 第一个发现问题的主节点举行一个选举，要求自己称为新的主节点。选举的规则和 Sentinel 中选举领头 Sentinel 非常相似，他们都基于 Raft 算法的领头选举方法。
    - 成功则称为新主节点
    - 失败则等待下一个从节点提出选举。
2. 被选中的从节点执行`SLAVEOF no one`命令，成为新的主节点。
3. 新的主节点接管已下线主节点的全部槽指派。
4. 新主节点广播 PONG 消息，让别的节点知道自己已经成为主节点。
5. 新主节点开始行使主节点职责。



### 高可用

Redis cluster 的高可用的原理，几乎跟哨兵是类似的。



#### 判断节点宕机

如果一个节点认为另外一个节点宕机，那么就是 `pfail` ，**主观宕机**。如果多个节点都认为另外一个节点宕机了，那么就是 `fail` ，**客观宕机**，跟哨兵的原理几乎一样，sdown，odown。

在 `cluster-node-timeout` 内，某个节点一直没有返回 `pong` ，那么就被认为 `pfail` 。

如果一个节点认为某个节点 `pfail` 了，那么会在 `gossip ping` 消息中， `ping` 给其他节点，如果**超过半数**的节点都认为 `pfail` 了，那么就会变成 `fail` 。



#### 从节点过滤

对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node。

检查每个 slave node 与 master node 断开连接的时间，如果超过了 `cluster-node-timeout * cluster-slave-validity-factor` ，那么就**没有资格**切换成 `master` 。



#### 从节点选举

每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举。

所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node `（N/2 + 1）` 都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master。

从节点执行主备切换，从节点切换为主节点。

#### 与哨兵比较

整个流程跟哨兵相比，非常类似，所以说，Redis cluster 功能强大，直接集成了 replication 和 sentinel 的功能。



### 集群命令

```shell
# 查看各个节点连接情况
> cluster nodes
# 查看集群状态
> cluster info
# 发现节点
> cluster meet IP PORT
```



### Summary

Cluster模式：

- `Redis`集群有`16384`个哈希嘈，对键的`CRC16`取模`16384`计算出哈希槽。
- 集群使用分片，使用节点方式，将哈希槽分布在每个节点上。也就讲数据分布存储上不同的节点上。
- 为了保证服务的高可用，每个节点都可以搭建主从。
- 数据库扩容需要添加节点，从新计算哈希嘈，将其他数据库的数据，转移到新节点上。也可以删除节点实现数据库的缩容，删除节点后，该节点的数据也会根据哈希嘈分配到其他节点上。



## 元数据维护

### 集中式

**集中式**是将集群元数据（节点信息、故障等等）集中存储在某个节点上。集中式元数据集中存储的一个典型代表，就是大数据领域的 `storm` 。它是分布式的大数据实时计算引擎，是集中式的元数据存储的结构，底层基于 zookeeper（分布式协调的中间件）对所有元数据进行存储维护。



### Gossip协议

> Redis cluster 节点间采用 gossip 协议进行通信。



Redis 维护集群元数据采用另一个方式， `gossip` 协议，所有节点都持有一份元数据，不同的节点如果出现了元数据的变更，就不断将元数据发送给其它的节点，让其它节点也进行元数据的变更。

gossip 协议包含多种消息，包含 `ping` , `pong` , `meet` , `fail` 等等。



- meet：某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其它节点进行通信。其实内部就是发送了一个 gossip meet 消息给新加入的节点，通知那个节点去加入我们的集群。
- ping：每个节点都会频繁给其它节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据。
- pong：返回 ping 和 meeet，包含自己的状态和其它信息，也用于信息广播和更新。
- fail：某个节点判断另一个节点 fail 之后，就发送 fail 给其它节点，通知其它节点说，某个节点宕机啦。



#### ping消息

ping 时要携带一些元数据，如果很频繁，可能会加重网络负担。

每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其它节点。当然如果发现某个节点通信延时达到了 `cluster_node_timeout / 2` ，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了。比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题。所以 `cluster_node_timeout` 可以调节，如果调得比较大，那么会降低 ping 的频率。

每次 ping，会带上自己节点的信息，还有就是带上 1/10 其它节点的信息，发送出去，进行交换。至少包含 `3` 个其它节点的信息，最多包含 `总节点数减 2` 个其它节点的信息。



### 总结

**集中式**的**好处**在于，元数据的读取和更新，时效性非常好，一旦元数据出现了变更，就立即更新到集中式的存储中，其它节点读取的时候就可以感知到；**不好**在于，所有的元数据的更新压力全部集中在一个地方，可能会导致元数据的存储有压力。

gossip 好处在于，元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续打到所有节点上去更新，降低了压力；不好在于，元数据的更新有延时，可能导致集群中的一些操作会有一些滞后。



## 分布式寻址算法

### Hash算法

来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致**大部分的请求过来，全部无法拿到有效的缓存**，导致大量的流量涌入数据库。



### 一致性Hash算法和虚拟节点

一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。

来了一个 key，首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环**顺时针“行走”**，遇到的第一个 master 节点就是 key 所在位置。

在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。

然而，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成**缓存热点**的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。



![consistent-hashing-algorithm](https://oss.xubighead.top/oss/image/202506/1930098979703459841.png)



### Hash Slot算法

Redis cluster 有固定的 `16384` 个 hash slot，对每个 `key` 计算 `CRC16` 值，然后对 `16384` 取模，可以获取 key 对应的 hash slot。

Redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 `hash tag` 来实现。

任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。



## Codis服务端路由

服务端路由一般指的就是，有个代理层专门对接客户端的请求，然后再转发到Redis集群进行处理。它与Redis Cluster最大的区别就是，Redis Cluster是直连Redis实例的，而Codis则客户端直连Proxy，再由Proxy进行分发到不同的Redis实例进行处理。

![img](https://oss.xubighead.top/oss/image/202506/1930099063321104386.png)



在Codis对Key路由的方案跟Redis Cluster很类似，Codis初始化出1024个哈希槽，然后分配到不同的Redis服务器中。哈希槽与Redis实例的映射关系由Zookeeper进行存储和管理，Proxy会通过Codis DashBoard得到最新的映射关系，并缓存在本地上。

默认分配1024个哈希槽，映射相关信息会被保存至Zookeeper集群。Proxy会缓存一份至本地，Redis集群实例发生变化时，DashBoard更新Zookeeper和Proxy的映射信息。



### 扩容Codis Redis实例的流程

##### 同步扩容

简单来说就是把新的Redis实例加入到集群中，然后把部分数据迁移到新的实例上。

1.「原实例」某一个Solt的部分数据发送给「目标实例」。

2.「目标实例」收到数据后，给「原实例」返回ack。

3.「原实例」收到ack之后，在本地删除掉刚刚给「目标实例」的数据。

4.不断循环1、2、3步骤，直至整个solt迁移完毕。



##### 异步扩容

Codis也是支持「异步迁移」的，针对上面的步骤2，「原实例」发送数据后，不等待「目标实例」返回ack，就继续接收客户端的请求。未迁移完的数据标记为「只读」，不会影响到数据的一致性。如果对迁移中的数据存在「写操作」，那会让客户端进行「重试」，最后会写到「目标实例」上

![img](https://oss.xubighead.top/oss/image/202506/1930099138415923202.png)

还有就是，针对 bigkey，异步迁移采用了「拆分指令」的方式进行迁移，比如有个set元素有10000个，那「原实例」可能就发送10000条命令给「目标实例」，而不是一整个bigkey一次性迁移（因为大对象容易造成阻塞）



## 总结

### Redis Cluster 和 Codis 对比

# Codis与Redis Cluster架构对比

| 对比维度         | Codis                                                        | Redis Cluster                                     |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------- |
| **数据路由信息** | • 中心化存储在Zookeeper<br>• Proxy节点本地缓存路由表         | • 全分布式设计<br>• 每个Redis实例保存完整路由信息 |
| **集群扩容**     | 需同时扩容：<br>• Codis Server<br>• Codis Proxy              | 仅需增加Redis实例                                 |
| **数据迁移**     | 支持两种模式：<br>• 同步迁移<br>• 异步迁移                   | 仅支持同步迁移                                    |
| **客户端兼容性** | • 兼容原生单机版客户端<br>• 无需特殊改造                     | 必须使用支持Cluster协议的客户端                   |
| **可靠性机制**   | • Server层：主从集群<br>• Proxy层：无状态设计<br>• ZK：CP协议保证（半数节点存活即可） | • 纯主从集群保证<br>• 无第三方依赖                |

> **技术选型建议**：
> 1. 需要平滑迁移/兼容旧客户端 → **Codis**
> 2. 追求纯Redis生态/避免ZK依赖 → **Redis Cluster**
> 3. 大规模集群管理 → Codis的Proxy架构更易控制
> 4. 数据强一致性要求 → Redis Cluster的同步迁移更可靠