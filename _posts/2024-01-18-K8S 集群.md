# Cluster

## Introduction

Master节点主要还是负责管理和控制。Node节点是工作负载节点，里面是具体的容器。

在集群管理方面，K8s将集群中的机器划分为一个Master节点和一群工作节点Node。Master节点上运行着集群管理相关的一组进程kube-apiserver、kube-controller-manager和kube-scheduler。这些进程自动化实现了整个集群的资源管理、Pod调度、弹性伸缩、安全控制、系统监控和纠错等管理功能。

一个 Kubernetes 集群由一组被称	作节点的机器组成。这些节点分为Master节点和Node节点，Master节点也叫控制平面，负责管理集群中的工作节点和 Pod ，为集群提供故障转移和高可用性。Node节点即工作节点，集群具有至少一个工作节点，工作节点负责管理Pod。



![img](../Image/2022/221206-14.jpg)

上图可以看到如下组件，使用特别的图标表示Service和Label：

- Kubernetes Master（Kubernetes主节点）
- Node（节点）
- Pod
- Container（容器）
- Label(label)（标签）
- Replication Controller（复制控制器）
- Service（enter image description here）（服务）



k8s系统组件之间通信只能通过API服务器通信，他们之间不会之间进行通信。

API服务器是和etcd通信的唯一组件，其他组件不会直接和etcd进行通信。

API服务器与其他组件之间的通信基本上是由其他组件发起的，



## Component

### Master

Master指的是集群控制节点。每个K8s集群里需要有一个Ms节点负责整个集群的管理和控制。Kubernetes Master提供集群的独特视角，并且拥有一系列组件。

- Kubernetes API Server（kube-apiserver），侍卫大统领！提供HTTP Rest接口的关键服务进程，是K8s里所有资源的增删改查等操作的唯一入口，也是集群控制的入口进程。API Server提供可以用来和集群交互的Rest端点。整个系统的对外接口，供客户端和其它组件调用，相当于“营业厅”。
- Kubernetes Controller Master（kube-controller-manager）掌印大太监，大总管！K8s里所有资源对象的自动化控制中心。负责管理控制器，相当于“大总管”。
- Kubernetes Scheduler（kube-scheduler），御马间的调度室！负责资源调度（Pod调度）的进程。创建和复制Pod的Replication Controller。负责对集群内部的资源进行调度，相当于“调度室”。



```
//获取Master节点服务健康状况
#kubectl get componentstatuses  
```



Master节点对集群做出全局决策(比如调度)，以及检测和响应集群事件，Master节点上运行如下一组关键进程：

**kube-apiserver**：提供HTTP Rest接口，是用户对集群所有资源增删改查的入口，也是集群控制入口进程。

**kube-scheduler**：负责资源调度，即控制新创建的未指定运行节点的Pods在哪个节点上运行。

**kube-controller-manager**：Kubernets里面所有资源对象的自动化控制中心。

**etcd**：etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。



#### ectd

etcd 是兼顾一致性与高可用性的键值对数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。你的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。

etcd是一个响应快，分布式，一致的key-value存储。是k8s存储集群状态和元数据唯一的地方，具有乐观锁特性，



**资源如何存储在etcd中**

flannel操作etcd使用的是v2的API，而kubernetes操作etcd使用的v3的API，所以在下面我们执行`etcdctl`的时候需要设置`ETCDCTL_API`环境变量，该变量默认值为2。

etcd是使用raft一致性算法实现的，是一款分布式的一致性KV存储，主要用于共享配置和服务发现。

Etcd V2和V3之间的数据结构完全不同，互不兼容。



**确保etcd集群一致性**

一致性算法要求大部分节点参与，才能进行到下一个状态，需要有过半的节点参与状态的更新，所以导致etcd的节点必须为奇数个。



#### kube-apiserver

API 服务器是 Kubernetes 控制平面的组件， 该组件负责公开了 Kubernetes API，负责处理接受请求的工作。 API 服务器是 Kubernetes 控制平面的前端。

Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平扩缩，也就是说，它可通过部署多个实例来进行扩缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。

Kubernetes API服务器为API对象验证和配置数据，这些对象包含Pod，Service，ReplicationController等等。API Server提供REST操作以及前端到集群的共享状态，所有其它组件可以通过这些共享状态交互。

1.API提供RESTful API的形式，进行CRUD(增删查改)集群状态

2.进行校验

3.处理乐观锁，用于处理并发问题，

4.认证客户端

（1）通过认证插件认证客户端

（2）通过授权插件认证客户端

（3）通过准入插件验证AND/OR修改资源请求

**API服务器如何通知客户端资源变更**

API服务器不会去创建pod，同时他不会去管理服务的端点，

它做的是，启动控制器，以及一些其他的组件来监控一键部署的资源变更，是得组件可以再集群元数据变化时候执行任何需要做的任务，



#### kube-scheduler

kube-scheduler 是控制平面的组件， 负责监视新创建的、未指定运行节点（node）的 Pods， 并选择节点来让 Pod 在上面运行。调度决策考虑的因素包括单个 Pod 及 Pods 集合的资源需求、软硬件及策略约束、 亲和性及反亲和性规范、数据位置、工作负载间的干扰及最后时限。

调度器指定pod运行在哪个集群节点上。

调度器不会命令选中节点取运行pod，调度器做的就是通过api服务器更新pod的定义。然后api服务器再去通知kubelet该pod已经被调用。当目标节点的kubelet发现该pod被调度到本节点，就会创建并运行pod容器。

![img](../Image/2022/221206-45.jpg)

调度方法：

1.通过算法过滤所有节点，找到最优节点

2.查找可用节点

（1）是否满足对硬件的资源要求

（2）节点是否资源耗尽

（3）pod是否要求被调度到指定的节点、

（4）是否和要求的lable一致

（5）需求端口是否被占用

（6）是否有符合要求的存储卷

（7）是否容忍节点的污染



#### kube-controller manager

控制器管理器（kube-controller-manager）是控制平面的组件， 负责运行控制器进程。从逻辑上讲， 每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在同一个进程中运行。
这些控制器包括：
节点控制器（Node Controller）：负责在节点出现故障时进行通知和响应
任务控制器（Job Controller）：监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成
端点控制器（Endpoints Controller）：填充端点（Endpoints）对象（即加入 Service 与 Pod）
服务帐户和令牌控制器（Service Account & Token Controllers）：为新的命名空间创建默认帐户和 API 访问令牌



**（1）RC控制器**

启动RC资源的控制器叫做Replication管理器。

RC的操作可以理解为一个无限的循环，每次循环，控制器都会查找符合其pod选择器的pod数量，并且将该数值和期望的复制集数量做比较。

**（2）RS控制器**

与RC类似

**（3）DaemonSet以及job控制器**

从他们各自资源集中定义pod模板创建pod资源，

**（4）Deployment控制器**

Deployment控制器负责使deployment的实际状态与对应的Deployment API对象期望状态同步。

每次Deployment对象修改后，Deployment控制器会滚动升级到新的版本。通过创建ReplicaSet，然后按照Deployment中定义的策略同时伸缩新、旧RelicaSet，直到旧pod被新的替代。

**（5）StatefulSet控制器**

StatefulSet控制器会初始化并管理每个pod实例的持久声明字段。

**（6）Node控制器**

Node控制器管理Node资源，描述了集群的工作节点。

**（7）Service控制器**

Service控制器就是用来在loadBalancer类型服务被创建或删除，从基础设施服务请求，释放负载均衡器的。

当Endpoints监听到API服务器中Aervice资源和pod资源发生变化时，会对应修改、创建、删除Endpoints资源。

**（8）Endpoint资源控制器**

Service不会直接连接到pod，而是通过一个ip和端口的列表，EedPoint管理器就是监听service和pod的变化，将ip和端口更新endpoint资源。

![img](../Image/2022/221206-46.jpg)

**（9）Namespace控制器**

当收到删除namespace对象的通知时，控制器通过API服务群删除后所有属于该命名空间的资源。

**（10）PV控制器**

创建一个持久卷声明时，就找到一个合适的持久卷进行绑定到声明。



#### cloud-controller-manager

一个 Kubernetes 控制平面组件， 嵌入了特定于云平台的控制逻辑。 云控制器管理器（Cloud Controller Manager）允许你将你的集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与你的集群交互的组件分离开来。cloud-controller-manager 仅运行特定于云平台的控制器。 因此如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的集群不需要有云控制器管理器。
与 kube-controller-manager 类似，cloud-controller-manager 将若干逻辑上独立的控制回路组合到同一个可执行文件中， 供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。
下面的控制器都包含对云平台驱动的依赖：
节点控制器（Node Controller）：用于在节点终止响应后检查云提供商以确定节点是否已被删除
路由控制器（Route Controller）：用于在底层云基础架构中设置路由
服务控制器（Service Controller）：用于创建、更新和删除云提供商负载均衡器



### Node

节点组件会在每个节点上运行，负责维护运行的 Pod 并提供 Kubernetes 运行环境。

节点（上图橘色方框）是物理或者虚拟机器，作为Kubernetes worker，通常称为Minion。每个节点都运行如下Kubernetes关键组件。

> (1) Kubelet：与Master节点协作，是主节点的代理，负责Pod对应容器的创建，启动，停止等任务。默认情况下Kubelet会向Master注册自己。Kubelet定期向主机点汇报加入集群的Node的各类信息。
>
> (2) Kube-proxy：Kubernetes Service使用其将链接路由到Pod，作为外部负载均衡器使用，在一定数量的Pod之间均衡流量。比如，对于负载均衡Web流量很有用。
>
> (3) Docker或Rocket：Kubernetes使用的容器技术来创建容器。



Node节点包括Docker、kubelet、kube-proxy、Fluentd、kube-dns（可选），还有就是**Pod**。

- Docker，不用说了，创建容器的。
- Kubelet，主要负责监视指派到它所在Node上的Pod，包括创建、修改、监控、删除等。
- Kube-proxy，主要负责为Pod对象提供代理。
- Fluentd，主要负责日志收集、存储与查询。



Node节点是Kubernetes集群的工作负载节点，每个Node会被Master分配一些工作负载（Docker容器），当Node宕机时，其上所有工作负载会被Master自动转移到其它Node。每个Node节点中都运行着以下关键进程：

**kubelet**：负责Pod对应的容器的创建、起停操作，同时与Master紧密协作实现集群管理基本功能。

**kube-proxy**：实现Kubernetes服务与负载均衡机制的重要组件。

**Docker Engine**：提供容器运行时环境。



#### kubelet

kubelet 会在集群中每个节点（node）上运行。 它保证容器（containers）都运行在 Pod 中。kubelet 接收一组通过各类机制提供给它的 PodSpecs， 确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。

简单来说，就是负责所有运行在工作节点上的全部内容。

第一任务，在api服务器中创建一个node资源来注册该节点；

第二任务，持续监控api服务器是否把该节点分配给pod；

第三任务，启动pod；

第四任务，持续监控运行的容器，向api服务器报告他们的状态，事件和资源消耗。

第五任务，kubelet也是运行容器的存活探针的组件，当探针报错时，他会重启容器；

第六任务，当pod从api服务器删除时，kubelet终止容器，并通知服务器pod已经终止。



#### Kube-proxy

kube-proxy 是集群中每个节点（node）所上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。kube-proxy 维护节点上的一些网络规则， 这些网络规则会允许从集群内部或外部的网络会话与 Pod 进行网络通信。如果操作系统提供了可用的数据包过滤层，则 kube-proxy 会通过它来实现网络规则。 否则，kube-proxy 仅做流量转发。



service是一组pod的服务抽象，相当于一组pod的LB，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP。
kube-proxy的作用主要是负责service的实现，具体来说，就是实现了内部从pod到service和外部的从node port向service的访问。

kube-proxy有两种代理模式，userspace和iptables，目前都是使用iptables。



## Cluster Manage

在生产环境中可能不止有一套kubernetes(k8s)集群，可以使用kubeconfig文件管理多套kubernetes(k8s)集群，使用 kubeconfig 文件，你可以组织集群、用户和命名空间。还可以定义上下文，以便在集群和命名空间之间快速轻松地切换。



### kubeconfig

用于配置集群访问的文件称为“kubeconfig 文件”。 这是引用配置文件的通用方法，并不意味着有一个名为 kubeconfig 的文件。

使用 kubeconfig 文件来组织有关集群、用户、命名空间和身份认证机制的信息。 kubectl 命令行工具使用 kubeconfig 文件来查找选择集群所需的信息，并与集群的 API 服务器进行通信。

默认情况下，kubectl 在 $HOME/.kube 目录下查找名为 config 的文件。 你可以通过设置 KUBECONFIG 环境变量或者设置 --kubeconfig参数来指定其他 kubeconfig 文件。

kubeconfig文件由3部分组成：clusters，users，contexts。clusters是kubernetes(k8s)集群信息，users是连接kubernetes(k8s)集群的用户信息，contexts上下文综合了clusters信息和users信息。



配置文件如下：

```yml
apiVersion: v1
# 集群信息
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ***0FURS0tLS0tCg==
    server: https://192.168.110.130:6443
  name: kubernetes
# 上下文信息
contexts:
- context:
    cluster: kubernetes
    namespace: kube-system
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
# 当前上下文
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
# 用户客户端信息
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDR***URS0tLS0tCg==
    client-key-data: LS0tLS1CRUdJTiBS***LRVktLS0tLQo=
```



### Manage Command

默认的kubeconfig文件为：~/.kube/config 文件

```sh
[root@k8scloude1 ~]# ls .kube/
cache  config  kubens

[root@k8scloude1 ~]# ll -h .kube/config 
-rw------- 1 root root 5.5K 1月  10 14:54 .kube/config
```



查看kubeconfig格式：

```sh
[root@k8scloude1 ~]# kubectl config view 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    ...
```



编写一个kubeconfig文件来管理两套k8s集群：

```
[root@k8scloude1 ~]# cp .kube/config config2

[root@k8scloude1 ~]# cat config2 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJ***UZJQ0FURS0tLS0tCg==
    server: https://192.168.110.130:6443
  name: cluster1
- cluster:
    certificate-authority-data:这里写第二个集群的密钥 
    server: https://cluster2_ip:6443
  name: cluster2
contexts:
- context:
    cluster: cluster1
    namespace: default
    user: admin1
  name: admin1@cluster1
- context:
    cluster: cluster2
    namespace: default
    user: admin2
  name: admin2@cluster2
current-context: admin1@cluster1
kind: Config
preferences: {}
users:
- name: admin1
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJ***URS0tLS0tCg==
    client-key-data: LS0tLS1CRUdJTiBSU0EgU***FURSBLRVktLS0tLQo=
- name: admin2
  user:
    client-certificate-data:这里写第二个集群的用户公钥 
    client-key-data:这里写第二个集群的用户私钥
```



查看全局上下文，看有几套集群

```shell
[root@k8scloude1 ~]# kubectl config get-contexts 
CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE
*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   kube-system
```



切换k8s集群

```shell
[root@k8scloude1 ~]# kubectl config use-context kubernetes-admin@kubernetes
Switched to context "kubernetes-admin@kubernetes".
```
